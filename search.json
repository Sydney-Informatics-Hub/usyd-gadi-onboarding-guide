[
  {
    "objectID": "notebooks/02_system_setup.html",
    "href": "notebooks/02_system_setup.html",
    "title": "Gadi overview",
    "section": "",
    "text": "This section will point you to the right sections of the NCI documentation and user guides to get you started on the Gadi HPC system.",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#resources",
    "href": "notebooks/02_system_setup.html#resources",
    "title": "Gadi overview",
    "section": "Resources",
    "text": "Resources\n\nWhat is Gadi?\nGetting Started at NCI\nGadi User Guide\nGadi FAQs",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#gadi-technical-summary",
    "href": "notebooks/02_system_setup.html#gadi-technical-summary",
    "title": "Gadi overview",
    "section": "Gadi technical summary",
    "text": "Gadi technical summary\nNCI Gadi is one of Australia’s most powerful supercomputers, designed to support advanced computational research.\n\n\n\n\n\n\n\nComponent\nDetails\n\n\n\n\nCompute\n- Nodes: 4,962- Processors: Intel Sapphire Rapids, Cascade Lake, Skylake, and Broadwell CPUs- GPUs: NVIDIA V100 and DGX A100 GPUs- Performance: Over 10 petaflops of peak performance\n\n\nStorage\n- Disk Drives: 7,200 4-Terabyte hard disks in 120 NetApp disk arrays- Capacity: 20 Petabytes total usable capacity- Performance: 980 Gigabytes per second maximum performance\n\n\nFilesystems\n- Total Capacity: Approximately 90 Petabytes- Global Lustre Filesystems: Five, with an aggregate I/O performance of around 450 GB/second- IO Intensive Platform: Dedicated filesystem using 576 2-Terabyte NVMe drives, achieving around 960 Gigabytes per second cumulative performance\n\n\nArchival Storage\n- Capacity: Over 70 Petabytes of archival project data stored in state-of-the-art magnetic tape libraries\n\n\nNetworking\n- Interconnect: 100-gigabit network links connecting high-performance computing with high-performance data\n\n\nCloud Systems\n- Nirin Cloud: High-availability and high-capacity zone integrated with Gadi and NCI’s multi-Petabyte national research data collections, comprising Intel Broadwell and Sandy Bridge processors and NVIDIA K80 GPUs",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#conditions-of-use",
    "href": "notebooks/02_system_setup.html#conditions-of-use",
    "title": "Gadi overview",
    "section": "Conditions of use",
    "text": "Conditions of use\n\nConditions of use and policies\n\nAll users of NCI agree that they will keep themselves informed of, and comply with, all relevant legislation and The Australian National University policies and rules.\nAll users must acknowledge and understand that a breach of these will result in not only a loss of access to NCI resources but the user may be subject to Federal criminal prosecution resulting in fines and/or gaol legislated under the Acts listed on the conditions of use and policies page.",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#login-nodes",
    "href": "notebooks/02_system_setup.html#login-nodes",
    "title": "Gadi overview",
    "section": "Login nodes",
    "text": "Login nodes\nThese nodes are the gateway for Gadi for users to access the resources of the HPC cluster. It is how you log in to Gadi, move around the filesystem, submit jobs to the scheduler, and do small tasks like view the contents of a file.\nWhen you first log into Gadi using ssh, you are working on a login node. These are distinct from the compute nodes and are not designed for running compute tasks or transferring data.\nWhile you can run simple commands on the login nodes on Gadi, commands are throttled. This means they execute more slowly than on the compute nodes and are automatically killed if they exceed CPU, memory and time restrictions that are in place. This is to ensure the login nodes are not over-loaded and remain responsive for all users. As such, all complex or resource-intensive tasks must be submitted to the job scheduler for execution on the cluster nodes.",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#compute-nodes",
    "href": "notebooks/02_system_setup.html#compute-nodes",
    "title": "Gadi overview",
    "section": "Compute nodes",
    "text": "Compute nodes\nThese nodes are the workhorses of any HPC. They are dedicated for executing computational tasks as delegated by the job scheduler when you submit a job. There are various types of compute nodes with different hardware, built for different purposes on Gadi. Depending on the resource requirements of your job (e.g. high memory, GPUs) and the queue you specify, your job will be sent to a specific type of compute node.\nPlease see the queue structure on Gadi and the queue limits pages for detailed information on the compute node specifications.\nFor details of how to choose and request a specific queue for your job, please visit the section on PBS scripts and job submission.",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#data-mover-nodes",
    "href": "notebooks/02_system_setup.html#data-mover-nodes",
    "title": "Gadi overview",
    "section": "Data mover nodes",
    "text": "Data mover nodes\nThese nodes are designed specifically for fast data movement. You can use these nodes to transfer files to and from Gadi at high-speed.\nPlease visit the Gadi file transfer page for more details.\nA comprehensive guide to data transfer between Gadi and University of Sydney RDS, including example commands, template scripts and recommendations, can be found in the section on transferring data.",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_system_setup.html#filesystems",
    "href": "notebooks/02_system_setup.html#filesystems",
    "title": "Gadi overview",
    "section": "Filesystems",
    "text": "Filesystems\n\n$HOME\nWhen you first log in to Gadi, you’ll be placed in your personal $HOME directory (i.e. /home/555/aa1234). You are the only person who can access this directory. No work should be done in here due to the strict 10 GB storage limit, but you may wish to install things like custom R or Python libraries here. It is backed up.\nYou can navigate back here at any point if required by running:\ncd ~\n\n\n/scratch\nAll Gadi projects have a dedicated /scratch allocation that is only accessible to members of your project. This is only intended for active work and not for long-term storage. This is not backed up and any files not accessed for 100 days will be purged from the system, so be sure to back up your work to RDS following our data transfer guide.\nVisit the NCI scratch file management page for details on the purge policy and how to recover files from ‘quarantine’.\nScratch will be /scratch/&lt;nci-project-id&gt;. Each member of a project will have read/write permissions for this parent directory, as well as their /scratch/&lt;nci-project-id&gt;/&lt;nci-user-id&gt; directory.\nYou can navigate to your /scratch space by running:\ncd /scratch/&lt;project&gt;\n\n\n/g/data\nSome Gadi projects have a dedicated /g/data allocation that is only accessible to members of that group. This in intended for long-term large data storage, for example large reference files or databases that are regularly required for your compute jobs. Compute node jobs can read /g/data so this is an ideal place to store those files so that they are not subject to /scratch purge. Files that are on /g/data for this kind of use should also have a copy on RDS, since /g/data/ is not backed up.\nIf you are unsure if your project has a /g/data allocation, you can check by running:\ncd /g/data/&lt;project&gt;\nScratch is provided at no additional cost to users, however /g/data has a cost per TB involved. This cost is covered by the NCI-Sydney Scheme for projects that have a justified need of it. To request /g/data storage space, please request this via an email to nci-sydney.scheme@sydney.edu.au with a brief justification.\n\n\n/apps\nThis directory is accessible to all Gadi users. It is a read-only system containing centrally installed software applications and their module files.\nPlease visit the Software section for more information on global apps and alternatives.",
    "crumbs": [
      "Introduction to Gadi",
      "Gadi overview"
    ]
  },
  {
    "objectID": "notebooks/02_intro_hpc.html",
    "href": "notebooks/02_intro_hpc.html",
    "title": "Introduction to HPC",
    "section": "",
    "text": "High performance computing (HPC) refers to the use of parallel processing techniques to solve complex computation problems efficiently. HPC systems, like Gadi, consist of clusters of interconnected computers, each equipped with multiple processors and large amounts of memory. These systems are capable of handling massive datasets and perform computations at speeds far beyond those achievable by your personal computer.",
    "crumbs": [
      "Introduction to Gadi",
      "Introduction to HPC"
    ]
  },
  {
    "objectID": "notebooks/02_intro_hpc.html#why-do-we-need-hpc-for-research-computing",
    "href": "notebooks/02_intro_hpc.html#why-do-we-need-hpc-for-research-computing",
    "title": "Introduction to HPC",
    "section": "Why do we need HPC for research computing?",
    "text": "Why do we need HPC for research computing?\nResearch computing comes in all shapes and sizes. In some cases, your compute needs are well-met by your personal computer or a web-based platform. In other cases, these platforms are not sufficient and this is where HPC is critical to ensuring a timely analysis of your data.\nTo use HPC, it is not a requirement that your workflow makes use of the multi-node architecture. There are many reasons why HPC would be justified:\n\nLarge input data requiring vast physical storage for inputs and outputs\nHigh CPU or node requirement\nGPU requirement\nHigh memory requirement\nLong walltime requirement\nFaster I/O operations than your local computer can handle\nFreeing up your local computer resources for other tasks, or simply to shut down for the day without affecting the compute analysis you are running\n\nHPC provides a reliable and efficient means of analysing data of all shapes and sizes from all research domains.",
    "crumbs": [
      "Introduction to Gadi",
      "Introduction to HPC"
    ]
  },
  {
    "objectID": "notebooks/01_setup.html",
    "href": "notebooks/01_setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "Before accessing Gadi, you will need to have an NCI account. Ensure you have completed this step by following directions on the Gadi Access instructions on the previous page before proceeding.\nTo work on NCI Gadi, you will need to use a terminal application on your local computer or work on NCI’s Australian Research Environment (ARE) platform, which includes a web-based terminal interface to Gadi. If you already have a terminal application that you have used to access Artemis, for example putty or Mac Terminal, you can continue to use that.\nBelow we will describe 3 options for accessing Gadi:",
    "crumbs": [
      "Set up your computer"
    ]
  },
  {
    "objectID": "notebooks/01_setup.html#are",
    "href": "notebooks/01_setup.html#are",
    "title": "Set up your computer",
    "section": "Use NCI’s ARE platform",
    "text": "Use NCI’s ARE platform\n\n\n\n\n\n\nNoteARE: fast access with limited customisation\n\n\n\nThis is a very lightweight solution for accessing Gadi, some interactive tools like Jupyter and RStudio. We recommended it for beginners who don’t want to customise their set up.\n\n\nNCI has created a web-based graphical interface for accessing their systems. It is very simple to use and recommended over access methods described below for beginners.\nSee NCI’s User Guide for instructions on how to access and use ARE.",
    "crumbs": [
      "Set up your computer"
    ]
  },
  {
    "objectID": "notebooks/01_setup.html#vscode",
    "href": "notebooks/01_setup.html#vscode",
    "title": "Set up your computer",
    "section": "Install Visual Studio Code",
    "text": "Install Visual Studio Code\n\n\n\n\n\n\nNoteVScode: customised configuration with an integrated terminal\n\n\n\nThis is a more advanced solution for accessing Gadi, with more customisation options. We recommended it for users who are comfortable with terminal applications and want to customise their set up.\n\n\nVisual Studio Code (VS Code) is a lightweight and powerful source code editor available for Windows, macOS and Linux computers. As an alternative to a terminal application it offers additional functionality including file editing.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\nConnect to your instance with VS code by adding the host details to your .ssh config file:\nHost Gadi\n  HostName gadi.nci.org.au\n  User &lt;your-nci-username&gt;\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host and Gadi\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your NCI password and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you’re running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /scratch/iz89 to open your workspace. You can change this at any point by opening a new folder. Keep in mind you will be requested to provide your password each time.\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‘home’ then click Yes, I trust the authors\nTo open a terminal, type Ctrl+J if you’re on a Windows machine or Cmd+J on MacOS\n\nTips for using VS Code\n\nPeriodically delete your ~/.vscode-server/ directory on Gadi! VSCode will fill this directory with numerous files and cause your home quota to be exceeded\nVS code cheatsheet for Windows\nVS code cheatsheet for MacOS",
    "crumbs": [
      "Set up your computer"
    ]
  },
  {
    "objectID": "notebooks/01_setup.html#terminal",
    "href": "notebooks/01_setup.html#terminal",
    "title": "Set up your computer",
    "section": "Access via a Terminal Application",
    "text": "Access via a Terminal Application\n\nUse MacOS native terminal\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‘terminal’. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2.\nTo log in to Gadi, you will use a Secure Shell (SSH) connection. To connect, you need 3 things:\n\nThe address of your NCI Gadi, gadi.nci.org.au.\nYour Gadi username, e.g. ab1234.\nYour password.\n\nThen typing the following command into your terminal:\nssh &lt;username&gt;@gadi.nci.org.au\nand provide your password when prompted.\n\n\n\n\n\n\nWarning‼️ Pay Attention ‼️\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You’ll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nOnce you’ve logged in successfully, you should see a welcome screen like this:\n\n\n\nInstall a terminal client on your windows machine\nWindows OS now comes with Windows Subsystem for Linux (WSL) so if you are familiar with using that, you can ssh to Gadi from a WSL terminal.\nIf not, you will need to install a terminal client. There are many options, including putty, Xwin-32 (which the University of Sydney has a license for), and MobaXterm.\nFor example, you can download the free version of MobaXterm using this link and refer to this step-by-step guide on how to set up this tool on your PC to connect to Gadi.",
    "crumbs": [
      "Set up your computer"
    ]
  },
  {
    "objectID": "notebooks/11_software.html",
    "href": "notebooks/11_software.html",
    "title": "Software options on Gadi",
    "section": "",
    "text": "global apps\nspecialised environments\nself-installed tools\nsingularity containers",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/11_software.html#global-apps",
    "href": "notebooks/11_software.html#global-apps",
    "title": "Software options on Gadi",
    "section": "Global apps",
    "text": "Global apps\nOn Gadi, there are shared (global) apps that are installed and managed by the system administrators. On Artemis, these were in /usr/local/ directory, on Gadi they are in /apps/ directory.\nYou can use the same module commands that you are familiar with on Artemis to query and load apps on Gadi.\n# List all global apps starting with 'p':\nls  /apps/p*\n# List all modules for python3:\nmodule avail python3\n# Load a specific version of python3:\nmodule load python3/3.12.1\nEach global app has a default version, so if you run without specifying a version, the default version will be loaded. While this is OK in some circumstances, it is typically recommended to specify the version you know works for your code. Default versions of global apps will change over time without warning, so reproducibility and functionality is best maintained by explicitly stating the version when you load a module within your script.",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/11_software.html#software-groups-and-specialised-environments",
    "href": "notebooks/11_software.html#software-groups-and-specialised-environments",
    "title": "Software options on Gadi",
    "section": "Software groups and specialised environments",
    "text": "Software groups and specialised environments\nNCI provides a range of software groups and specialised environments for different research fields. Before attempting to self-install a tool, you should check if the tool you need is available through global apps or one of these environments.\nPlease visit the NCI pages for more details:\n\nAI/machine learning\ndata analysis\nbioinformatics and genomics\nclimate and weather\nearth observation and environment\ngeophysics\nquantum computing\nvisualisation",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/11_software.html#licensed-software",
    "href": "notebooks/11_software.html#licensed-software",
    "title": "Software options on Gadi",
    "section": "Licensed software",
    "text": "Licensed software\nThe University of Sydney licence server is connected to Gadi, allowing eligible users to run certain University‑licensed applications (e.g. MATLAB, ANSYS, and others) directly on the supercomputer.\nAccess to a licensed application requires that you first join the corresponding NCI licence (software) project. For detailed instructions related to a given piece of software, go to the NCI Help pages and search for your software there.\n\nLog in to MyNCI\nIn the left menu, choose “Projects & Groups”.\nSearch for the application name (e.g. “matlab”).\nRequest to join the relevant licence project (its name typically includes the software name and sometimes an organisational suffix, e.g. _usyd).\nWait for approval (normally &lt; 1 business day). You will receive an email when added.\n\nYou cannot check out a licence token in a job until your membership is approved.\n\nUsing a licence in a PBS script\nTwo things are generally required in your job script:\n\nA PBS directive requesting the licence resource: #PBS -l software=&lt;project_name&gt; so the scheduler accounts for licence availability.\nLoading both the application module (e.g. module load matlab) and any associated licence helper module (e.g. module load matlab_licence/usyd).\n\nBelow is a canonical matlab example; adapt the same pattern for other licensed applications by substituting the software project and module names.\n#!/bin/bash\n\n#PBS -P &lt;project_code&gt;\n#PBS -q normal\n#PBS -l walltime=02:00:00\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l jobfs=400GB\n#PBS -l software=matlab_usyd   # licence project name\n#PBS -l wd\n\n# Always pin module versions for reproducibility\nmodule load matlab/R2019b\nmodule load matlab_licence/usyd   # verify exact helper module name\n\n# Run MATLAB headless; use semicolons to separate MATLAB commands\nmatlab -nodisplay -nosplash -r \"outputDir='${PBS_JOBFS}'; numberOfWorkers=${PBS_NCPUS}; mfile; quit\" &gt; \"${PBS_JOBID}.log\" 2&gt;&1\nTo confirm the correct helper module name (spelling can differ), run for example:\nmodule avail matlab | grep -i lic\n\n\nRunning AnsysEM tools on Gadi and connecting to the licence server\nWe have discovered that the AnsysEM tools do not respect the environment variables when selecting the licence server. Instead, they look for the licensing information in a file ~/.ansys_licensing/ansyslmd.ini.\nTo run AnsysEM tools you need to manually generate the ~/.ansys_licensing/ansyslmd.ini file with the following contents:\nSERVER=1055@ansys1.lic-ext.sydney.edu.au​​",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/11_software.html#self-installed-tools",
    "href": "notebooks/11_software.html#self-installed-tools",
    "title": "Software options on Gadi",
    "section": "Self-installed tools",
    "text": "Self-installed tools\nUnlike Artemis, request of new apps to be installed are not always agreed to. NCI limits global apps to those with a high user base. This is to ensure good maintenance and curation of global apps.\nUsers are encouraged to either self-install apps from source into their /home or /g/data locations, or (recommended) use singularity containers. Installing into /scratch is not recommended due to the 100-day file purge policy. Install into /g/data is ideal when other members of your project need to use the same tool.\nNCI may provide support for users through the self-install process; to request assistance, please email your detailed request including what software tool and version you are attempting to install and describe the issues you are having with the installation.\nOnce a tool has been installed, you can make that tool available to the module commands, by following the steps described here. This is not essential, but can be helpful when managing tools that multiple group members will use.\nTo avoid the burden of installing software that is not available through global apps or specialised environments, the use of singularity containers is recommended.",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/11_software.html#running-singularity-containers-on-gadi",
    "href": "notebooks/11_software.html#running-singularity-containers-on-gadi",
    "title": "Software options on Gadi",
    "section": "Running singularity containers on Gadi",
    "text": "Running singularity containers on Gadi\n\n\n\n\n\n\nTip\n\n\n\nBy default, Singularity caches files in your home directory that has 10 GB quota assigned. To avoid exceeding this limit when pulling or building large images, you should consider setting the cache and tmp directories to /scratch. For example:\nexport SINGULARITY_TMPDIR=/scratch/$PROJECT/$USER/singularity_tmp\nexport SINGULARITY_CACHEDIR=/scratch/$PROJECT/$USER/singularity_cache\nYou can replace the environment variables $PROJECT and $USER with other directories that suit this purpose.\n\n\nSingularity can be used to execute containerised applications on Gadi. It is installed as a global app:\nmodule load singularity\nsingularity version\n# 3.11.3\nNote that the singularity project was recently migrated to apptainer. Please continue to run singularity commands on Gadi for now.\nSingularity (Apptainer) is a\n\n“container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Apptainer on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don’t have to worry about how to install all the software you need on each different operating system.” — from https://apptainer.org\n\nThere are numerous container repositories, for example Docker Hub or quay.io. You can search these repositories, or build your own container if the tool or tool version you need is not yet available.\nSeqera have greatly simplified the process of building custom containers with their build-your-own container tool! Simply search for the tool(s) you want in your container, and click Get Container. This tool will manage the build for you, and host the created container file.\n\nExample\nLet’s assume you want to use the tool FoldSeek. Below are the steps to search for, obtain, test, and use this container in a PBS job script.\n\nRun module load singularity in your Gadi terminal (or copyq container download script)\nVisit quay.io and search for this tool by typing the tool name into the search bar at the top right of the page. There may be multiple containers available. These may reflect different contributors, different tool versions, etc. Look for containers with recent updates and a high star rating where possible.\nSelect the biocontainers container, and then select the Tags page from the icons on the left (options are Information, Tags, Tag history).\nOn the far right of the most recent tag, select the Fetch tag icon and then change image format to Docker Pull (by tag).\nCopy the command. We need to made some changes to the command before we execute it.\nPaste the command into your terminal (or copyq container download script) and change docker to singularity and add prefix docker:// to the container path, as shown below:\n# Default command:\ndocker pull quay.io/biocontainers/foldseek:10.941cd33--h5021889_1\n# Change to:\nsingularity pull docker://quay.io/biocontainers/foldseek:10.941cd33--h5021889_1\nRun the pull command (note: you need to have the singularity module loaded in your terminal or download script). This will download the docker container to a Singularity Image File (.sif). Most containers are lightweight enough to pull directly on the login node. If a container is bulky and slow to download (or you need many containers), you may need to submit the download as a copyq job.\nTest the container with a basic help or version command. When running a container, typical usage involves the following command structure:\nsingularity  exec &lt;container&gt; &lt;command&gt; [args]\nSo everything after the container name is the same as you would run when using a locally installed version of the tool. For foldseek, we would use foldseek version to view the tool version or foldseek help to view the help menu. To run these commands via the container:\nsingularity exec foldseek_10.941cd33--h5021889_1.sif foldseek version\nsingularity exec foldseek_10.941cd33--h5021889_1.sif foldseek help\nNote that the version of foldseek corresponds to the tag name: in this case, v. 10.941cd33.\nTest run the full tool command that you need to use in your analysis. Where possible, use a small subset of your data to test the command (as you would routinely do for new software). You may need to use the interactive job queue or a compute job script for testing if the test command exceeds resource limits on the login nodes.\nAdd the command to your job script. Ensure to include module load singularity and call the container using the structure singularity  exec &lt;container&gt; &lt;command&gt; [args].\n\nBelow is an example Gadi job script running the foldseek container:\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -q normal\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l walltime=02:00:00\n#PBS -l storage=scratch/aa00+gdata/aa00\n#PBS -l wd\n\nmodule load singularity\n\ninput=/g/data/aa00/foldseek-inputs/input.fasta\ndatabase=/g/data/aa00/foldseek-inputs/database\noutput_dir=./foldseek-run/output\nresults=./foldseek-run/foldseek_result.tsv\nlog=./foldseek-run/foldseek.log\n\nsingularity exec \\\n    foldseek_10.941cd33--h5021889_1.sif \\\n    foldseek easy-search  ${input} \\\n    ${database} \\\n    ${output_dir} \\\n    --threads ${PBS_NCPUS} \\\n    ${results} &gt; ${log} 2&gt;&1",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/11_software.html#installing-r-packages-locally-on-gadi",
    "href": "notebooks/11_software.html#installing-r-packages-locally-on-gadi",
    "title": "Software options on Gadi",
    "section": "Installing R Packages Locally on Gadi",
    "text": "Installing R Packages Locally on Gadi\nLocal installation of R packages on gadi is often necessary since you may require specific versions of them or often system-wide R packages are unavailable.\nYou can install R packages yourself in a local directory that will be mounted on the compute nodes when you run your PBS script. To do this you can use the following steps:\n\n1. Create a Local R Library Directory.\nYou must store your R packages in a directory with sufficient storage space. Note that your home directory (/home) may not have the required quota for this. Instead, create a directory in /g/data or /scratch.\nRun the following command to install local R packages in /g/data (replace /g/data, &lt;project&gt; and &lt;username&gt; appropriately):\nmkdir -p /g/data/&lt;project&gt;/&lt;username&gt;/R/Library\n\n\n2. Set the R Library Path\nSet the R_LIBS environment variable to point to your local library directory. This tells R where to install and load packages.\nexport R_LIBS=/g/data/&lt;project&gt;/&lt;username&gt;/R/Library\n\n\n3. Load Required Modules\nBefore running R, load the required software modules. You can usually determine these by checking the installation documentation for the R package you are trying to install. For R version 4.5.0 and to use the intel compiler to build the libraries, use:\nmodule load R/4.5.0\nmodule load intel-compiler-llvm/2025.0.4\nmodule load intel-mkl/2025.0.1\nNOTE: If you are planning to use the intel compiler as above make sure you use the intel compiler version that matches the R version you are planning to use. If you take a look at the file /apps/R//README.nci (use cat /apps/R/&lt;version&gt;/README.nci and replace &lt;version&gt; with the R version you are loading) you will see which modules were used during the R build. The loaded intel compiler module must match this.\n\n\n4. Install R Packages from package repository\nLaunch the R command-line interface:\nR\nIn the R prompt, install packages using install.packages(). The command below will install the package from the University of Melbourne CRAN mirror, use another location if your package requires it. Replace &lt;package&gt; with the name of your package.\ninstall.packages(\"&lt;package&gt;\", repos = \"http://cran.ms.unimelb.edu.au/\")\nFor example to install the commonly used R package “rstan”:\ninstall.packages(c(\"StanHeaders\", \"RcppEigen\", \"rstan\"),\n                 repos = \"http://cran.ms.unimelb.edu.au/\")\n\n\n5. Verify the Installation\nTo check if a package was installed successfully, use the library() function. Again replace &lt;package&gt; with the name of your package:\nlibrary(\"&lt;package&gt;\")\nIf no error is returned, the package should have been installed correctly.\nFinally to ensure your PBS job scripts use your locally installed R packages, always remember to include include the same\nexport R_LIBS=/g/data/&lt;project&gt;/&lt;username&gt;/R/Library\nline from step 2 above in your job script along with the standard module load commands for any dependencies.\nThese steps should work for most simple R package installations. For further hints for some specific packages, consult the NCI Documentaion.",
    "crumbs": [
      "Working on Gadi",
      "Software"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html",
    "href": "notebooks/12_walltime.html",
    "title": "Working within walltime limits",
    "section": "",
    "text": "In this section, we will discuss ways of adapting your long walltime jobs from Artemis to NCI platforms.\nWatch the pre-recorded session",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html#introduction",
    "href": "notebooks/12_walltime.html#introduction",
    "title": "Working within walltime limits",
    "section": "",
    "text": "In this section, we will discuss ways of adapting your long walltime jobs from Artemis to NCI platforms.\nWatch the pre-recorded session",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html#gadi-walltime-limit",
    "href": "notebooks/12_walltime.html#gadi-walltime-limit",
    "title": "Working within walltime limits",
    "section": "Gadi walltime limit",
    "text": "Gadi walltime limit\nThe maximum walltime permitted on any of the Gadi HPC queues is 48 hours. In some cases, the walltime may be less (for example when requesting large numbers of nodes, or on copyq). See the Default walltime limit column of the queue limits tables to discover the maximum walltime that applies according to your resources requested.\nGiven that Artemis has much longer maximum walltimes, we understand this may generate some apprehension. Staff at both NCI and SIH can support you in adapting your workflows to NCI if you are still having difficulty after reviewing the suggestons below.\nIn short, there are 3 options to adapting a long-running Artemis workflow to NCI:\n\nSplit your single large job Artemis into a series of smaller jobs on Gadi\nUse NCI’s Nirin cloud instead of Gadi\nSpecial exception to the Gadi walltime limit granted on a case-by-case basis",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html#option-1.-split-or-checkpoint-your-job",
    "href": "notebooks/12_walltime.html#option-1.-split-or-checkpoint-your-job",
    "title": "Working within walltime limits",
    "section": "Option 1. Split or checkpoint your job",
    "text": "Option 1. Split or checkpoint your job\nThere are many advantages to splitting your job up into smaller discrete chunks.\n\nCheckpointing: if one of your jobs in a series fails, you only need to resubmit that discrete job script, rather than either the whole job or some finnicky “hashed out” version of your very long and complex workflow script. This simplifies debugging and rerunning, saves you hands-on time and walltime, minimises errors, and saves KSU\nEase of code maintenance: changing part of workflow, for example adjusting parameters, input files or software versions, is far simpler to implement for shorter chunks of code than it is for a long and complex code with many steps\nEase of benchmarking: Different stages of a complex workflow typically have different compute requirements, for example some long running single core tasks coupled with some GPU tasks, some high memory, some high CPU tasks etc. Benchmarking is more straightforward and informative when performed on discrete workflow chunks.\nGreater job efficiency: By benchmarking and optimising the resource configurations for each stage of the workflow, the series of jobs can be placed on an appropriate queue, and will not be reserving (and being charged for) unused resources. This will reduce KSU usage and resource wastage.\nShorter queue times: Requesting resources for a shorter walltime will result in a shorter queue time. The NCI scheduler is geared towards favouring ‘wider and shorter’ jobs, ie more CPUs/nodes for less time, over ‘taller and slimmer’ jobs (ie fewer CPUs/nodes for a longer time). For example a job may queue for less time if it requests 48 CPU for 1 hour, compared to 1 CPU for 48 hours. Of course the queue is highly dynamic and this cannot be predicted or calculated ahead of time, but in general, shorter walltimes will lead to shorter queue times.\n\nAt the end of this section we will demonstrate a handful of examples of real long-running Artemis workflows that have been adapted to fit within Gadi’s shorter maximum walltime.",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html#option-2.-use-nirin",
    "href": "notebooks/12_walltime.html#option-2.-use-nirin",
    "title": "Working within walltime limits",
    "section": "Option 2. Use Nirin",
    "text": "Option 2. Use Nirin\nYour NCI KSUs can be used on Nirin as well as Gadi. Nirin has the advantage of theoretically infinite walltime, along with internet access which is another limitation of the Gadi compute queues.\nAs such, Nirin presents an easily accessible solution for users whose jobs are irreconcilably affected by the walltime and lack of internet access aspects of Gadi.\nThe Nirin quickstart guide walks you through the process of setting up your instance, including easy to follow screenshots for each step.",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html#option-3.-gadi-special-walltime-request",
    "href": "notebooks/12_walltime.html#option-3.-gadi-special-walltime-request",
    "title": "Working within walltime limits",
    "section": "Option 3. Gadi special walltime request",
    "text": "Option 3. Gadi special walltime request\nIf your job cannot be split/checkpointed into a series of shorter jobs, and the Nirin flavours are not suited to your compute needs, you can make a request to NCI for an increase to the walltime. NCI will ask you to provide details of your job including the relevant code saved on Gadi, as well as a description of why you require a lift to the walltime for this particular job.\nFrom the Gadi queue limits page:\n“If a higher limit on core count (PBS_NCPUS) and walltime is needed, please launch a ticket on NCI help desk with a short description of the reasons why the exception is requested. For example, a current scalability study suggests linear speedup at the core count beyond the current PBS_NCPUS limit. We will endeavour to help on a case-by-case basis”",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/12_walltime.html#examples-of-splitcheckpointed-jobs",
    "href": "notebooks/12_walltime.html#examples-of-splitcheckpointed-jobs",
    "title": "Working within walltime limits",
    "section": "Examples of split/checkpointed jobs",
    "text": "Examples of split/checkpointed jobs\n\nExample 1: A workflow with multiple discrete commands\nIn the field of genomics, the raw data is processed through a series of steps before the final output files are produced. Many groups perform all of these steps within a single, multi-command job, in order to only have to run one job to perform all the work.\nBy splitting apart each of these steps so that each is its own job, we can improve code manageability, reduce walltime, and increase overall processing efficiency.\nWhile this does require some extra effort in terms of submitting multiple processing jobs rather than just one, the benefits described above far outweigh this. The burden of multiple job submission can be ameliorated by parallel processing per sample, and even further by a workflow manager such as Nextflow. For an example Nextflow genomics processing workflow, view this repository, and for parallel jobs on Gadi, see this section.\nIn the below Artemis script example, a samplesheet is read in containing metadata from about 10 samples to be analysed. Each sample has one pair of raw ‘fastq’ input files that are processed through an analysis loop containing 4 steps:\n\n\n\n\n\n\n\n\n\n\nStep\nTask\nInput\nWalltime (hrs)\nCPUs used\n\n\n\n\n1\nQuality check\nRaw data\n2\n2\n\n\n2\nMap to reference\nRaw data\n7\n24\n\n\n3\nRecalibration metrics\nOutput of step 2\n5.5\n1\n\n\n4\nApply recalibration\nOutput of steps 2 + 3\n8.5\n1\n\n\n\nThe total walltime is 23 hours per sample, so the requested walltime in the below script is 240 hours (10 samples x 24 hours per sample)\nThere are multiple inefficiencies within this method, giving rise to an inflated walltime requirement of ~ 1 day per sample plus a very low overall CPU utilisation of the job.\n\nNow compare the above to a Gadi workflow, where each of these 4 steps are separated into their own job, with appropriate resource requests per job.\nJob 1: Quality check\nThe fastQC tool can only run one thread per file. If you provide multiple files through globbing, and provide multiple CPUs to the -t flag, it will process as many files at a time as the value you have provided to -t.\nSo for the current example with 10 samples each with a pair of files, we have 20 files and can run this section of the analysis workflow with 20 CPU. In this way, 100% of the 20 CPU requested are utilised, unlike the Artemis script above, where only one sample’s fastq files at a time could be analysed and thus used only 8.3% (2 CPU used of 24 requested).\n\nJob 2: Map to reference\nThe bwa tool can multi-thread, and tool benchmarking in peer-reviewed literature shows almost perfect scalability up to a thread count of 36. Gadi normal queue has 48 CPU per node, so you could run this job with 48 CPU, assigning 36 CPU to the mapping and 12 CPU to the piped sort command.\nThe key detail is to map each sample’s raw data as it’s own distinct job, instead of looping over each sample in series like the demo Artemis script. On Artemis, we can do this with job arrays but these are not available on Gadi. NCI and SIH recommend the use of nci-parallel (a custom wrapper utility for OpenMPI) for repeated runs of the same job script - see parallel jobs on Gadi for more details.\nTo avoid complicating this walltime section, we will provide an example of using a simple for loop for job submission. NOTE: loops should ONLY be used for a VERY SMALL NUMBER OF JOBS, and always include a sleep in the loop! NCI does monitor the login nodes and serial offending with long for loops will be targeted!\nNote that the directive for job name is provided on the command line as an argument to qsub, the sample metadata is provided with the qsub -v varname=\"varvalue\" syntax, and a 3-second sleep is used to avoid over-loading the job scheduler.\nScript:\n\nTo submit 10 samples as separate jobs:\nwhile read SAMPLE FC LN LIB PL CN\ndo\n  qsub -N map-${SAMPLE} -v SAMPLE=\"${SAMPLE}\",FC=\"${FC}\",LN=\"${LN}\",LIB=\"${LIB}\",PL=\"${PL}\",CN=\"${CN}\" step2_map.pbs \n  sleep 3\ndone &lt; my_samples_metedata.txt\nJob 3 and 4: Recalibration metrics and apply recalibration\nNote from the table above that these two steps do not multi-thread, and both have long walltimes. If you require a task like this in your workflow, it’s critical to interrogate the tool documentation for ways to increase throughput and efficiency.\nWithin this tool’s guide, we find there is a -L interval flag, which allows the tool to operate over discrete intervals of the reference file, rather than scanning the sample data over the whole reference file in one long running single-CPU task. The smaller the interval, the faster the run time, and the resultant output files are merged. This is an example of scatter-gather parallelism, where smaller sub tasks are scattered (distributed across the compute cluster) and then gathered (in this case, merged) to massively speed up a ‘tall and slim’ job (few resources consumed for a long time) into a ‘short and wide’ job (many resources consumed for a short time). Introducing parallelism into your job is crucial to get the most out of HPC.\nSince this section is not a specialised bioinformatics training, we will not go into details for this tool here, but instead provide the main overview of steps and how with a bit of extra work, massive walltime savings can be made.\nSteps 3 and 4 from the Artemis workflow are now executed as 5 jobs:\n\nSplit the reference file into intervals using the tools’s split intervals function\nRun step 3 over each interval for each sample as a separate job. For 32 intervals and 10 samples, that is 32 * 10 = 320 single-CPU jobs. To do this, we would use Open MPI via nci-parallel\nMerge the 32 outputs per sample into a single per-sample file with the tool’s merge function, using nci-parallel to launch the 10 sample * 1 CPU jobs\nRun step 4 over each interval for each sample as a separate job, using the merged output of step 3, another 32 * 10 = 320 single-CPU jobs launched in parallel by nci-parallel\nMerge the 32 outputs per sample into one final output file per sample with the tool’s merge function, using nci-parallel to launch the 10 sample * 1 CPU jobs\n\nAs you can see, our workflow which was one long-running single job with very poor overall CPU utilisation has now been split into 7 jobs. This may sound tedious, yet the massively improved walltime and CPU utilisation will pay off, and you will get to your results in a much faster turnaround time with fewer KSU expended. In this example, walltime of 240 hours has been reduced to 10 hours!\n\n\n\n\n\n\n\n\n\n\n\nStep\nTask\nInput\nWalltime (hrs)\nCPUs used per job\nCPUs total\n\n\n\n\n1\nQuality check\nRaw data\n2\n20\n20\n\n\n2\nMap to reference\nRaw data\n7\n48\n480\n\n\n3\nSplit intervals\nReference file\n&lt;1\n1\n1\n\n\n4\nRecalibration over intervals\nOutput of steps 2 + 3\n&lt;1\n1\n320\n\n\n5\nMerge recalibration tables\nOutput of step 4\n&lt;1\n1\n10\n\n\n6\nApply recalibration over intervals\nOutput of steps 2 + 5\n&lt;1\n1\n320\n\n\n7\nMerge recalibrated final output\nOutput of step 6\n&lt;1\n1\n10\n\n\n\n\n\nExample 2: A long running command with innate checkpointing\nMany commonly used tools have a built in option for saving the state of the process to a local file after a specified number of iterations in a repetitive task. This local file can be reloaded from disk and then the process can restart from where it previously left off. This is often called ‘checkpointing’. It’s a good idea to check the documentation of the tool you are using to see if it has some kind of checkpointing functionality built in since this greatly simplifies splitting up long running tasks into smaller chunks.\nIt is also recommended practice to use some kind of checkpointing because that way you can have a look at the output of jobs mid-way through to ensure that things are progressing as expected. Jobs can then be restarted with different parameters in case of unwanted results, thus saving resource allocations on your project.\nCheckpointing is often very simple to implement in iterative parameter minimisation tasks such as machine learning pipelines that train or fine-tune a model. As such most machine learning packages will have options for checkpointing built-in. If you are building your own machine learning pipeline, simple instructions can be found for how to implement checkpointing in either TensorFlow here or PyTorch here.\nIn this example we’ll demonstrate a script with checkpointing that comes from the SIH developed aigis package. aigis is a tool that fine-tunes a detectron2 image segmentation machine learning model to detect trees and buildings in aerial imagery datasets.\nAerial imagery datasets can be quite large and fine-tuning detectron2 models using them can take a long time. Because of this the aigis fine-tuning script has a flag that will only let it run for a given number of iterations and save the fine-tuned model to disk. The script can then be restarted with the previous output given as input.\nBelow shows an example that runs the aigis script called fine_tune_detectron2.py on artemis. The PBS script looks like:\naigis_script.pbs\n#! /bin/bash\n\n#PBS -P SIHNextgen\n#PBS -N fine_tune_example\n#PBS -l select=1:ncpus=1:mem=16gb:ngpus=1\n#PBS -l walltime=60:00:00\n#PBS -q defaultQ\n\n...   # Module loading and setup go here\n\n#Actually run the program\nfine_tune_detectron2.py --dataset-name MyDataSet --train-json MyDataSet_Train.json --test-json MyDataSet_Test.json --max-iter 20000 --output-dir /project/aigis/model_fine_tuning\nThe full run of this script with 20000 iterations (--max-iter 20000) takes 60hrs, which is longer than the maximum walltime allowed on gadi, however we can change the number of iterations to 10000 and run the script twice with checkpointing.\nTo do this we would convert the above PBS script above to run on gadi and set the walltime to 31 hours and max-iter to 10000:\niteration1.pbs\n#! /bin/bash\n\n#PBS -P qc03\n#PBS -N fine_tune_example_s1\n#PBS -l walltime=31:00:00\n#PBS -l ncpus=1\n#PBS -l ngpus=1\n#PBS -l mem=16GB\n#PBS -q gpuvolta\n#PBS -l wd\n#PBS -lstorage=scratch/qc03\n\n# NOTE: We use 31 hours waltime rather than 30\n# to allow for overheads in running the script\n\n...   # Module loading and setup go here\n\nfine_tune_detectron2.py --dataset-name MyDataSet --train-json MyDataSet_Train.json --test-json MyDataSet_Test.json --max-iter 10000 --output-dir /scratch/qc3/model_fine_tuning\nThen we can create a second pbs script that will pick up the output of the above and continue running for 10000 more iterations, again allowing half the walltime:\niteration2.pbs\n#! /bin/bash\n\n#PBS -P qc03\n#PBS -N fine_tune_example_s2\n#PBS -l walltime=31:00:00\n#PBS -l ncpus=1\n#PBS -l ngpus=1\n#PBS -l mem=16GB\n#PBS -q gpuvolta\n#PBS -l wd\n#PBS -lstorage=scratch/qc03\n\n...   # Module loading and setup go her - likely the same as above.\n\nfine_tune_detectron2.py --dataset-name MyDataSet --train-json MyDataSet_Train.json --test-json MyDataSet_Test.json --max-iter 10000 --output-dir /scratch/qc3/model_fine_tuning --model-file /scratch/qc3/model_fine_tuning/MODEL_9999.pth\nWe can split this into as many shorter chunks as we would like, each starting from the output of the previous iteration. These can then be run sequentially by adding the flag -W depend=afterok:&lt;jobid&gt; to qsub for each script after the first at the gadi command prompt like so:\n\nSubmit iteration1.pbs at the command line\n\n$ qsub iteration1.pbs\n136554542.gadi-pbs\n\ngadi will return the jobid of the first script: 136554542.gadi-pbs. You will need the jobid of the first script to pass to the second qsub command.\nNow we tell the scond script to only execute after the job in the first script has finished:\n\n$ qsub -W depend=afterok:136554542.gadi-pbs iteration2.pbs\nWhen you run qstat after this you will see the second script will have state H saying it has been held until iteration1.pbs has completed.\nIf you find the task of entering all these qsub commands tedious you can also make a bash script which you only need to run once to submit all the scripts:\nsubmit_jobs_checkpointed.py\n#!\\bin\\bash\n\n# Submit first PBS script and save the run id to JOB1.\nJOB1=$(qsub iteration1.pbs)\n\n# Submit second pbs script and tell it to run only after the first has COMPLETED.\n# Save the second run id to JOB2\nJOB2=$(qsub -W depend=afterok:$JOB1 iteration2.pbs)\nYou can of course chain as many iterations as necessary by continuing the above script.\nPlease be aware when you submit multiple long-running job scripts in this way that you should check the output from time to time to ensure everything is going smoothly, and kill the running script if it isn’t before fixing issues and restarting.\n\n\nExample 3: Add your own checkpointing to existing code\nSometimes, you might be running your own software which has no checkpointing available. In this case it still might be possible to add checkpointing to your code yourself with a minimum of fuss, particularly if the code you are running is a long sequential pipeline where each step depends on the result of the previous one.\nAn effective way to add checkpointing to existing code is by saving the current state of the variables in your code to disk and then reloading them before starting the next step. The process of doing this is called serialisation and many different popular python packages (e.g. Numpy, Pandas) are able to do it with a simple command. Even native python variables (lists dicts etc. can be saved to disk using the pickle module).\nAn example of how you might do this comes from an astronomy pipeline (VAST) which reads thousands of sky images and makes multiple measurements from then followed by associating the stars in them with one another and then generating statistics on each star.\nThis simple pipeline is easily split into multiple steps: 1. Read images 2. Measure stars in images 3. Associate stars with one another between images 4. Generate statistics about the stars.\nHere is a minimal example of the pipeline code between step 1 and 2. Step one is simple a function that returns a pandas DataFrame containing the image information and step 2 is another function that reads that dataframe before measureing stars in the image data from step 1.\npipeline.py\n# Imports and setup at top of pipeline script\n\n...\n\n# Pipeline Step 1: read images\n# Some call to a function that reads images into a pandas dataframe\nimages_df = get_parallel_assoc_image_df(\n                image_file_paths, ...\n            )\n\n...\n\n# Pipeline Step 2: make measurements\n# Call a function that measures stars from our image data in images_df\nsources_df = get_sources(\n                images_df, ...\n             )\n\n...\nThis can be split into python sripts step1 and step2 where the end of step1 involves writing the returned dataframe to disk and then the start of step2 involves reading the result from disk.\nstep1.py\n\n# Pipeline Step 1: read images\n# Some call to a function that reads images into a pandas dataframe\nimages_df = get_parallel_assoc_image_df(\n                image_file_paths, ...\n            )\n\n...\n\nimages_df.write_parquet('images_df.parquet')\nstep2.py\nimport pandas as pd\n\n# Possible config setup here.\n\nimages_df = pd.read_parquet('images_df.parquet')\n\n# Pipeline Step 2: make measurements\n# Call a function that measures stars from our image data in images_df\nsources_df = get_sources(\n                images_df, ...\n)\n\n...\n\npd.write_parquet(...)\nThese scripts can then be run sequentially using the method described in the previous example.\n\n\nExample 4: auto-resume within a persistent session\nPersistent sessions\nNCI have established persistent sessions to enable long running, low-resource tasks such as workflow management tools. We can use a persistent session to run a simple shell script that monitors your job periodically and resubmits the same script until successful completion. Similar to screen or tmux, you can log out of persistent sessions and return later to check on your task.\nWorkflow setup\nThis method requires 2 scripts:\n\nThe shell script that is run within the persistent session\n\n\nThis script submits the PBS job with qsub and monitors with qstat\nUser sets the monitor frequency (no less than 10 minutes per NCI guidelines) and maximum number of resubmissions\nScript will exit when “success” is discovered, or after maximum retries\n\n\nThe PBS job script, which contains directives and the actual commands to run the job. This script may:\n\n\nEmbed the work directly (eg calling the application or task in-line), or\nCall an external script that runs the tool/commands\n\nFor this method to be successful, there must be:\n\nA checkpoint file that is updated by the job as the work progresses\nA method of reading the checkpoint file to determine where to continue from after a walltime failure\nA reliable method of determining “success”\n\nDeterminging previous job success\nRobustly checking for job success is critical for determining whether to re-submit the job. The below example scripts use an overly simplistic method of determining success, by relying solely on a user-defined “success message” and the PBS job exit status. The success message must be generated as standard output by the job and printed to the PBS job log, as well as defined in the auto-resume shell script. There are numerous potential issues with this simple approach, for example if the success message chosen is used elsewhere in the job standard output, or some component of the task fails yet the overall PBS job exits with status zero. For this reason, using thorough, precise success checks tailored to your job are recommended. In addition to checking the job exit status and including a user-defined success message that is unlikely to ever be encountered in other standard output, you might also include checks for expected output files, expected file sizes, expected file contents, etc.\nSimple example auto-resume job\nIn the simple example scripts below, the PBS script contains commands to run 10 iterations of 10 seconds each, requiring a total walltime of 100 seconds. Only 1 minute walltime is requested so we expect to have to run two jobs in series.\nAuto-resume script that submits and monitors jobs:\n#!/bin/bash\n\n# !!!!!! NCI message on qstat frequency: \n# \"Our recommendation is to query your jobs status a maximum of once every 10 minutes, this should be more than enough\"\n\n##### User-defined parameters\n\njob_script=\"checkpointed_job.pbs\" # your job script\nsuccess_message=\"Checkpointed job finished successfully\" # user-defined success message - must be printed by job script\nmax_retries=3 # only resubmit this many times - adjust as needed\ncheck_interval=12  # seconds between checks - DO NOT SET BELOW 600 FOR REAL JOBS\n\n\n##### Begin submission and monitoring\n\nattempt=1\n\nwhile (( attempt &lt;= max_retries )); do\n    printf \"########################################\\n\"\n    printf \"Submitting %s attempt %d of %d - %s\\n\" \"$job_script\" \"$attempt\" \"$max_retries\" \"$(date)\"\n    \n    full_job_id=$(qsub \"$job_script\")\n    if [[ -z \"$full_job_id\" ]]; then\n        echo \"Failed to submit job. Exiting.\"\n        exit 1\n    fi\n\n    job_id=$(echo \"$full_job_id\" | sed -E 's/^([0-9]+)\\..*/\\1/')\n\n    printf \"\\tJob ID: %s\\n\\tWaiting for job to exit...\\n\" \"$job_id\"\n\n    while qstat \"$job_id\" &&gt;/dev/null; do\n        sleep \"$check_interval\"\n    done\n\n    log_file=$(ls *.o\"$job_id\" 2&gt;/dev/null)\n\n    if [[ ! -f \"$log_file\" ]]; then\n        echo \"Expected output file $log_file not found. Skipping check.\"\n        (( attempt++ ))\n        continue\n    fi\n\n    printf \"\\tPBS output log file %s detected at %s\\n\" \"$log_file\" \"$(date)\"\n\n    # Extract exit status from PBS output log\n    status=$(grep \"Exit Status\" \"$log_file\" | awk -F : '{print $NF}' | sed 's/ //g')\n\n    # Check for success message AND exit status == 0\n    if grep -q \"$success_message\" \"$log_file\" && [[ \"$status\" -eq 0 ]]; then\n        printf \"\\n\\t---&gt; Job %s completed successfully:\\n\\tContains \\\"%s\\\" and has exit status %s.\\n\\n\" \\\n               \"$job_id\" \"$success_message\" \"$status\"\n        exit 0\n    else\n        printf \"\\n\\t---&gt; Job failed or incomplete. Exit status: %s. Resubmitting...\\n\\n\" \"$status\"\n        (( attempt++ ))\n    fi\ndone\n\necho \"Reached maximum attempts ($max_retries). Job $job_id did not complete successfully.\"\nexit 1\nExample job script:\n#!/bin/bash\n\n#PBS -P er01\n#PBS -N resubmit-test\n#PBS -l walltime=00:01:00\n#PBS -l ncpus=1\n#PBS -l mem=1GB\n#PBS -W umask=022\n#PBS -q expressbw\n#PBS -l wd\n#PBS -l storage=scratch/qc03\n\ncheckpoint_file=progress.chk\ntarget_steps=10 \nsleep=10 \n\nif [[ -f \"${checkpoint_file}\" ]]\nthen\n    start_step=$(( $(cat ${checkpoint_file}) + 1 ))\n    printf \"Resuming from step ${start_step}\\n\"\nelse\n    start_step=1\n    echo Starting from step 1\nfi\n\n# Simulated workload\nfor (( step=start_step; step&lt;=target_steps; step++ ))\ndo\n    printf \"Processing step ${step} of ${target_steps}\\n\"\n    sleep \"${sleep}\"\n\n    echo \"$step\" &gt; \"${checkpoint_file}\"\n    sync\ndone\n\n# Print \"success message\" to PBS job log:\necho \"Checkpointed job finished successfully\"\nRunning the example job\nFirst, logon to Gadi then follow the instructions on the Gadi user guide to start a persistent session:\n#persistent-sessions start -p &lt;project&gt; &lt;session-name&gt;\npersistent-sessions start -p qc03 demo-resume\nFollow the instructions in the standard output to connect to your persistent session (\\&lt;session-name\\&gt;.\\&lt;nciUserId\\&gt;.\\&lt;project\\&gt;.ps.gadi.org.au):\nssh demo-resume.cew562.qc03.ps.gadi.nci.org.au\n#Warning: Permanently added '[demo-resume.cew562.qc03.ps.gadi.nci.org.au]:2222' (ED25519) to the list of known hosts.\nNotice that your command prompt now shows the session name, rather than the Gadi login node.\nRegardless of your working directory when starting the session, you will be placed back in home. Change to your working directory, and then run the auto resume shell script with nohup. Using ‘no-hangup’ helps ensure your shell script will continue to run even after you exit the persistent session. When running in nohup, commands such as control+C and control+D will not terminate the script execution.\nIn the screenshot below, after the auto-resume shell script is run, control+D is executed to log out of the session. Reconnection to the persistent session over ssh followed by a process check with ps shows that the process with process ID (PID) 490 is still running. After the PID 490 is no longer running, the nohup log shows that the job was submitted a total of 2 times to achieve successful completion.\n\nAs expected, there are 2 sets of PBS job logs, one that failed on walltime and one containing the success message and an exit status of zero:\n\nAdapting the example to your own work\n\nPlease observe the request by NCI to not run qstat queries more frequently than every 10 minutes. Increase the check_interval value from the example of 12 to AT LEAST 600\nIncrease the maximum number of resubmission attempts to suit your needs\nReplace the example job script name with the name of your script\nUpdate the simple success check to something more robust and suited to your job, as described earlier in this section\nEnsure checkpointing is correctly set up such that each resubmission with the same script can continue from the last checkpoint",
    "crumbs": [
      "Running a job",
      "Working within walltime"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html",
    "href": "notebooks/13_parallel_jobs.html",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "",
    "text": "Unlike Artemis, job arrays are not permitted on Gadi. In this section, we will discuss how you can run embarrassingly parallel jobs on Gadi using the nci-parallel utility in place of the Artemis job array method.\nWatch the pre-recorded session.",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#introduction",
    "href": "notebooks/13_parallel_jobs.html#introduction",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "",
    "text": "Unlike Artemis, job arrays are not permitted on Gadi. In this section, we will discuss how you can run embarrassingly parallel jobs on Gadi using the nci-parallel utility in place of the Artemis job array method.\nWatch the pre-recorded session.",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#embarrassingly-parallel-jobs-on-gadi",
    "href": "notebooks/13_parallel_jobs.html#embarrassingly-parallel-jobs-on-gadi",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Embarrassingly parallel jobs on Gadi",
    "text": "Embarrassingly parallel jobs on Gadi\nA parallel job is a job that is broken down into numerous smaller tasks to be executed across multiple processors, nodes, or cores simultaneously to speed up computations. For example, operating the same analysis over different input files, over different parameter values, or by dividing a large computational task into smaller subtasks that run concurrently, using shared memory or distributed computing.\nEmbarrassingly parallel jobs are a specific type of parallel job where the tasks are completely independent of each other and do not require inter-process communication or shared memory. These types of jobs can be trivially parallelized by running each task separately on different cores or nodes.\nOn Artemis, we could use the #PBS -J &lt;range&gt; directive to submit arrays of embarrassingly parallel tasks. On Gadi, job arrays are not supported. In order to simplify embarrassingly parallel jobs without the use of arrays, NCI have created a tool nci-parallel to be used with OpenMPI for distributing parallel tasks across compute resources on a Gadi queue, within a single PBS job.",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#using-nci-parallel",
    "href": "notebooks/13_parallel_jobs.html#using-nci-parallel",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Using nci-parallel",
    "text": "Using nci-parallel\nUse of this utility will typically require 2-4 job files:\n\nPBS script\n\n\nContains PBS directives to launch the parallel job\nSets the number of CPUs to assign to each task\nSpecifies the inputs (job file 2) and/or task script (job file 3)\nRuns openmpi and nci-parallel to distribute concurrent tasks across the resources requested in the directives\n\n\nInput arguments file or command file\n\n\nA text file that provides a new set of arguments/commands per line\nEach line of the input arguments file/commands file is used as the input for a separate parallel task\n\n\n(optional) Task script\n\n\nThe script file (eg Python, bash) that contains the commands to run the job\nUses the arguments provided by the input arguments file/commands file\nFor some tasks where the command fits on a single line, a task script may not be required and the commands file may be sufficient\n\n\n(optional) Script to make the input arguments/commands file\n\n\nFor complex arguments/commands files, where the creation of these cannot easily be achieved with a simple for loop or similar on the command line, an optional fourth script may be required to make the input arguments/commands file\nFor example, where the inputs are derived from a complex metadata file or require some further manipulation prior to generating the input/arguments list\nA make inputs script that is saved alongside the parallel script collection for the job is useful for reproducibility and portability\n\nNCI provides an example where the input to the task script (in this case, named test.sh) is a single numeric value. The command test.sh &lt;value&gt; is included in the input argument file, as many times as needed for the job (in this case, 1000). Another way to do this (demonstrated in the simple example further below) is having an input argument list that does not include the name of the script file. The name of the script file is instead included within the PBS script. Both methods achieve the same result, the user can choose whichever they prefer.\nOption 1: task script call with arguments is provided in a command file\n\nOption 2: task script call is combined with arguments by the PBS script\n\nOption 3: no task script, simple one-line command and arguments provided in a command file\n\nBefore submitting your parallel job, always check that the task script reads in the input arguments correctly and that the binding of tasks to resources is correct.\n\nSimple example\nBelow is a trio of job files to run a very simple demonstration of nci-parallel.\nEach parallel task requires only 1 CPU. The input file demo.inputs contains 12 input arguments, and the PBS directives request 12 CPU, so this means all tasks will execute concurrently, ie at the same time. If 6 CPUs were requested for the job, then the first 6 tasks would run, and the next tasks would be executed as those initial tasks completed, ie only up to 6 tasks could run concurrently.\nJob file 1: PBS script that launches the parallel tasks\n#!/bin/bash\n\n#PBS -P qc03\n#PBS -N demo\n#PBS -l walltime=00:05:00\n#PBS -l ncpus=12\n#PBS -l mem=12GB\n#PBS -l wd\n#PBS -q normal\n#PBS -W umask=022\n#PBS -o ./demo_parallel.o\n#PBS -e ./demo_parallel.e\n\nset -e\n\n# Always load nci-parallel here\nmodule load nci-parallel/1.0.0a\n\n# Also load tool modules here, they will be inherited by the parallel tasks\n# modules... \n\n# Number of CPUs per parallel task, also inherited by the parallel tasks\n# BE SURE TO MAKE YOUR COMMANDS/TASKS SCRIPT MAKE USE OF THIS MANY CPU\nNCPUS=1\n\n# The task script to run\nSCRIPT=./demo.sh\n\n# The input arguments list\nINPUTS=./demo.inputs\n\n#########################################################\n# configure parallel\n#########################################################\n\n# Number of concurrent tasks to run per node\nM=$(( PBS_NCI_NCPUS_PER_NODE / NCPUS )) \n\n# Combine the task script with the arguments (makes a command file)\nsed \"s|^|${SCRIPT} |\" ${INPUTS} &gt; ${PBS_JOBFS}/input-file\n\n# Run the number of tasks per node times the number of nodes concurrently with mpi\nmpirun --np $((M * PBS_NCPUS / PBS_NCI_NCPUS_PER_NODE)) \\\n        --map-by node:PE=${NCPUS} \\\n        nci-parallel \\\n        --verbose \\\n        --input-file ${PBS_JOBFS}/input-file\nObserve the equations in the script above:\nM=$(( PBS_NCI_NCPUS_PER_NODE / NCPUS )) \nM = 48 / 1\nM = 48\n\nThe value of M sets the maximum number of tasks that can run per node in the requested queue, based on the chosen provision of CPU per task\nPBS_NCI_NCPUS_PER_NODE is an environment variable that is set based on the queue, in this case the normal queue which has 48 CPUs per node\nThe value of the bash variable NCPUS is set by the user in the PBS script, and is distinct from the environment variable PBS_NCPUS which is the total CPUs requested by the job in the directive #PBS -l ncpus=&lt;num&gt;\n\nmpirun --np M * PBS_NCPUS / PBS_NCI_NCPUS_PER_NODE\nmpirun --np 48 * 12 / 48\nmpirun --np 12\n\nmpirun is used to run parallel jobs with the MPI (Message Passing Interface) framework. It allows you to execute the program across multiple processors or nodes. When you load the nci-parallel/1.0.0a module, openmpi v. 4.1.0 is loaded as a requirement\n--np specifies the number of processes for MPI to run in parallel. This is determined by the number of tasks that can run on each node (M), and the number of nodes requested (requested CPUs divided by number of CPUs per node in that queue)\n\n--map-by node:PE=${NCPUS}\n\nmapping by node means that processes will be distributed evenly across all nodes requested by the job. Mapping by NUMA node will be described in a later section\nPE=${NCPUS} (‘processing elements’) indicates that each distributed task will be allocated ${NCPUS} CPUs, which is a user-defined variable\n\nnci-parallel \\\n  --verbose \\\n  --input-file ${PBS_JOBFS}/input-file\n\nnci-parallel is the application being run by mpirun\n--verbose and --input-file are parameters for the nci-parallel tool, run nci-parallel -h to see other available parameters/flags\nThe input file is where nci-parallel reads the task commands from; each line of the file is treated as a separate command to be ‘farmed out’ as an independent parallel task to the resources reserved for the job. If the number of lines in the input file exceeds the number of processes MPI can run in parallel/simultaneously (ie, the value given to mpirun --np &lt;value&gt;), the commands are assigned to resources in order, starting from line 1, until all resources are in use. As a running task completes, the next task in the list is assigned to those now-free resources, until either all input lines have been assigned to resources and the tasks complete, or the job meets a fatal error (such as out of walltime).\n\nTest your understanding:\n\nHow many tasks could run per normal node if NCPUS was set by the user as 6?\n\n\n\n\n\n\n\nNoteNumber of tasks per normal node\n\n\n\n\n\nM=$(( PBS_NCI_NCPUS_PER_NODE / NCPUS )) \nM = 48 / 6\nM = 8\n8 6-CPU tasks per normal node\n\n\n\n\nHow many 6-CPU tasks could run concurrently if this job requested 10 normal nodes?\n\n\n\n\n\n\n\nNoteNumber of tasks running concurrently on 10 normal nodes\n\n\n\n\n\nmpirun --np M * PBS_NCPUS / PBS_NCI_NCPUS_PER_NODE\nmpirun --np 8 * 480 / 48\nmpirun --np 80\n80 concurrent 6-CPU tasks on 10 normal nodes\n\n\n\nJob file 2: the input arguments file\nA plain text file that contains the arguments for each parallel task, one per line. Since this is a simple file it can be made easily on the command line. More complex arguments/commands files may require a helper make inputs script as a fourth member of the file set for the parallel job.\nThe input arguments file for this demonstration job was simply made by:\nshuf -i 1-100 -n 12 &gt; demo.inputs\n51\n20\n16\n92\n12\n99\n54\n48\n6\n44\n36\n56\nJob file 3: the task script\nA script that reads in the single argument and executes the command. Note: this script must be executable! If not, the job will fail with permission denied errors.\n#!/bin/bash\n\nargument=$1\n\necho My favourite number is ${argument}\nNow to run the parallel job:\nqsub demo_run_parallel.pbs\nNote that unlike job arrays, this is a single job so when you run qstat, you will not see the square brackets denoting a job array that you are familiar with on Artemis. To qstat, it will appear as any other non-parallel job.\n\n\nChecking a completed parallel job\nOnce the job completes, check the job logs. Since the task script in this example did not redirect the output, it was by default printed to the PBS .o job log:\n\nNote that the order of the output differs from the order of the input. This is because the tasks are independent and executed in parallel - so it is not good practice to allow important output to be sent only to the .o log file like this! Always redirect your output!\nNow for the .e log:\n\nEach parallel task has a line containing information including the ID of the node it was run on, the individual task exit status, and the command that ran the task.\nAlways check the exit status from the PBS .o log AND the per-task exit statuses from the .e log! An exit status of 0 for the parent job does not mean all tasks completed without error. And of course, as always, check the outputs, as an exit status for both parent job and task still does not necessarily indicate a successful job.\nCount the number of tasks with exit status 0 - this should equal the number of lines in your input arguments/command file:\ngrep \"status 0\" demo_parallel.e | wc -l\nIf the number if tasks with exit status 0 is less than the number of inputs, either the job has met a fatal error (and the PBS .o log will have a non-zero exit status), or some individual tasks have failed. If all tasks have started, they will have an exit status in the .elog, so you can retrieve the failed tasks to a new input file to be resubmitted (after first troubleshooting the error). For example:\n grep -E \"status [1-9]\" demo_parallel.e | awk -F\"status [0-9]+: \" '{print $2}' &gt; failed.input\nIf however the tasks didn’t start for example the job exceeded walltime before they were allocated to resources, they won’t have an entry in the .elog:\n\nTo capture failed tasks as well as tasks that did not start, you would need to extract the lines from the original input file that do not have ‘exited with status 0’ within the .e log.",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#checking-the-parallel-job-before-submission",
    "href": "notebooks/13_parallel_jobs.html#checking-the-parallel-job-before-submission",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Checking the parallel job before submission",
    "text": "Checking the parallel job before submission\nBefore submitting your parallel job, always check that the:\n\nTask script reads in the input arguments correctly\nTask executes correctly with those provided arguments on the compute node\nBinding of tasks to resources is correct and appropriate for your job\n\n\n1. Check that the task script reads in the input arguments correctly\nTo check that the task script reads in the arguments correctly, this need not be done on the compute nodes. You can run a quick test on the login node where your task script prints out the arguments it reads in from the input arguments file then exits before running any analysis.\nYour code may already have a test condition; if so please run that prior to submitting the full parallel job.\nIf not, you can easily test the arguments for any type of script with the simple method below:\n\nThe task script is edited to include print statements that check the arguments read in from the inputs file, followed by an exit command\nRun the task script on the login node, providing one line of the input arguments file as command-line argument\n\n1. Edit the task script to print out the arguments that are read in\n\n2. Run the task script on the login node with one line of the inputs\n\nOnce this check has proved successful, go on to check a single task running on the compute node.\n\n\n2. Check that the task executes correctly with those provided arguments on the compute node\nThis second test will also help you with the third and final check of binding tasks to resources by the mpirun command.\nIdeally, you have already performed benchmarking to determine appropriate resources for your job. If your tasks are long-running, you could choose to either subset the input for the test task, or allow the test task to run for just enough time to determine that the inputs and scripts are set up correctly. If you have not done prior benchmarking on a subset followed by a full task at the chosen resources per task, it’s preferable here to allow that test task to complete, even if it is long running. Allowing one task to fail now can give you the opportunity to debug and prevent your whole parallel workflow from failing, potentially costing many KSU - and your time and frustration!\nIn the below example, each task requires 6 CPU for approximately 5 minutes walltime.\nFirst, delete or hash out the exit command if it is still present from the previous checking of input arguments. Then:\n\nTake the first line of the input arguments file as a ‘test’ inputs file\nEdit the PBS script to point to the test input file, and reduce CPU and memory to what is required for one task\nSubmit the single task to the scheduler with the qsub command\n\n1. Create a single line test input\n\n2. Edit the PBS script to point to the test input and reduce resources to one task\n\n3. Submit the single task to the scheduler\nqsub demo_run_parallel.pbs\nNote that the mpirun command and all other aspects of the PBS script remain the same as for the full parallel job.\nOnce the job completes, check the exit status in both logs as well as the job outputs.\n\n\n3. Check the binding of resources to parallel tasks\nThis refers to how the tasks are distributed across resources by OpenMPI. At this point in your parallel job setup, you should have a good understanding of what resources are required per task from benchmarking, and have determined that your job scripts are set up correctly for an nci-parallel job.\nHere we will review the mpirun command a bit more closely and look at two alternative ways you can map/bind tasks to resources.\nThe --map-by parameter in the mpirun command can map by node, socket, or NUMA nodes.\nThe example on the NCI page shows --map-by ppr:&lt;value&gt;:NUMA:PE:&lt;value&gt; and the simple example above shows --map-by node:PE=&lt;value&gt;. What’s the difference?\nppr stands for processes per resource, and specifies the number of tasks assigned to that resource, where ‘resource’ may be node, socket or NUMA node. A NUMA (Non-Uniform Memory Access) node/domain is a physical unit consisting of one or more CPU cores and the memory directly attached to them. CPUs can access the memory that is within the same NUMA node faster than memory from other NUMA nodes.\nThe mpirun command in the NCI example:\nPBS_NCPUS=384\nexport ncores_per_task=4\nexport ncores_per_numanode=12\nmpirun -np $((PBS_NCPUS/ncores_per_task)) \\\n    --map-by ppr:$((ncores_per_numanode/ncores_per_task)):NUMA:PE=${ncores_per_task}\nExpands to:\nmpirun -np 384/4 --map-by ppr:12/4:NUMA:PE=4\nmpirun -np 96 --map-by ppr:3:NUMA:PE=4\nRemember, --np specifies the number of processes for MPI to run concurrently, in this case, 96 of the 1,000 input tasks can run at a time. Processes (tasks) per resource is 3 and the resource is given as NUMA. So the job will run 3 tasks per NUMA node, each with 4 (PE=4) CPU.\nThe Gadi queue structure page states there are 12 CPUs per NUMA node on the Cascade lake normal queue, so this job fits nicely on that architecture. What if you wanted to run tasks with more CPUs than available on a single NUMA?\nFor example, for 24 CPUs per task, the equation would yield:\nmpirun -np $((PBS_NCPUS/ncores_per_task)) \\\n    --map-by ppr:$((ncores_per_numanode/ncores_per_task)):NUMA:PE=${ncores_per_task}\nmpirun -np 384/24 --map-by ppr:12/24:NUMA:PE=24\nmpirun -np 16 --map-by ppr:0.5:NUMA:PE=24\nThis job would fail with non-zero exit status and display the error “Your job has requested more processes than the ppr for this topology can support”.\nFor jobs where more CPU (or memory) is required per task than exists on a single NUMA node, the --map-by node:PE=&lt;value&gt; method shown in the simple example above can enable this. For tasks of 24 CPU on the normal, memory is obtained across 2 whole NUMA nodes. For tasks of 16 CPU each, CPU and memory is obtained from 2 NUMA nodes, and some NUMA nodes will provide memory and CPU to 2 different tasks. To assess the effect of this memory latency on your workflow, benchmarking is recommended.\nMapping by NUMA node only works if the remainder of ncores_per_numanode divided by ncores_per_task is zero. Assume you have benchmarked that your tasks achieve optimum efficiency with 8 CPU per task on the normal queue with 1 task running per NUMA node. 12/8 leaves a remainder of 4 so this binding is not possible. However, you can achieve the same by assigning ncores_per_task the same value as ncores_per_numanode, ie 12 in this example, to map 1 task per NUMA node. Then, define an additional variable within the PBS script with the value of 8, ensuring that the task script uses this variable, rather than ncores_per_task, as the number of CPUs to utilise.\nWhat is the advantage to mapping by NUMA node? The CPU cores within a NUMA node can access the memory within that NUMA node more quickly than the memory located in other NUMA nodes. Your jobs may have improved performance, especially for memory-intensive applications, if the memory per task is within a single NUMA domain. As always, it is ideal to perform benchmarking to determine the optimal resource configuration for your workflow.\nFor more advanced applications where you are interested in which sockets and CPUs within the node your tasks are bound to, you can add the flag --report-bindings to the mpirun command. The example below shows the output of this flag (within the PBS .e job log) for a parallel job running 6 single-CPU tasks concurrently:",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#cpu-efficiency",
    "href": "notebooks/13_parallel_jobs.html#cpu-efficiency",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "CPU efficiency",
    "text": "CPU efficiency\nBenchmarking should initially be done on the single task, in order to optimise resource requests for the full job. In some cases, CPU efficiency can decline for parallel jobs, and this results in an increased walltime per task and overall SU cost for the job. Where possible, benchmarking tests should also be performed at scale, to assess any decline in performance when the workload is scaled up. This can enable you to further optimise, or allow for the decline in efficiency when requesting walltime, to prevent avoidable job failures due to running out of walltime.\nFor example, you have determined the optimum resources for your tasks to be 12 CPU and 48 GB mem, for just under 20 minutes. You have 1,000 tasks to run. This equates to 250 normal nodes. Prior to submitting a 20 minute 250-node job, you should:\n\nTest 4 tasks in a parallel job - that’s one full node. Does the CPU efficiency remain the same as benchmarked for the single task?\nTest a handful of nodes, say 10 - does the CPU efficiency remain the same as benchmarked, now 40 tasks are running concurrently?\nIf so, go ahead and submit the remaining tasks (remember to remove the tasks already run through this benchmarking, if you have done this on full size inputs). If not, what happened to the walltime? If the efficiency loss was drastic and walltime much worse, consider reoptimising. If the decline was minimal, extrapolate from this to estimate the walltime required for the full job, potentially running a larger task lot first (for example the next 100 tasks in the input argument list) for more accurate extrapolation.\n\nKeep records of the benchmark runs and full run resources for future reference. We recommend the Gadi usage script which will summarise compute resources into a tab delimited table for ease of review and record keeping. If you prefer to rely on your Gadi job logs alone for long term records of your jobs usage, please ensure to back these up to RDS as they will be purged from scratch in time.\n\nThis may sound like a lot of extra work, but it could save a large amount of KSU as well as walltime and your own frustration in the long run. Understanding how well your workload scales to a parallel job is important prior to submitting a large multi-node job like this.",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#walltime-management-and-cpu-utilisation",
    "href": "notebooks/13_parallel_jobs.html#walltime-management-and-cpu-utilisation",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Walltime management and CPU utilisation",
    "text": "Walltime management and CPU utilisation\nThis section descibes one disadvantage of nci-parallel jobs compared to PBS job arrays, and how utilisation can be maximised and walltime minimised with a few management strategies.\nIn PBS job arrays, each subjob of the array is a discrete task consuming a chunk of resources that may or may not be on the same node as other subjobs of the array. When that subjob completes, those resources are returned to the cluster and become available for other jobs. As such, they are no longer consuming CPU hours and accruing resource usage to the user’s account. In contrast, nci-parallel reserves the entire collection of resources for all parallel tasks in the job as a single large chunk. This means that tasks that have completed while other tasks are still running are still consuming those resources (even though they are idle) and thus still being charged for.\nBecause of this, parallel jobs where there is a large distribution of walltime across the tasks are likely to result in poor efficiency and a lot of wasted resources. There are a number of strategies you can employ alone or in combination to mitigate this:\n\nUse the nci-parallel timeout parameter, to cap the amount of walltime per task. This will prevent unexpectedly long-running tasks from dragging out the walltime when all other tasks have completed. These tasks can then be resubmitted separately with longer walltimes.\nRequest less resources than are required for all tasks to run in parallel so the amount of idle CPU while long-running tasks complete is reduced\nUse a loop for jobs with small numbers of parallel tasks, with varying walltime based on expectations for the task, adding a sleep within the submission loop\nSort tasks by expected order of walltime, longest to shortest, and request resources such that less than the total number of tasks are running in parallel\nGroup subsets of parallel tasks by expected walltime, and submit multiple separate parallel jobs, for example instead of one large job, run a “fast”, “medium” and “slow” job\n\nThe wastage of resources that can occur within parallel jobs with unequal task sizes is demonstrated in the figure below. In this example of 5 tasks, applying the second strategy on the list increases CPU utilisation but at the cost of increased walltime. By using both strategy two and strategy four together, the optimal CPU utilisation and walltme is achieved:",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#memory-management",
    "href": "notebooks/13_parallel_jobs.html#memory-management",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Memory management",
    "text": "Memory management\nThe number of CPUs allocated to each parallel task is controlled by the user-defined Bash variable within the PBS script (NCPUS in the map by node example, and ncores_per_task in the NCI map by NUMA example). However, each of these parallel tasks has access to the total amount of memory on the node for jobs &gt;=1 node, or to the job for jobs &lt;=1 node.\nTo avoid memory contention, you should:\n\nBenchmark memory usage: Before running large-scale jobs, benchmark the tasks to understand their memory requirements. Monitor memory usage during execution using nqstat_anu or a more advanced profiling tool, and check the completed job log. Use the peak memory requirement per task as the basis for requesting resources.\nEnforce memory limits per task (if possible): If the tool allows, restrict each task’s memory usage in proportion to its CPU allocation. For example, for a 4 CPU task running on the normal queue, set a per-task memory limit of 4 * 4 = 16 GB within the task script to prevent excessive memory consumption.\n\nIf memory contention occurs, the first tasks to start will consume the available memory, while other concurrently running tasks that started later may run significantly slower or even fail due to memory exhaustion. Depending on the tool and workload, this could result in either individual task failures or a complete job failure.\nNote: Mapping by NUMA node does not prevent CPUs from accessing memory from other NUMA nodes (‘remote memory’).",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#example-parallel-jobs",
    "href": "notebooks/13_parallel_jobs.html#example-parallel-jobs",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Example parallel jobs",
    "text": "Example parallel jobs\n\nExample 1: Genomics\nParallel processing in genomics can facilitate rapid processing of terabytes of raw DNA sequencing data. These workloads are highly amenable to parallel processing for 2 reasons:\n\nThe same processing steps typically need to be run on many different samples\nMost of these processing steps can be further broken apart into smaller subtasks\n\nBy parallelising at the level of sample, we may have say 10 or 100 tasks in a parallel job, depending on the number of samples in the analysis. Yet if we break the processing steps each into smaller subtasks, we can then parallelise to a much higher level of throughput, number of samples * number of subtasks, which can easily order into the thousands.\nOne excellent example for how HPC and distributed computing can massively speed up processing is the case of mapping the DNA sequences to the reference genome. In sequencing projects, each sample may have a few hundred million DNA sequences. Users often map all of these in one job, that can take several hours in a multi-threaded job. However, since each DNA sequence is mapped independently to the reference, we can physically split the input into numerous smaller inputs and perform the mapping in parallel.\nSay we have 100 samples, each with approximately 400 million DNA sequences requiring mapping to a reference file. To parallelise by sample, we could run 100 tasks concurrently. If we split the input into chunks of 10 million sequences, each sample would have 40 inputs, so we could run a parallel job with 100 * 40 =  4,000 tasks.\nAssuming the tasks were benchmarked to perform optimally with 6 normal CPU and required 5 minutes walltime, to run all tasks concurrently would require 6 * 4000 = 24,000 CPU. This equates to 500 nodes, which exceeds the maximum nodes per job of 432 for the normal queue. We can add our benchmarking metrics into the SIH parallel job resource calculator which you can download here to help us select resources for this job.\nBy halving the number of nodes requested to 250, we halve the number of tasks that can run concurrently, so we would need to double the walltime:\n\n250 nodes is a large chunk of resources, so possibly 1,000 tasks concurrently for 20 minutes on 125 nodes would be easier to schedule and result in less queue time. This is a massive speedup compared to the ~ 6-8 hours required to process all 400 million sequences in one task.\nIn practice, scaling this 100-sample mapping to 4,000 embarassingly parallel tasks does take a number of steps - but remember that it’s all worth it to achieve the vast speedup 😃\n\nPhysically split the inputs into numerous smaller files (a 100-task job, parallel by sample)\nCheck the logs and outputs to verify successful split job\nUse a helper make input script to read the per-sample metatdata and combine it with each of the split input files into an input arguments file\nSet up the task script to read the input arguments and run the mapping analysis\nSelect resource directives with the help of the resource calculator, then submit the PBS script with qsub\nCheck the logs and outputs to verify successful mapping job\nMerge the split data to a per-sample output file (another 100-task job, parallel by sample)\n\nFor this workflow, only the metadata and/or make inputs file would be edited each time the analysis was run on a new set of input samples, and the PBS script directives would be adjusted based on the number of tasks to be run. The mapping task script would not need to be edited between repeat jobs at all (aside from updated tool versions/parameters etc). So while it may seem complex to set up, the advantages for speedup, reproducibility and portability will save you much time in the long run. Ensure to back up your job files to RDS to avoid losing your hard work to scratch purge!\n\nArtemis job array vs Gadi nci-parallel job\nWe won’t cover the details of splitting or merging here as that is domain-specific, but we will look at the job files required for the massively parallel mapping job on Gadi, as compared to the same analysis with job arrays on Artemis.\nAssume that the same comma-delimited input arguments file has been created for both the NCI and Artemis job:\n\nNote that the Artemis array job must be submitted 4 times, each with a different set of 1,000 tasks, in line with the maximum number of elements per job array on Artemis.\n\n\nObserve the similarities and differences:\n\nBoth workflows use the same analysis commands - for Gadi, the commands are in a different script to the directives, for Artemis, all is within the PBS script\nBoth workflows read in the same variables from the same input arguments files, but use a slightly different command\nBoth workflows request 6 CPU per task\nThe Artemis memory and CPU directives are per task, where the Gadi memory and CPU directives request the total for all concurrently running tasks\nAll tasks on Gadi are run in the same job, Artemis array requires batching\nSlight differences to the directives, as outlined in the section on Gadi PBS scripts compared to Artemis\nThe Gadi PBS script has the mpirun and nci-parallel command, where the Artemis job array script includes the -J &lt;range&gt; directive\n\nAs you can see from this example, porting an existing Artemis job array to Gadi is straightforward and requires little extra work.\n\n\n\nExample 2: Calculation of pi\nIn this example we’ll run through a simple parallel workflow using a python script that leverages the multiprocessing library for internal concurrency. This script can also be submitted multiple times simultaneously to the queue either via Artemis job array or the Gadi equivalent: nci-parallel. This is a typical case for simulation style workflows, where a single script is run multiple times with varying parameters and each script instance uses multiple cores for its processing.\nThe script whatispi.py implements a monte-carlo estimation of pi by randomly generating points in a box and calculating the fraction of those points that lie within a circle enclosed by it.\n\nThe whatispi.py file is shown below. The higher the number of trials the closer the result should be to the true value of pi. This script is embarassingly parallel in that each instance of it can be run independently, the final estimate of pi across all independent runs is calculated by taking the weighted average of each of the independent estimates.\nwhatispi.py\nimport numpy.random as rng\nimport time\nimport sys\nfrom multiprocessing import Pool\nfrom numpy import power\n\ndef pi_run(num_trials):\n    \"\"\"Calculate pi based on points with a box and circle of radius 1\n    \"\"\"\n    r = 1.0\n    r2 = r * r\n    Ncirc = 0\n    for _ in range(num_trials):\n        x = rng.random()\n        y = rng.random()\n        if ( power(x, 2) + power(y, 2) &lt; r2 ):\n            Ncirc += 1\n    pi = 4.0 * ( Ncirc / num_trials )\n    return pi\n\ndef cycle_pi(trials, noprocs=1):\n    \"\"\"calculate pi based on a set of trials.\n    -- trials: a large number representing points\n    -- noprocs: number of python processes used in simulation\n    \"\"\"\n    print(\"number of processes used: \", noprocs)\n    with Pool(noprocs) as pool:\n        pi_estimates = pool.map(pi_run, trials)\n\n    return pi_estimates\n\nif __name__ == \"__main__\":\n\n    trials = list(map(int, sys.argv[2:]))\n    print(\"estimating pi based on\", trials)\n    print(cycle_pi(trials, int(sys.argv[1])))\nWithin the whatispi.py file above, some basic tools from Python multiprocessing library are used:\nthe Pool(noprocs) object creates a pool of Python processes. noprocs is the number of worker processes to use. If processes is None then the number returned by os.cpu_count() is used. This is the first argument to our script.\nThe pool.map(pi_run, trials) uses the pool to map a defined function (pi_run) to a list/iterator object (trials).\nTo run our estimation script with a pool of 2 CPUs with 3 trials of varying length, we would use:\npython whatispi.py 2 1000 2000 3000\nthis would produce the output:\nestimating pi based on [1000, 2000, 3000]\nnumber of processes used:  2\n[3.124, 3.134, 3.154]\nThe three trials above can be converted into a single estimate by taking the average of the runs weighted by the number of trials. Note that the selection of the pool of CPUs in a single run of the script (2 in this example) is the number of CPUs that will be used for each instance of the script running - in the next section we will run multiple instances of the script each with an independent pool of CPUs.\n\nExample run of whatispi.py using Artemis job array\nIn the implementation of the whatispi.py script above the Pool(noprocs) multiprocessing pool can only request CPUs in the NCI node in which the script is running. To scale out this implementation to larger numbers of CPUs we would run the whatspi.py script multiple times simultaneously across multiple nodes on the cluster. We can easily automate this embarrasingly parallel process using either Artemis job arrays or Gadi nci-parallel. In this section we’ll show a basic setup of this workflow using artemis job arrays and then in the next we’ll explain how to convert the Artemis script to Gadi nci-parallel.\nTo start with we set up a list of parameters we want to pass to the script for both the Artemis and Gadi job (NOTE: We have kept this short to keep the example clear, normally for a large HPC job we might be running with hundereds or even thousands of parameter sets):\njob_params\n2 5000000 1000000\n2 1000000 2000000\n2 2000 4000\n2 1000 2000\nIn this example we will run four instances of the whatispi.py script (for larger lists of parameters this would run across multiple nodes) and each instance will create a multiprocessing pool of 2 CPUs and run two trials with the sizes specified. Note the larger run sizes of the first two instances, we will investigate the implications of this later on.\nFrom some benchmarking investigations we estimate that our longest run in job_params will take ~10 minutes. We can use the parallel resource calculator to estimate the required resources for our run by filling in the red highlighted cells there. Note that in this simple example we only request ~0.17 nodes since that is all we require - normally for large parallel jobs you would require &gt;&gt;1 nodes.\n\nThe resource calculator has estimated that we can run our 4 jobs in parallel with 8 cores and should end up using 4 SU for the run. This is a worse case estimate since we entered 10 minutes per task and some of the runs will be much shorter than this.\nHere is a simple Artemis PBS script that takes the parameter file above and runs it using an array job:\npiDemo-job_array.pbs\n#!/bin/bash\n#PBS -P SIHnextgen\n#PBS -l select=1:ncpus=2:mem=2GB\n#PBS -l walltime=00:20:00\n#PBS -J 1-4\n#PBS -o piDemo^array_index^.o\n#PBS -e piDemo^array_index^.e\n\nmodule load python/3.8.2\n\ncd $PBS_O_WORKDIR\nparams=`sed \"${PBS_ARRAY_INDEX}q;d\" job_params`\npython3 whatispi.py ${params}\nDetails about how the script above is set up are beyond the scope of this guide - they can be found here. In summary, the script will run whatispy.py for the 4 sets of parameters in job_params above. When the jobs are complete the script on Artemis will produce 4 .o and .e files numbered by their job array number. These outputs can then be combined in to a final result during postprocessing.\nNOTE: Each job used separate resources on the queue so the shorter running jobs were not waiting for the longer running to finish.\n\n\nConvert Artemis script to Gadi script\nAs shown in Example 1 it is relatively straightforward to convert the basic Artemis PBS script to a Gadi script. We will use nci-parallel and request 4 subjobs on a single node with 2 CPUs per subjob for our short set of parameters in job_params. We use the same equation in the simple example to define the number of subjobs (which will be 4 for 8 requested CPUs). Here is a Gadi script that performs the same task as the Artemis script above:\npiDemo-nci-parallel.pbs\n#!/bin/bash\n#PBS -P qc03\n#PBS -q normal\n#PBS -l ncpus=8\n#PBS -l walltime=00:20:00\n#PBS -l mem=6GB\n#PBS -l storage=scratch/qc03\n#PBS -l wd\n#PBS -N test_parallel\n#PBS -o piDemo-nci-parallel.o\n#PBS -e piDemo-nci-parallel.e\n\nSCRIPT='python3 whatispi.py'\nINPUTS=./job_params\n\nmodule load nci-parallel/1.0.0a\nmodule load python3/3.8.5\n\nsed \"s/^/${SCRIPT} /\" ${INPUTS} &gt; ${PBS_JOBFS}/input_cmd\n\n# 2 CPUs per subjob\nNCPUS=2\nM=$(( PBS_NCI_NCPUS_PER_NODE / NCPUS ))\n\nmpirun -np $((M * PBS_NCPUS / PBS_NCI_NCPUS_PER_NODE)) \\\n       --map-by node:PE=${NCPUS} \\\n       nci-parallel --input-file ${PBS_JOBFS}/input_cmd \\\n       --timeout 4000 \\\n       --verbose \\\n       -o ./\nNote the main differences between the Artemis piDemo-job_array.pbs and Gadi piDemo-nci-parallel.pbs examples:\n\nIn the PBS directives, the Artemis script requests the number of CPUs and memory per subjob, while the Gadi script requests the complete pool of CPUs and memory across all subjobs.\nThe Gadi script generates a new command file input_cmd by prepending python3 whatispi.py to the list of commands.\nThe Gadi script is run using mpirun with --map-by parameter definining 4 simultaneous jobs each using 2 CPUs. This example would use CPUs from only one node but would use more as the task scales up to a larger list of parameters.\nOutput files are named differently between the two scripts. The -o ./ option to the nci-parallel command puts the output from each subjob into files named stderr.&lt;id&gt; and stdout.&lt;id&gt;. The general output from the job (resource summary, info on failures etc.) is put into the PBS log files piDemo-nci_parralel.o and piDemo-nci-parallel.e.\n\nRunning the Gadi PBS script piDemo-nci-parallel.pbs with the list of parameters in job_params yields the following resource usage summary in piDemo-nci-parallel.o:\n======================================================================================\n                  Resource Usage\n   Job Id:             137235074.gadi-pbs\n   Project:            qc03\n   Exit Status:        0\n   Service Units:      3.19\n   NCPUs Requested:    8                      NCPUs Used: 8\n                                           CPU Time Used: 00:20:51\n   Memory Requested:   6.0GB                 Memory Used: 1.96GB\n   Walltime requested: 00:20:00            Walltime Used: 00:11:58\n   JobFS requested:    100.0MB                JobFS used: 8.05MB\n======================================================================================\nWe have used the 8 CPUs requested for a total of ~12 minutes, which gives 3.19 service units in total. However based on the number of trials of each job in job_params it is likely that the last two shorter jobs have completed much more quickly than the first two. This is an example of the issue described above: In the Artemis job arrays example the resources from the last two jobs would be freed as soon as those jobs complete. This is not the case with nci-parallel jobs where the requested 8 CPUs are retained for the entire length of the running job. In the next section we will look at a way to optimise this workflow on Gadi to decrease the SU usage and little cost to walltime.\n\n\nOptimise the Gadi script\nThe list of parameters in our job_params file can broadly be separated into 2 groups of long- and short- running jobs. When we ran the nci-parallel job above, the script allocated all of the resources for all the 4 jobs at the beginning. The 2 shorter jobs completed very quickly (in a matter of seconds), however Gadi has held onto the resources for all of them for the entire duration of the job, including the longer jobs which ran for 10 minutes. This has resulted in an unnecessary consumption of SU, while 4 CPUs are sitting idle waiting for the longer jobs to finish running.\nA simple way to improve this would be to separate the job into 2, one for the short running jobs that would terminate quickly and free their resources and another for the longer running jobs. Indeed a recommended way to save on kSU usage when running large parallel jobs is to group them by equal expected processing time and submit these as separate jobs.\nAnother way to do this with only a single job using nci-parallel would be to reduce the resource request to NCI parallel to consume only two jobs at a time rather than 4. That way it will do the 2 longer subjobs first with the idea that the shorter tasks can backfill as longer tasks complete (see the figures above for reference). This way only 4 CPUs are needed and the walltime is only fractionally longer than the 4 CPU case. Note that for this to work optimally the sorting of our jobs in decreasing order of expected length as done in our job_params is important.\nTo do this we change the above mpirun command to only run 2 jobs of 2 CPUs:\nmpirun -np 2\\\n       --map-by node:PE=2 \\\n      nci-parallel --input-file ${PBS_JOBFS}/input_cmd \\\n      --timeout 4000 \\\n      --verbose \\\n      -o ./\nor alternatively we can just set #PBS -l ncpus=4 in the header of the above pbs script and the supplied calculation M * PBS_NCPUS / PBS_NCI_NCPUS_PER_NODE will do the work for us.\nwhich gives up the following resource usage report:\n======================================================================================\n                  Resource Usage\n   Job Id:             137236250.gadi-pbs\n   Project:            qc03\n   Exit Status:        0\n   Service Units:      1.64\n   NCPUs Requested:    4                      NCPUs Used: 4\n                                           CPU Time Used: 00:20:23\n   Memory Requested:   6.0GB                 Memory Used: 1.12GB\n   Walltime requested: 00:20:00            Walltime Used: 00:12:19\n   JobFS requested:    100.0MB                JobFS used: 8.05MB\n======================================================================================\nNow the job used ~ half the SU with only a ~3% increase in walltime.",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#summary",
    "href": "notebooks/13_parallel_jobs.html#summary",
    "title": "Running embarrassingly parallel jobs on Gadi",
    "section": "Summary",
    "text": "Summary\n\nPBS job arrays are not permitted on Gadi. These can be easily adapted to nci-parallel jobs\nAlways benchmark your tasks prior to submitting a full parallel workflow\nAlways test your parallel job scripts prior to submitting a full parallel workflow\nTake steps to maximise parallel job efficiency\nDetermine whether mapping by node or by NUMA node is most appropriate for your job, and adapt the mpirun command accordingly\nParallel tasks can run all at once (concurrently) or in batches sequentially, this is determined by the CPU per task and total resources requested\nIf not known, request less resources than required to run all tasks concurrently - NCI recommends 10% of tasks concurrently but good efficiency can be achieved with more depending on your specific workflow\nYou can use our calculator to simplify adjusting total resource requests based on the results of your benchmarking\n\nThoroughly check each task in a completed parallel job: check both PBS logs as well as all job outputs - a custom checker script can help here\nEnsure to redirect important output, don’t allow it to be printed only to the PBS logs as there is one per job, not one per task like there is with job arrays",
    "crumbs": [
      "Running a job",
      "Parallel jobs"
    ]
  },
  {
    "objectID": "notebooks/05_data_transfer.html",
    "href": "notebooks/05_data_transfer.html",
    "title": "Transferring data to and from Gadi",
    "section": "",
    "text": "In this section, we will look at transferring data between the Research Data Store (RDS) and Gadi. There is a lot of content with step-by-step examples here due to the numerous methods available for data transfer.\nWatch the pre-recorded session\nThe table below summarises the data transfer methods covered in this section:\n\n\n\nMethod\nSuitable data size/type\nPros/Cons\n\n\n\n\nGUI based data transfer client e.g. filezilla or cyberduck\nOnly small files, config scripts etc.\n\nEasy to use\nHost computer can limit transfer speed\n\n\n\nsftp transfer from Gadi terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nUses login node for computation\nCannot restart after interruption\n\n\n\nrsync transfer from Artemis terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nCan restart after interruption\nUses login node for computation\nArtemis only around until August\n\n\n\nsftp copy using copyq\nLarge files and datasets to copy all in one go.\n\nRobust, runs in background\nFaster speed than lftp\nAlways copies all files\n\n\n\nlftp sync using copyq\nLarge datasets with many files and only a few updated with each transfer.\n\nRobust, runs in background\nOnly copies whats changed\nSlower speed than sftp\n\n\n\nArtemis rsync using dtq\nAny large dataset.\n\nRobust, runs in background\nOnly copies whats changed\nArtemis only around until August\n\n\n\nGlobus\nAny file/dataset size.\n\nShould do everything\n\nGUI\nTerminal copy and sync\ncopyq copy and sync\n\nUnknown time-frame for availability",
    "crumbs": [
      "Working on Gadi",
      "Data transfer"
    ]
  },
  {
    "objectID": "notebooks/05_data_transfer.html#introduction",
    "href": "notebooks/05_data_transfer.html#introduction",
    "title": "Transferring data to and from Gadi",
    "section": "",
    "text": "In this section, we will look at transferring data between the Research Data Store (RDS) and Gadi. There is a lot of content with step-by-step examples here due to the numerous methods available for data transfer.\nWatch the pre-recorded session\nThe table below summarises the data transfer methods covered in this section:\n\n\n\nMethod\nSuitable data size/type\nPros/Cons\n\n\n\n\nGUI based data transfer client e.g. filezilla or cyberduck\nOnly small files, config scripts etc.\n\nEasy to use\nHost computer can limit transfer speed\n\n\n\nsftp transfer from Gadi terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nUses login node for computation\nCannot restart after interruption\n\n\n\nrsync transfer from Artemis terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nCan restart after interruption\nUses login node for computation\nArtemis only around until August\n\n\n\nsftp copy using copyq\nLarge files and datasets to copy all in one go.\n\nRobust, runs in background\nFaster speed than lftp\nAlways copies all files\n\n\n\nlftp sync using copyq\nLarge datasets with many files and only a few updated with each transfer.\n\nRobust, runs in background\nOnly copies whats changed\nSlower speed than sftp\n\n\n\nArtemis rsync using dtq\nAny large dataset.\n\nRobust, runs in background\nOnly copies whats changed\nArtemis only around until August\n\n\n\nGlobus\nAny file/dataset size.\n\nShould do everything\n\nGUI\nTerminal copy and sync\ncopyq copy and sync\n\nUnknown time-frame for availability",
    "crumbs": [
      "Working on Gadi",
      "Data transfer"
    ]
  },
  {
    "objectID": "notebooks/05_data_transfer.html#where-to-put-project-files-on-gadi",
    "href": "notebooks/05_data_transfer.html#where-to-put-project-files-on-gadi",
    "title": "Transferring data to and from Gadi",
    "section": "Where to put project files on Gadi",
    "text": "Where to put project files on Gadi\nOn Gadi (like on Artemis) you have access to a number of different storage areas for project files and data. Check the NCI User Guides for more detailed information. Here we provide a brief introduction for those familiar with Artemis.\nEach space is intended for use in a specific way:\n\n/home\nYour home space (/home/&lt;nci_user_id&gt;) is owned by you and has 10 GiB of available space. This cannot be increased.\nIt works similarly to your /home space on Artemis and should only be used to store things like program code, batch scripts or software configuration information. Note that as space is extremely limited here it is unadvisable to use this space for storing project data.\n\n\n/scratch\nYour scratch space (/scratch/&lt;project&gt;) is owned by your project and has 1 TiB of available space which can be increased upon request to NCI. It is roughly equivalent to /scratch on Artemis.\nData are not backed up and files not accessed for 100 days will be quarantined for 14 days and then removed (See here for instructions for removing files from quarantine).\nScratch should be used for temporary files associated with a job that has large data IO and not for longer term storage. Always ensure any data you need from a job that is left in /scratch is promptly backed up to the Research Data Store.\n\n\n/g/data\nYour /g/data space (/g/data/&lt;project&gt;) is owned by your project and has the available space allocated by the Sydney Scheme Manager.\nThe files on /g/data are not backed up but they will persist there for the lifetime of your project.\n/g/data is intended to be used to store longer term files that are regularly used by your project during its lifetime. Always ensure your data is regularly backed up from here to the Research Data Store.\n/g/data may be accessed directly from PBS job scripts by using the -lstorage PBS directive.\nTo check the amount of disk space you have available in the data areas listed above you can type the command lquota at the Gadi prompt.",
    "crumbs": [
      "Working on Gadi",
      "Data transfer"
    ]
  },
  {
    "objectID": "notebooks/05_data_transfer.html#research-data-store-rds",
    "href": "notebooks/05_data_transfer.html#research-data-store-rds",
    "title": "Transferring data to and from Gadi",
    "section": "Research data store (RDS)",
    "text": "Research data store (RDS)\nThe RDS is NOT being decommissioned along with Artemis HPC. Any RDS projects you currently have will persist on RDS. It is your responsibility to backup any data on Artemis filesystems (/home, /scratch, /project) that you wish to keep prior to the decommission date of August 29 2025. For information on how to go about this see the SIH Artemis Training Series.\nIn this section, we will mainly focus on how to transfer data between Gadi HPC and RDS. You should be able follow similar methods for copying data between your own laptop/server and Gadi.",
    "crumbs": [
      "Working on Gadi",
      "Data transfer"
    ]
  },
  {
    "objectID": "notebooks/05_data_transfer.html#data-transfer-options",
    "href": "notebooks/05_data_transfer.html#data-transfer-options",
    "title": "Transferring data to and from Gadi",
    "section": "Data transfer options",
    "text": "Data transfer options\nDepending on the size and complexity of the data you are transferring you have multiple options available:\n\nFor small transfers (&lt;1GB) you can use a GUI based data transfer client such as filezilla or cyberduck.\nFor mid sized transfers up to tens of GB you can use terminal based transfer.\nFor large transfers you should use the data transfer queue options on either Gadi (copyq) or Artemis (dtq).\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data transfer to and from Gadi should be made using the “Data Mover Node” at gadi-dm.nci.org.au where possible rather than the login nodes. This ensures that data transfer will not consume otherwise limited resources on the login nodes.\n\n\n\nGlobus - COMING SOON\nIn the coming months, Globus will be available for simplified and efficient data transfer. We will provide training and materials on this once available.\nIn the meantime, the below options are available, and detailed examples for each method are provided in the subsequent sections.\n\n\nTransfer using RDS mapped network drive and data transfer client\nFor smaller files or datasets, for example a set of scripts that you are going to run, you can map your RDS project as a network drive and transfer the data to Gadi via an intermediate data transfer client GUI such as filezilla or cyberduck.\nWhile simple to use, these are not recommended for large data transfers, as the local computer becomes a bottleneck and they are generally not resumable after interruption. Faster speeds will be obtained if you are on campus, but still this method may be prohibitively slow for larger datasets.\n\n\n\n\n\n\nTipExample using cyberduck\n\n\n\nThe following are instructions using the cyberduck data transfer client. The process using filezilla is similar with the same username and server IP address as shown in this example.\nTo mount your RDS drive in either Windows or MacOS, please follow the instructions described here. You should have a File Explorer (Windows) or Finder (MacOS) window open and displaying the files and folders in your RDS project directory.\nNext download cyberduck from https://cyberduck.io and open it and connect to Gadi:\n\nClick on the Open Connection icon at the top of the window.\nSelect SFTP (SSH File Transfer Protocol) from the drop-down menu at the top of box.\nIn the Server field, enter gadi-dm.nci.org.au.\nIn the Username field, enter your NCI Username.\nIn the Password field, enter your NCI password.\nClick Connect.\nIf an Unknown fingerprint box appears, click the Always check box in the lower-left hand corner, then click Allow.\n\nIf you have successfully logged in, you will see a directory listing of /home/&lt;user_login&gt;. You can browse to your project folder either in /scratch or /g/data by pressing Ctrl + g and then typing /scratch/&lt;project_id&gt; or /g/data/&lt;project_id&gt;.\nYou can then transfer data to and from RDS and NCI by dragging and dropping files between your computer’s file explorer and the Cyberduck window.\n\n\n\n\nTransfer from Gadi/Artemis terminal to/from RDS\nYou can use commands in your terminal application (Mac and linux: Terminal, Windows: Windows Terminal or Powershell equivalent) as an alternative to graphical applications.\nSince the connection will be terminated if your computer sleeps, terminal crashes, network drops out etc, this method is not particularly robust for large transfers. For these rather use the queue based methods (either copyq on Gadi or dtq on Artemis) described below.\n\nTransfers from a terminal on Gadi\nDue to stringent security settings around Artemis and RDS, familiar commands like rsync or scp cannot be initiated from NCI Gadi login nodes. Instead you have to use commands like sftp to copy the data.\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that running these commands on the login nodes is not the recommended way to transfer research data to Gadi! For smaller downloads, this is OK, but for normal purposes the use of Gadi’s copyq and data mover nodes are the appropriate tools.\n\n\n\n\n\n\n\n\nNoteHow to transfer data from RDS to Gadi (and vice-versa) from a Gadi login.\n\n\n\nTo transfer data between RDS and Gadi from the Gadi login shell:\n\nOpen a terminal using the ‘Terminal’ app on MacOS or the ‘Command Prompt’ app on Windows and then log into Gadi using ssh:\nssh &lt;nci_user_id&gt;@gadi.nci.org.au\nYou may be prompted to enter your NCI password at this point.\nGet the data from RDS, to a specific location on Gadi, e.g:\nsftp -r &lt;your_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder&gt; &lt;location on Gadi&gt;\nFor example if I wanted to copy data from my Training project on RDS in the folder MyData to Gadi in the scratch space for my NCI project named qc03:\nsftp -r &lt;my_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-Training/MyData /scratch/qc03/MyData\nreplace /rds in the above with /project or /scratch for your preferred source folder or file.\nIf you want to copy the other way around (ie. from Gadi /scratch to RDS) use\nsftp &lt;your_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder&gt; &lt;&lt;&lt; $\"put -r &lt;location on Gadi&gt;\"\n\n\n\n\n\nTransfers from a terminal on Artemis using rsync\n\n\n\n\n\n\nWarning\n\n\n\nThis option is only available prior to the decommission of Artemis on 29 August 2025 - after that date you will have to use either GLOBUS (preferred), or copy data when logged into the Gadi terminal (either at the login shell or using copyq scripts).\n\n\nWhen logged into Artemis you can use the rsync or scp command to copy data directly to/from Gadi, since Gadi allows the kind of secure connection that these commands require.\nUsing rsync will allow you to sync data between RDS and Gadi, this means that only files that have been updated since the last transfer will be copied. This will allow small changes to large datasets to be transferred quickly.\n\n\n\n\n\n\nTipTransfer from direct connection to RDS\n\n\n\nYou can also initiate the transfer in a terminal from a connection via ssh to research-data-int.sydney.edu.au (on campus or USyd VPN) and using the rsync method described here - just replace hpc.sydney.edu.au with research-data-int.sydney.edu.au in step 1.\n\n\n\n\n\n\n\n\nNoteHow to transfer from RDS to Gadi (and vice-versa) at an Artemis terminal using rsync\n\n\n\nTo transfer data between RDS and Gadi from the Artemis login shell:\n\nOpen a terminal using the ‘Terminal’ app on MacOS or the ‘Command Prompt’ app on Windows and then log into Artemis using ssh:\nssh &lt;your_unikey&gt;@hpc.sydney.edu.au\nYou may be prompted to enter your password at this stage.\nCopy the data from rds, to a specific location on Gadi, e.g:\nrsync -rtlPvz /rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder in RDS&gt; &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;Destination on Gadi&gt;\nFor example if I wanted to sync data from my Training project on RDS in the folder MyData to Gadi in the scratch space for my NCI project named qc03:\nrsync -rtlPvz /rds/PRJ-Training/MyData &lt;nci_user_id&gt;@gadi-dm.nci.org.au:/scratch/qc03/MyData\nYou will be prompted for the password associated with your username on Gadi and the transfer will commence.\n\nIf you want to copy the other way around (ie. from Gadi /scratch to RDS) then simply reverse the order of the above command in step 2, e.g:\nrsync -rtlPvz &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;Source on Gadi&gt; /rds/PRJ-&lt;Project Short ID&gt;/&lt;Destination on RDS&gt;\n\n\n\n\n\nUsing tmux to run commands in persistent terminal sessions\n\n\n\n\n\n\nWarning\n\n\n\nWhile this method can help you run your copy job in the background over a long period of time, it is recommended to rather use the copyq transfer method described below for large file transfers, as running jobs on the login node can overwhelm their scant resources.\n\n\nYou can run data transfers or other long-running commands in a tmux persistent session which can be detached to run in the background. This allows you to log out and even switch off your computer while the command still runs within the detached session. You can reattach to the tmux session later to review the command’s progress and output.\nFor example, to copy data from RDS to Gadi with rsync within a tmux session:\n\nLogin to RDS: ssh &lt;unikey&gt;@research-data-int.sydney.edu.au\nStart a tmux session: enter tmux\nOnce inside the new session, issue the rsync command: rsync -rtlPvz /rds/PRJ-&lt;project&gt;/&lt;path-to-data&gt; &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;gadi-destination-path&gt;\nDetach from the session: enter ctrl B + D\nYou can later reattach to the same session from a new ssh connection to research-data-int.sydney.edu.au with: tmux attach\n\nFor more information about tmux and its options, check here.\n\n\nTransfer using sftp or lftp from Gadi copyq\nThe data transfer queue on Gadi is called copyq. This is comparable to the data transfer queue on Artemis dtq. Data transfer methods/scripts that you used to put data onto Artemis for example from the web via wget or from another server should be easily portable to use on Gadi’s copyq.\n\n\n\n\n\n\nImportant\n\n\n\nIf you have been relying on ssh key pairs between Gadi and Artemis for passwordless data transfers, please create a new ssh key pair between Gadi and research-data-ext (the RDS login server), as after 29th August, your Gadi-Artemis key pair will no longer work. Users have home directory on research-data-ext with a very small quota to enable storing of ssh key files.\n\n\nPlease note that the compute nodes on Gadi do not have internet access like the Artemis compute nodes do, so all required data must first be downloaded before submitting a compute job that requires the data.\nDue to stringent security settings around Artemis and RDS, commands like rsync or scp cannot be initiated from NCI Gadi login nodes or copyq. To initiate the transfer from Gadi, sftp or lftp must be used. In the not too distant future Globus will become available for data transfer and then that will be the preferred method for transferring data to and from Gadi.\n\nHow to set up SSH keys for passwordless data transfer\nIf you are transferring data directly for example scp on the command line or via a transfer client on your local computer, entering a password to initiate the transfer is straightforward. If however you want to transfer via a job submitted to either copyq or dtq, you will need to set up SSH keys first, or else your script will halt while it waits for a password to be entered.\n**If you have been relying on ssh key pairs between gadi and Artemis for passwordless data transfers, please create a new ssh key pair between Gadi and research-data-ext (the RDS login server), as after 29th August, your Gadi-Artemis key pair will no longer work.\nYou only need to set this up once.\nSSH key pairs are used for secure communication between two systems. The pair consists of a private key and a public key. The private key should remain private and only be known by the user. It is stored securely on the user’s computer. The public key can be shared with any system the user wants to connect to. It is added to the remote system’s authorized keys. When a connection is attempted, the remote system uses the public key to create a message for the user’s system.\nThere are many general guides for this online, for example this one.\n\n\n\n\n\n\nNoteSet up keys between Gadi and RDS\n\n\n\nFollow the below steps carefully to set up SSH keys between RDS and Gadi. Note, you only need to do this once.\n\nLog into Gadi with your chosen method, e.g:\nssh ab1234@gadi.nci.org.au\nMove to your home directory:\ncd ~\nMake a .ssh directory, if you don’t already have one:\nmkdir -p .ssh \nSet suitable permissions for the .ssh directory and move into it:\nchmod 700 .ssh\ncd .ssh\nGenerate SSH key pair:\nssh-keygen\nHit enter when prompted, saving the key in ~/.ssh/id_rsa and enter for NO passphrase. A public key will be located in ~/.ssh/id_rsa.pub and a private key in ~/.ssh/id_rsa.\nSet suitable permissions for the keys:\nchmod 600 id_rsa\nchmod 644 id_rsa.pub\nMake an authorized_keys file if you don’t already have one:\ntouch ~/.ssh/authorized_keys\nCopy the contents of the public key file (~/.ssh/id_rsa.pub) to the authorized_keys file:\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nSet permissions for the authorized_keys file:\nchmod 600 ~/.ssh/authorized_keys\nConnect to USyd’s RDS login server using lftp and your unikey:\nlftp sftp://&lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nProvide your unikey password when prompted. When you log in, you are in your personal home directory on the RDS login server. This is NOT the place to store data, only ssh key files.\nThen make and move into a .ssh directory if you don’t already have one:\nmkdir -p ~/.ssh\ncd ~/.ssh\nTransfer the authorized_keys file from Gadi to USyd’s RDS login server:\nput authorized_keys\nDoing this will transfer authorized_keys on Gadi to your current directory on RDS (/home/). With lftp, it will look for the file relative to where you launched lftp. You can check where you are on Gadi using:\nlocal pwd\nExit your lftp connection to RDS by entering ctrl + d. You are now back to your Gadi session. Then, test the passwordless connection:\nsftp &lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nThis time, you shouldn’t be prompted for a password. You can proceed to transfer data between Gadi and RDS now on the copyq.\n\nIf you get the error “Fatal error: Host key verification failed” you may have to get an “ssh fingerprint” first. Do this by sending an ssh request to the RDS with:\nssh &lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nAccept that you trust the connection and enter your password. The connection will then close with the following message:\nThis service allows sftp connections only.\nConnection to research-data-ext.sydney.edu.au closed.\nBut now try lftp connection again!\n\n\n\n\nTemplate copyq scripts for transferring data with sftp\nThe scripts below use sftp to transfer data between RDS and Gadi on the Gadi copyq. sftp can transfer whole files and directories but must copy all of your data every time, it cannot only copy modified files like rsync can. This makes it considerably slower for copying large datasets where only minor changes have been made during a run. An alternative command lftp can behave like rsync but is slower to transfer than sftp. We also provide a template lftp script below.\nCopies of these scripts have been placed in /scratch/qc03/data-transfer-scripts/gadi-scripts. You can make a copy of these scripts to your /scratch/&lt;nci-project-code&gt; or /home/&lt;nci-user-id&gt; workspace on Gadi and edit (for example using nano &lt;script&gt;), by replacing the names described in the header to suit your needs.\nThere are two scripts:\n\nfrom_gadi_to_rds.pbs is used to transfer a file or folder from Gadi to RDS\nfrom_rds_to_gadi.pbs is used to transfer a file or folder from RDS to Gadi\n\n\n\n\n\n\n\nNoteTransfer from Gadi to RDS\n\n\n\nfrom_gadi_to_rds.pbs script, replace variables in &lt;brackets&gt; as described:\n#!/bin/bash\n\n# Transfer from Gadi to RDS\n#\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;      : Your USyd unikey\n# &lt;rds_project&gt; : Your RDS project name\n# &lt;local_path&gt;  : The local file or folder you want to copy\n# &lt;remote_path&gt; : The location on RDS to put your folder\n# &lt;nci_project&gt; : Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\nremote_path=/rds/PRJ-&lt;rds_project&gt;/&lt;remote_path&gt;\n\n# NOTE: Add a trailing slash (/) to local_path if you don't want to create the\n# parent directory at the destination.\nlocal_path=&lt;local_path&gt;\n\nsftp ${remote_user}@${remote_host}:${remote_path} &lt;&lt;&lt; $\"put -r ${local_path}\"\n\n\n\n\n\n\n\n\nNoteTransfer from RDS to Gadi\n\n\n\nfrom_rds_to_gadi.pbs script, replace variables in &lt;brackets&gt; as described:\n#!/bin/bash\n\n# Transfer a folder from RDS to Gadi\n# This will recreate your RDS path (/rds/PRJ-&lt;rds_project&gt;)\n# on Gadi in /scratch/&lt;nci_project&gt;\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;      : Your USyd unikey\n# &lt;rds_project&gt; : Your RDS project name\n# &lt;remote_path&gt; : The location on RDS of your file ot directory to copy\n# &lt;local_path&gt;  : The name of the folder to copy to\n# &lt;nci_project&gt; : Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\n# RDS:\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\n\n# NOTE: Add a trailing slash (/) to remote_path if you don't want to create the\n# parent directory at the destination. \nremote_path=/rds/PRJ-&lt;rds_project&gt;/&lt;remote_path&gt;\n\n# Gadi:\ndest_path=/scratch/&lt;nci_project&gt;/&lt;local_path&gt;\n\n# Copy folder with sftp\nsftp -r ${remote_user}@${remote_host}:${remote_path} ${dest_path}\n\n\nHere is an example showing you how to transfer a folder called MyData in the RDS project Training to some scratch space in Gadi owned by project aa00.\n\n\n\n\n\n\nTipExample copyq transfer from RDS to Gadi\n\n\n\nLog into Gadi and change directory to your project space and make a folder for your workspace:\n# Using an example username tm0000\nssh tm0000@gadi.nci.org.au\n\ncd /scratch/aa00\n\n# Make a folder called workspace in /scratch/aa00/tm0000\nmkdir -p /scratch/aa00/tm0000/workspace\nCopy the required data transfer script template from /scratch/qc03 to your newly made workspace. In this case we are copying from RDS to Gadi so we use the from_rds_to_gadi.pbs script. You can also cut and paste the template script into your editor from above and save the edited script to your workspace.\ncp /scratch/qc03/data-transfer-scripts/gadi-scripts/from_rds_to_gadi.pbs /scratch/aa00/tm0000/workspace\nThen follow the script and move to that workspace and open the script in an editor (in this example we’ll use the nano editor):\ncd /scratch/aa00/tm0000/workspace\n\nnano from_rds_to_gadi.pbs\nYou need to edit the script by replacing all the variables marked with &lt;&gt; described in the script header and fill in the following details before using it:\nIn the #PBS variables part of the script:\n\nProvide the -P variable by replacing &lt;nci_project&gt; with your NCI project code. In this example aa00.\nIncrease the walltime if you are transferring large files, the limit on this queue is 10 hours.\nAlter -lstorage=scratch/&lt;project&gt; as required. If you also need to access g/data, you can change this to scratch/&lt;project&gt;+g/data/&lt;project&gt;. In this example we’ll just use scratch/aa00\n\nIn the body of the script:\n\nProvide the remote_user variable by replacing &lt;unikey&gt; with your USyd unikey.\nProvide the remote_path variable by replacing &lt;rds_project&gt; and &lt;local_path&gt; with your RDS project name and path to the file or directory you want to transfer. In this example we use remote_path=/rds/PRJ-Training/MyData\n\nHere is what the script will look like when correctly edited for this example:\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/aa00\n\n# RDS:\nremote_user=tm0000     #Example unikey tm0000\nremote_host=research-data-ext.sydney.edu.au\n\n# NOTE: Add a trailing slash (/) to remote_path if you don't want to create the\n# parent directory at the destination. \nremote_path=/rds/PRJ-Training/MyData\n\n# Gadi:\n# This will create /scratch/aa00/MyData if transferring a folder and it doesn't already exist.\ndest_path=/scratch/aa00/MyData\n\n# Copy with sftp\nsftp -r ${remote_user}@${remote_host}:${remote_path} ${dest_path}\nWhen you have finished editing the script save it (using &lt;ctrl&gt;-x and answering y at the prompt if using nano as your editor)\nRun the transfer script\nOnce you have customised the script, you can submit it to the copyq on Gadi. Run the script from the directory where you saved it:\nqsub from_rds_to_gadi.pbs\nThis can be a nerve-wracking process, especially if you are transferring large files. You can check the status of your job on Gadi using:\nqstat -Esw\nOnce it says R (running), you can confirm it is going to where you want on RDS/Artemis or Gadi by logging into the system and checking for the presence of the file/directory in its expected location using:\nls MyData\n\n\n\n\nConfirm transfers after completion\nTo confirm the transfer was successful, you’ll need to check your job logs. These are located in the same directory as your script and are named transfer.o&lt;jobid&gt;.\nHowever, this doesn’t guarantee the integrity of the files. You should check the files themselves to ensure they are intact. You can do this using md5checksums as described below.\n\n\nTemplate copyq scripts for syncing data with lftp\nIf you have a large project with many files and only a few of them are modified at any time you can use the mirror command in lftp to only sync the modified files when you are backing up data to RDS. The lftp - mirror command transfers data at a slower speed than sftp but will significantly speed up the backup of data since it generally only needs to copy a small subset of all of the data on Gadi if only a few files have changed since the last sync.\nBelow is a template .pbs script that can be used to sync files between Gadi and RDS, you can copy it to your workspace and modify it as needed similarly to the example above. You can also find this script on Gadi in /scratch/qc03/data-transfer-scripts/gadi-scripts/sync_gadi_to_rds.pbs.\n\n\n\n\n\n\nWarning\n\n\n\nNote that when using this script to copy a folder you should ensure the target directory exists on RDS first, otherwise the parent directory will not be synced with the data.\n\n\n\n\n\n\n\n\nNoteSync between Gadi and RDS\n\n\n\n#!/bin/bash\n\n# Mirror directory from Gadi to RDS\n#\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;:       Your USyd unikey\n# &lt;rds_project&gt;:  Your RDS project name\n# &lt;path-on-rds&gt;:  The location on RDS to put your directory\n# &lt;path-on-gadi&gt;: The directory on Gadi to mirror to rds\n# &lt;nci_project&gt;:  Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\nremote_path=/rds/PRJ-&lt;rds-project&gt;/&lt;path-on-rds&gt;\n\nsource_path=&lt;path-on-gadi&gt;\n\noutfile=\"${PBS_O_WORKDIR}/${PBS_JOBNAME}_${PBS_JOBID}.log\"\necho \"Writing log to: ${outfile}\"\ntouch $outfile\n\nlftp -u ${remote_user}, sftp://${remote_host} &lt;&lt;EOF\nset xfer:log true\nset xfer:log-file \"${outfile}\"\nmirror -p --verbose -R ${source_path} ${remote_path}\nexit\nEOF\n\n\n\n\nSuggested workflow for copying data between RDS and Gadi and keeping it up-to-date\nThe sftp copy method and lftp sync methods described above both have their pros and cons:\n\nsftp has a faster transfer speed but can only copy all your files in bulk when it is run.\nlftp has a slower transfer speed but it can sync only the subset of files that have changed.\n\nBecause of this we suggest users be selective about which method they use based on their needs.\nFor example a simple workflow for large projects that have a lot of data and many files in a folder, only a few of which are changed between backups to RDS would be:\n\nInitially use the sftp based from_rds_to_gadi.pbs script to bulk copy your data from RDS to your project space on Gadi, since this is faster for bulk transfers.\nSubsequently use the lftp based sync_gadi_to_rds.pbs to sync smaller files (e.g. output logs and data) back to RDS, without having to re-copy the bulk of the data back to RDS.\n\n\n\n\nTransfer using rsync from Artemis dtq\n\n\n\n\n\n\nWarning\n\n\n\nThis option is only available prior to the decommission of Artemis on 29 August 2025 - after that date you will have to use either GLOBUS (preferred), or copy data when logged into the Gadi terminal (either at the login shell or using copyq scripts).\n\n\nFor transfer of large files directly from Artemis to Gadi, the use of resumable rsync is recommended (see script below). The transfer can be initiated using Artemis dtq and using Gadi’s data mover node: gadi-dm.nci.org.au. The below template script can be used with Artemis’ dtq using rsync.\nFor further info about copying data from Artemis dtq can be found in the SIH Artemis Training Series.\n\n\n\n\n\n\nNoteTemplate Artemis data transfer script using rsync\n\n\n\n#!/bin/bash\n\n# This is an Artemis data transfer script\n\n#PBS -P &lt;project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q dtq\n\n# NOTE: Add a trailing slash (/) to source_path if you don't want to create the\n# parent directory at the destination. \nsource_path=/rds/PRJ-&lt;project&gt;/&lt;path&gt;/&lt;file&gt;\n\ndestination=&lt;user&gt;@gadi-dm.nci.org.au\ndestination_path=&lt;path-on-gadi&gt;\n\nwhile [ 1 ]\ndo\n        echo Transferring ${source_path} to ${destination}:${destination_path}\n        rsync -rtlPvz --append-verify ${source_path} ${destination}:${destination_path}\n\n        if [ \"$?\" = \"0\" ]\n        then\n                echo \"Rsync completed normally\"\n                dt=$(date)\n                echo Finished transferring at $dt\n        exit\n        else\n                echo \"Rsync failure. Backing off and retrying in 3 minutes\"\n                sleep 180\n        fi\ndone",
    "crumbs": [
      "Working on Gadi",
      "Data transfer"
    ]
  },
  {
    "objectID": "notebooks/05_data_transfer.html#checksum",
    "href": "notebooks/05_data_transfer.html#checksum",
    "title": "Transferring data to and from Gadi",
    "section": "Verify the integrity of your transferred data",
    "text": "Verify the integrity of your transferred data\nWhen you transfer files between systems, especially over a network using tools described above, there is always a small risk that the data could become corrupted. This might happen for example due to network issues, disk errors, or file system problems. It is therefore important that you verify that your files were transferred completely and accurately. We can do this using a checksum.\nA checksum is a small, fixed-size value calculated from the contents of a file using a specific algorithm (like MD5, SHA-1, or SHA-256). Think of it as a unique fingerprint for the file’s data: if even one byte changes, the checksum will be different. By comparing the checksum before and after the transfer, you can confirm that the file is exactly the same on both ends.\nThe overall process of verifying data transfer is outlined in the figure below. In some cases, you may already have checksum files that you have previously generated, or even downloaded with your raw data. If so, there is no need to re-generate the “source” checksum files.\nFor transfers from RDS to Gadi, RDS is the “source” and Gadi is the “destination”:\n\nNote that the source and destination are reversed when transferring output files generated on Gadi back to RDS.\n\nUsing md5sum to validate files: from RDS to Gadi\nHere we demonstrate how you can use the command md5sum to generate a list of MD5 checksums for a directory on RDS, and then use these checksums to validate the integrity of your files after the data has been transferred to Gadi.\n1. Create a list of checksums for your files at the source (before copying the data)\nConnect to the RDS internal login node with ssh (remember to have VPN connected if off-campus):\nssh &lt;unikey&gt;@research-data-int.sydney.edu.au\nEnter your unikey password, and once connected to RDS, navigate to the folder containing the data you are going to transfer to Gadi.\nUse the md5sum command to produce a md5 hash for every file within that directory and its subdirectories:\ncd /rds/PRJ-&lt;dashR-project&gt;/myData\n\nfind -type f \\( -not -name \"myData.md5\" \\) -exec md5sum '{}' \\; &gt; myData.md5\nThis will generate a file called myData.md5 in your folder listing a hash for every file (except for the myData.md5 file).\n2. Copy the data to Gadi\nYou should do this using one of the methods described above in this page - make sure to also transfer the .md5 file and keep it in the same relative location as it was at the source (RDS).\n3. Verify the copy of the files on Gadi\nOnce the transfer of data and checksum files to Gadi are complete, you can use the -c (check) flag to the checksum command to verify file integrity.\nFor small files, you can easily run this on the Gadi login nodes, eg:\ncd /scratch/&lt;nci-project-id&gt;/myData\nmd5sum -c myData.md5\nThis command goes through the list of files in myData.md5 and compares the md5 sum of the file on Gadi to the md5 sum that was created for the file on RDS.\nFor larger files, or very large numbers of files, the checksum process can take some time (eg &gt; 10 minutes for a single 100 GB file) so it is more robust to run the checksum commands within a PBS job submitted to the Gadi job scheduler.\nYou can do this simply by placing the checksum command described above inside a Gadi job script.\nFiles which have been transferred with full integrity will report “OK”:\nfile1.txt: OK\nsub_folder/file2.txt: OK\nIf there was a problem with any file, you’ll see a warning like:\nfile3.txt: FAILED\nRedirecting the output of the md5sum -c command to a file can simplify collecting failed transfers for resubmission:\nmd5sum -c myData.md5 &gt; myData.md5.check\ngrep FAILED myData.md5.check &gt; myData.md5.check.FAILED\nAny files with FAILED status should be re-copied from RDS to Gadi, and the checksum comparison run again.\n\n\nUsing md5sum to validate files: from Gadi to RDS\nWhen working on Gadi, you will want to regularly back up important data (for example job outputs, logs, scripts) to RDS. Gadi scratch and gdata filesystems are not backed up, so regular backups to RDS are critical.\nThe high-level checksum process is the same as it was for RDS to Gadi, but the source and destination are reversed.\n1. Create a list of checksums for your files at the source (before copying the data)\nOn Gadi, you can create the checksum file with the same md5sum command as described above, either on the Gadi login node, or from within a Gadi job script.\nImportantly, the Gadi compute queues can read gdata, unlike Artemis where the general compute queues could not read RDS. So, if your data is on gdata, you do not need to copy it to scratch for checksum commands run within Gadi PBS jobs.\n2. Copy the data to RDS\nYou should do this using one of the methods described above in this page - make sure to also transfer the .md5 file and keep it in the same relative location as it was at the source (Gadi).\n3. Verify the copy of the files on RDS\nOnce the transfer of data and checksum files to RDS are complete, you can use the -c (check) flag to the checksum command to verify file integrity.\nConnect to the RDS internal login node with ssh (remember to have VPN connected if off-campus):\nssh &lt;unikey&gt;@research-data-int.sydney.edu.au\nEnter your unikey password, and once connected, navigate to the directory containing the data you have just transferred from Gadi to RDS.\ncd /rds/PRJ-&lt;dashR-project&gt;/myData\nmd5sum -c myData.md5 &gt; myData.md5.check\ngrep FAILED myData.md5.check &gt; myData.md5.check.FAILED\nAny files with FAILED status should be re-copied from Gadi to RDS, and the checksum run again.\n\n\nAlternate methods for running checksum commands on RDS\nFor large files or datasets with numerous files, creating checksums or running checksum comparisons can be time-consuming. When we ssh into research-data-int.sydney.edu.au and run the checksum command, this is reliant on your terminal window staying open and connected, your internet connection remaining stable, your computer not going to sleep or being switched off, etc. On Gadi, we can mitigate these issues by submitting long walltime checksum commands to the job scheduler. Below are some other options to run checksum commands (or any other data management commands) on data stored on RDS:\n\nMap your RDS as a network drive on your local computer. Then, use your mac terminal, Windows Powershell, WSL2, etc to run the commands using your local computer’s resources. You will still need to ensure stable internet connection and that your computer does not go to sleep mode (unless using tmux, see below), however this setup may be more stable than relying on the ssh connection to the RDS login node.\n\nRun your checksum commands within a persistent terminal session with tmux. You can do this with an ssh connection to research-data-int.sydney.edu.au, or from the mac/Powershell/WSL2 terminal reading the RDS mapped network drive. When you run your checksum command inside a tmux session and then detach from it, the session (and the command) will remain active, even if you lose internet connection, close your terminal, or even power off your computer. You can reattach to the same session later to check the status of the checksum command. See the section on tmux above for basic use.\nUntil decommission date of August 29, the data transfer queue dtq on Artemis can be used to run checksums on data within RDS. dtq is the only queue on Artemis that can read RDS.\nGlobus (coming soon) will simplify and expedite data transfer between RDS and Gadi. Globus will handle checksum creation and verification for you if the appropriate flags are set.",
    "crumbs": [
      "Working on Gadi",
      "Data transfer"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support and resources",
    "section": "",
    "text": "NoteAttention researchers\n\n\n\nPlease see the University of Sydney Research Computing documentation for a comprehensive overview of support available to you as a University of Sydney researcher using NCI resources.",
    "crumbs": [
      "Support and resources"
    ]
  },
  {
    "objectID": "support.html#technical-documentation",
    "href": "support.html#technical-documentation",
    "title": "Support and resources",
    "section": "Technical documentation",
    "text": "Technical documentation\nFamiliarise yourself with Gadi using NCI’s documentation and the University of Sydney’s Research Computing documentation.\n\nNCI Gadi user guide\nUSyd Research Computing documentation",
    "crumbs": [
      "Support and resources"
    ]
  },
  {
    "objectID": "support.html#live-and-self-directed-training",
    "href": "support.html#live-and-self-directed-training",
    "title": "Support and resources",
    "section": "Live and self-directed training",
    "text": "Live and self-directed training\nIf you are new to HPC and/or NCI, we suggest you attend NCI’s Intro to Gadi courses.\nThere are various trainings offered by SIH and NCI on HPC and data analysis topics:\n\nSIH training calendar\nNCI training calendar",
    "crumbs": [
      "Support and resources"
    ]
  },
  {
    "objectID": "support.html#demonstrations",
    "href": "support.html#demonstrations",
    "title": "Support and resources",
    "section": "Demonstrations",
    "text": "Demonstrations\nHPC experts at SIH have created a series of videos to help you get jobs running on Gadi:\n\nSummarising the Sydney Scheme and how to access NCI\nAdapting Artemis job scripts to Gadi\nData transfer between RDS and Gadi\nWorking within the Gadi walltime limit\nKSU estimation and compute resource benchmarking on Gadi\nRunning embarassingly parallel jobs on Gadi (replaces Artemis job arrays)",
    "crumbs": [
      "Support and resources"
    ]
  },
  {
    "objectID": "support.html#i-need-to-talk-to-someone",
    "href": "support.html#i-need-to-talk-to-someone",
    "title": "Support and resources",
    "section": "I need to talk to someone",
    "text": "I need to talk to someone\nPlease contact the following people depending on your needs:\n\n\n\nType of issue\nWho\nHow\nDetails\n\n\n\n\nService unit allocation for running jobs\nSydney Scheme managers\nMake a request\nSydney Documentation. Use VPN if off the USyd network.\n\n\nRequest for increased g/data storage\nSydney Scheme managers\nSend an email\nSydney Documentation\n\n\nRequest for increased scratch space\nNCI Helpdesk\nLog an NCI ticket\nProvide brief justification for the need including number of files, size of files, data lifecycle\n\n\nRequest for increased iNode quota in gdata or scratch\nNCI Helpdesk\nLog an NCI ticket\nProvide brief justification for the need including number of files, size of files, data lifecycle, efforts to reduce iNode footprint\n\n\nA technical issue with NCI systems or compute jobs\nNCI Helpdesk\nLog an NCI ticket\nProvide all relevant information such as your error, log file, jobid\n\n\nA bug in an SIH pipeline\nSIH\nSubmit an issue on Github\nProvide details e.g. this issue\n\n\nGeneral research computing or data analysis advice\nSIH\nLog an SIH ticket\nProvide relevant context, errors, tool names, scripts",
    "crumbs": [
      "Support and resources"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCI for USyd researchers",
    "section": "",
    "text": "National Computational Infrastructure (NCI) provides high performance computing (HPC), cloud computing, and data services to Australian researchers. This guide has been developed to support University of Sydney researchers in setting up their computational workflows on NCI’s Gadi HPC.\n\n\n\n\n\n\nNote\n\n\n\nPlease be mindful this is not an exhaustive resource for using Gadi. It is only intended to orient you to the system and navigate the comprehensive NCI user documentation. See the University of Sydney Research Computing documentation for more information on research computing infrastructure available at USyd.\n\n\n  Navigate to the following sections to access the information you need to get started:\n\n\n\n\n\n\n\nSection\nWhat is covered\n\n\n\n\nIntroduction to Gadi\nExplanation of Gadi resources and expectations\n\n\nAccess Gadi\nSydney Scheme documentation\n\n\nSet up your computer\nInstalling and configuring an IDE or terminal\n\n\nWorking on Gadi\nKey concepts you need to understand to work effectively on Gadi\n\n\nRunning a job\nWorked examples of different types of jobs on Gadi\n\n\nSupport and resources\nResources and useful points of contact",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "notebooks/09_job_monitoring.html",
    "href": "notebooks/09_job_monitoring.html",
    "title": "Monitoring your job",
    "section": "",
    "text": "You should monitor your jobs to keep track of their progress but refrain from checking your jobs too frequently. Repeated queries will be considered attacks, especially in quick succession and you may get a warning from NCI. NCI recommends querying your jobs’ status a maximum of once every 10 minutes.\nLike Artemis, you can use the qstat command to monitor jobs on Gadi. The NCI Gadi job monitoring page describes some commonly used flags to qstat.",
    "crumbs": [
      "Running a job",
      "Job monitoring"
    ]
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-monitoring",
    "href": "notebooks/09_job_monitoring.html#job-monitoring",
    "title": "Monitoring your job",
    "section": "",
    "text": "You should monitor your jobs to keep track of their progress but refrain from checking your jobs too frequently. Repeated queries will be considered attacks, especially in quick succession and you may get a warning from NCI. NCI recommends querying your jobs’ status a maximum of once every 10 minutes.\nLike Artemis, you can use the qstat command to monitor jobs on Gadi. The NCI Gadi job monitoring page describes some commonly used flags to qstat.",
    "crumbs": [
      "Running a job",
      "Job monitoring"
    ]
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-ids",
    "href": "notebooks/09_job_monitoring.html#job-ids",
    "title": "Monitoring your job",
    "section": "Job IDs",
    "text": "Job IDs\nLike Artemis jobs, jobs submitted to Gadi are given a jobID. This is shown to you as soon as it has been accepted, for example 135615373.gadi-pbs.\nWhen querying the job with qstat, you can use the full ID, or just the string of numbers (omit the .gadi.pbs).\nFor example, the below two commands are equivalent:\nqstat -xf 135615373.gadi-pbs\nqstat -xf 135615373\nIf you have multiple jobs running, you do not need to check them individually with the job ID. You can check the status of multiple jobs using your NCI user ID:\nqstat -u &lt;nci-user-id&gt;",
    "crumbs": [
      "Running a job",
      "Job monitoring"
    ]
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-logs",
    "href": "notebooks/09_job_monitoring.html#job-logs",
    "title": "Monitoring your job",
    "section": "Job logs",
    "text": "Job logs\nBy default, the PBS job logs will be created in the directory from which the qsub command was entered, and combine the job name and the job ID.\nFor example, a job with #PBS -N convert and job ID 133703660 will have standard output and resource usage written to convert.o133703660 and standard error written to convert.e133703660.\nThis differs slightly to Artemis, which has the same default filepath behaviour except the standard output is sent to the .o and the resource usage is sent to .o_usage.\nThe Gadi .o file has the resource usage at the end of the log making it easy to view a quick summary with the tail command:\n$ tail -n 11 convert.o133703660 \nThe output should look similar to:\n                  Resource Usage on 2025-02-05 16:16:10:\n   Job Id:             133703660.gadi-pbs\n   Project:            aa00\n   Exit Status:        0\n   Service Units:      23.94\n   NCPUs Requested:    7                      NCPUs Used: 7               \n                                           CPU Time Used: 02:57:33        \n   Memory Requested:   63.0GB                Memory Used: 51.14GB         \n   Walltime requested: 08:00:00            Walltime Used: 02:44:09        \n   JobFS requested:    100.0MB                JobFS used: 0B              \n======================================================================================\nIf desired, you can change the default log filepaths with the -o and -e directives, for example:\n#PBS -o ./logs/convert-fast5.o\n#PBS -e ./logs/convert-fast5.e\nThis omits the job ID from being included in the log file name and sends the logs to a different directory.\nWhen a job is submitted to one of the Gadi GPU queues (i.e. gpuvolta and dgxa100), the job log will also contain information related to the GPU usage (lines 9-10):\n                  Resource Usage on 2025-10-03 11:05:11:\n   Job Id:             151522816.gadi-pbs\n   Project:            aa00\n   Exit Status:        0\n   Service Units:      0.96\n   NCPUs Requested:    12                     NCPUs Used: 12\n                                           CPU Time Used: 00:03:13\n   Memory Requested:   95.0GB                Memory Used: 9.81GB\n   NGPUs Requested:    1                 GPU Utilisation: 86%\n                                         GPU Memory Used: 9.02GB\n   Walltime requested: 00:10:00            Walltime Used: 00:01:36\n   JobFS requested:    2.0GB                  JobFS used: 53.9KB\n======================================================================================",
    "crumbs": [
      "Running a job",
      "Job monitoring"
    ]
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#monitoring-resource-usage-in-real-time",
    "href": "notebooks/09_job_monitoring.html#monitoring-resource-usage-in-real-time",
    "title": "Monitoring your job",
    "section": "Monitoring resource usage in real time",
    "text": "Monitoring resource usage in real time\n\nMonitoring CPU jobs\nTo view the CPU and memory utilisation of a running job, you can use the bespoke nqstat_anu utility (ANU = The Australian National University, where Gadi is housed).\nBy default, running the command nqstat_anu will report on all jobs for the current user under their default project, providing the following details:\n\n% CPU utilisation\nwalltime used\nwalltime requested\nRSS\nmaximum memory used by the job\nmemory requested\nCPUs requested\nwhether the job is queued or suspended\n\nRun nqstat_anu -h for more options.\n\n\n\n\n\n\nTipSwitching default project\n\n\n\nTo change your default project, run switchproj &lt;nci-project-id&gt;, providing the ID of the project you want to switch to. If you don’t know what your default project is, run the command nci_account with no arguments. Since -P &lt;nci-project-id&gt; is not supplied, it will report on your default project.\n\n\n\n\nMonitoring GPU jobs\nNCI provides a useful utility called gpustat to monitor GPU utilisation. To access this tool, you need to first join the project dk92 via MyNCI portal.\nOnce your request has been approved, you can follow the instructions provided on the NCI User Guide to use gpustat. Please note that, while the qstat command accepts both the full and short formats of jobID, gpustat-run only takes the full ID string (i.e. including the .gadi-pbs part) to initialise the session.\nA help information will display on your terminal when gpustat executes successfully. This should include a command to run on your local machine, following the format of\nssh -N -L &lt;local port&gt;:&lt;Gadi GPU hostname&gt;:&lt;remote port&gt; &lt;your Gadi username&gt;@gadi.nci.org.au\ngpustat leverages SSH port forwarding (also known as SSH tunneling) to securely tunnel network connections through an encrypted SSH session. This is achieved by running the SSH command on your local terminal with the -L flag enabled. For users who are working with a CLI tool (such as a Mac or Git terminal), you can simply copy the command and then execute it. It will appear hanging because all the command does is forwarding the port so you can access the information on your local machine.\nFor GUI applications such as MobaXterm, you can create an SSH tunnel using its built-in SSH session manager:\n\nOpen MobaXterm and create a new SSH tunnel. \nIn the session settings, fill in the following information. \n\n\nForwarded port: This is the local port number you will type in your browswer (e.g. 3000) to access to the monitoring dashboard. It does not have to be the same number as the remote port on Gadi (which changes everytime you rerun gpustat-run).\nSSH server: This configures the SSH server information, including the server url (gadi.nci.org.au), your gadi username (e.g. jf1411), and the port number (default number for SSH connection is 22).\nRemote server: This includes adding the GPU server hostname (e.g. gadi-gpu-v100-0095) as the “Remote server” and setting the “Remote port”. Such information can be both found in the gpustat-run output.\n\n\nClick “Save” and then the “Start/stop” button to establish the tunnel.\n\nOnce the tunnel is established, you can access the gpustat web page on your local machine, i.e. by opening http://localhost:&lt;local port&gt; in your browser.",
    "crumbs": [
      "Running a job",
      "Job monitoring"
    ]
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#monitoring-job-disk-usage",
    "href": "notebooks/09_job_monitoring.html#monitoring-job-disk-usage",
    "title": "Monitoring your job",
    "section": "Monitoring job disk usage",
    "text": "Monitoring job disk usage\nEach project has an allocation of physical disk space and inodes on scratch, gdata and home. When running jobs on Gadi, it’s important to ensure you have sufficient allocation remaining to meet the needs of the job you are running, as exceeding the limit of these resources can cause a running job to fail. Please see the section on disk and inode usage for more details and how to query usage of these resources.",
    "crumbs": [
      "Running a job",
      "Job monitoring"
    ]
  },
  {
    "objectID": "notebooks/00_gadi_access.html",
    "href": "notebooks/00_gadi_access.html",
    "title": "Accessing NCI",
    "section": "",
    "text": "University of Sydney researchers are entitled to fully-subsidised access to NCI facilities through the Sydney Scheme. Access to NCI systems will be an ongoing feature of the Sydney Research Cloud initiative.\n\n\n\n\n\n\nNoteAttention researchers\n\n\n\nPlease see the University of Sydney Research Computing documentation for details on how to access NCI.",
    "crumbs": [
      "Access Gadi"
    ]
  },
  {
    "objectID": "notebooks/04_command_line.html",
    "href": "notebooks/04_command_line.html",
    "title": "Command line environment",
    "section": "",
    "text": "Working on Gadi will require you to have reasonable confidence on the Linux/Unix command line.",
    "crumbs": [
      "Working on Gadi",
      "Command line"
    ]
  },
  {
    "objectID": "notebooks/04_command_line.html#linux-command-line-training",
    "href": "notebooks/04_command_line.html#linux-command-line-training",
    "title": "Command line environment",
    "section": "Linux command line training",
    "text": "Linux command line training\nNCI host a number of training sessions, including introductory Linux. Please visit their training calendar.\nThere are numerous self-directed training tutorials online, such as this one from Sandbox.bio.\nA bash cheatsheet such as this one from NCI or this one can also be helpful in increasing your familiarity with the command line.\nDue to the plethora of high-quality introductory command line materials available online, we will not reproduce those here.",
    "crumbs": [
      "Working on Gadi",
      "Command line"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html",
    "href": "notebooks/08_job_script.html",
    "title": "Running jobs on Gadi",
    "section": "",
    "text": "In this section, we will compare Artemis job scripts to Gadi job scripts, and provide some guidance on how to adapt your Artemis workflow to NCI Gadi HPC. This will include the similarities and differences with the PBS directives component of your scripts, selecting the right compute queue for your job based on Gadi’s queue hardware and resource limits, and how to work around the lack of internet access for Gadi compute queues.\nWatch the pre-recorded session",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#introduction",
    "href": "notebooks/08_job_script.html#introduction",
    "title": "Running jobs on Gadi",
    "section": "",
    "text": "In this section, we will compare Artemis job scripts to Gadi job scripts, and provide some guidance on how to adapt your Artemis workflow to NCI Gadi HPC. This will include the similarities and differences with the PBS directives component of your scripts, selecting the right compute queue for your job based on Gadi’s queue hardware and resource limits, and how to work around the lack of internet access for Gadi compute queues.\nWatch the pre-recorded session",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#job-scheduler",
    "href": "notebooks/08_job_script.html#job-scheduler",
    "title": "Running jobs on Gadi",
    "section": "Job scheduler",
    "text": "Job scheduler\nLike Artemis, NCI runs the Altair PBS professional workload manager.\nWhile you can run simple commands on the login nodes on Gadi, commands are throttled. This means they execute more slowly than on the compute nodes and are automatically killed if they exceed CPU, memory and time restrictions that are in place. This is to ensure the login nodes are not over-loaded and remain responsive for all users. As such, all complex or resource-intensive tasks must be submitted to the job scheduler for execution on the cluster nodes.\nSubmitting jobs on Gadi is very similar to submitting jobs on Artemis. You will submit a PBS (Portable Batch System) submission script that specifies your job’s compute requirements along with the commands to execute the tasks.\nPBS scripts are text files that contain directives and commands that specify the resources required for a job and the commands to run. Typically they are named &lt;script_name&gt;.pbs however the .pbs suffix is not required, merely helpful to discern the intention of the script.\nOnce submitted to the PBS job scheduler with the qsub command, the scheduler reads the compute requirements from the directives component of the script, and either runs your job right away (if the requested resources are immediately available) or queues the job to run later (if the requested resources are not currently available).",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#job-scheduling-priority",
    "href": "notebooks/08_job_script.html#job-scheduling-priority",
    "title": "Running jobs on Gadi",
    "section": "Job scheduling priority",
    "text": "Job scheduling priority\nOn Artemis, you will have some familiarity with the concept of fair share use, where compute jobs you run increase your project’s ‘fair share weight’ which temporarily decreases the priority of your jobs in the queue. This is not the case on Gadi, where all jobs have equal priority. The only factors that limit how quickly your jobs leave the queue and start running are the resources you request combined with current resource availability. In order to have your job be queued (and not ‘held’ after submission), you must have sufficient KSU in your project. This will be described under queue charge rates.",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#pbs-directives",
    "href": "notebooks/08_job_script.html#pbs-directives",
    "title": "Running jobs on Gadi",
    "section": "PBS directives",
    "text": "PBS directives\nPBS directives outline your job’s resource needs and execution details. Each directive starts with #PBS in order to directly communicate with the job scheduler and not be confused with other code or comments in your script. The directives section should sit at the top of your script, with no blank lines between them, and any commands required to perform your compute task follow below the last directive.\nBelow is a simple example of the PBS directives portion of a Gadi job script. For details on more options, please see the NCI Gadi PBS directives guide.\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -q normal\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l jobfs=200GB\n#PBS -l walltime=02:00:00\n#PBS -l storage=scratch/aa00+gdata/aa00\n#PBS -l wd\n\n-P: Project code for resource accounting. Must be a valid NCI project code of which you are a member\n-q: Queue selection (e.g., normal or hugemem). See Gadi’s queue structure and queue limits pages for more details\n-l ncpus: Number of requested CPUs\n-l mem: amount of requested memory\n-l jobfs: Local-to-the-node disk space on the compute node\n-l walltime: Requested job walltime. Your job will only be charged for the walltime it uses, not the maximum walltime requested\n-l storage: Filesystems your job will access. /scratch/&lt;project&gt; is accessible by default. To access any other scratch or gdata locations, list them here. Note to use no spaces or leading / characters\n-l wd: Set the working directory to the submission directory. This is equivalent to cd $PBS_O_WORKDIR",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#differences-between-artemis-and-gadi-pbs-scripts",
    "href": "notebooks/08_job_script.html#differences-between-artemis-and-gadi-pbs-scripts",
    "title": "Running jobs on Gadi",
    "section": "Differences between Artemis and Gadi PBS scripts",
    "text": "Differences between Artemis and Gadi PBS scripts\n\nThe -l storage directive is required on Gadi but not Artemis. Failure to include required storage locations will kill the job, for example with No such file or directory errors\nOn Gadi, users must review their resource requirements against the queue structure and limits in order to request a specific queue. On Artemis, the scheduler managed this automatically according to requested resources and queue loads\nMaximum walltime for any queue is 48 hours. For large numbers of nodes requested in a single job, the maximum walltime reduces. This is described in the queue limits page. See Working within walltime limit for more details\nThe requested resources are checked against the quantity of remaining KSU in the project specified at -P. If there is insufficient KSU to run the job, the job will be held. This will show as H status when the job is queried with qstat. See queue charge rates for more details\nJob arrays (eg #PBS J 1-1000) are not permitted on Gadi. See Parallel jobs and nci-parallel for more details\nUnlike Artemis, Gadi compute nodes lack internet access. If you have a job script that relies on an external network call such as reading from a live database, you will need to adapt your method (for example pre-downloading the required information with copyq before running the compute job) or use an alternate platform such as Nirin\n\nBelow is an example Artemis job script:\n#!/bin/bash\n\n#PBS -P &lt;USyd project code&gt;\n#PBS -N myjobname\n#PBS -l walltime=02:00:00\n#PBS -l select=1:ncpus=4:mem=16gb\n#PBS -q defaultQ\n\nmodule load python/3.12.2\n\ncd $PBS_O_WORKDIR\n\npython3 ./myscript.py ./myinput\nThe same job script, adjusted for Gadi:\n#!/bin/bash\n\n#PBS -P &lt;NCI project code&gt;\n#PBS -N myjobname\n#PBS -l walltime=02:00:00\n#PBS -l ncpus=4\n#PBS -l mem=16GB\n#PBS -q normal\n#PBS -l storage=scratch/bb11+gdata/aa00+gdata/bb11\n#PBS -l wd\n\nmodule load python3/3.12.1\n\npython3 ./myscript.py ./myinput\nAs you can see, there is very little difference between these two scripts. They both request 4 CPUs, 16 GB RAM, and 2 hours walltime. They both change the working directory to the submission directory, they both load python (different versions as available on the system) and both run the same job command.\nThe command to submit this script is also the same on Artemis and Gadi:\nqsub run_my_python_script.pbs\nAdapting your existing Artemis job scripts to Gadi should be fairly simple for most users, beginning with adjusting the directives and establishing required software. See Software for more details on Gadi software availability.",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#selecting-the-right-queue",
    "href": "notebooks/08_job_script.html#selecting-the-right-queue",
    "title": "Running jobs on Gadi",
    "section": "Selecting the right queue",
    "text": "Selecting the right queue\nArtemis defaultQ routed jobs to the appropriate queue based on directives and resource availability. Gadi requires users to directly specify the appropriate queue.\nTo select the queue, you match up the resources your job needs to the queue limits, also factoring in the charge rate.\nView the available queues on the Gadi queue structure page. Note that there are:\n\ngeneral purpose queues\nlarge memory queues\nexpress queues\nGPU queues\ndata transfer queue (copyq)\n‘Cascade Lake’ and ‘Broadwell (ex-Raijin)’ queues\n\nThe Cascade Lake nodes are newer hardware and thus faster than the Broadwell nodes. Raijin was the previous NCI HPC, decommissioned in 2019\nThey have a lower charge rate than the equivalent Cascade Lake queue, and this can be utilised to help minimise compute cost when the reduced processor speed is not overly detrimental to the job or your research timeline\nThey have different numbers of CPU (48 or 28) and different total memory per node\n\n\nEach queue has different hardware, limits, and charge rates. Before submitting any jobs on Gadi, it is important to review this page along with the queue limits page which describes each queue in more detail.\nYou will note that each queue also has a corresponding queue that ends in -exec. You cannot submit directly to the -exec (execution) queue. Your jobs will be placed on the execution queue via the ‘route queue’ that you submit to. For example, for a job you want to run on the Cascade Lake normal queue, you will include the directive #PBS -q normal (submit to route queue) and the job will run on normal-exec (execution queue).\n\nQueue charge rates\nBy now you should be familiar with the concept of an NCI service unit (SU, or sometimes KSU for 1,000 SU or MSU for 1 million SU).\nEach new NCI project under the Sydney Scheme is granted 1 KSU by default, and requests can be made for more as required.\nA service unit is based on a CPU hour, ie ‘one hour of walltime on one CPU’. Each queue has a different charge rate applied to the CPU hour, so that one CPU hour on a given resource may cost between 1.25 SU and 6 SU, depending on the charge rate for that queue. More specialised and scarce resources are charged at a higher rate to ensure that only users who genuinely need these use them.\nThe charge rates can be found in column 4 of the queue limits table.\nIt’s important to understand that requested memory also impacts the charge rate, not just the requested CPU, walltime and queue. In each queue, a CPU has an allocated amount of memory for accounting purposes. For example, in the Cascade Lake normal queue, there are 48 CPU and 192 GB total RAM. The amount of memory per CPU for accounting purposes is therefore 192 / 48 = 4 GB. If you request 1 CPU and 4 GB RAM, only the CPU affects the charge rate, as you are using only the memory allocation for one CPU. If however you request 1 CPU and 8 GB RAM, your charge rate will be based off 2 CPU of use, since you are using the memory allowance of 2 CPU. Note this is ‘for accounting purposes’ only, ie it is technically feasible for your job to run on 1 CPU and access 8 GB RAM (or more). This accounting is described on the NCI job costs page, and will also be summarised below.\nDon’t be alarmed by the charge rates: please submit your job to the most appropriate queue based on required resources. The accounting method combined with stricter walltimes, newer hardware and software, and more vast physical resources compared to Artemis will likely see your compute jobs complete in a faster turnaround time compared to what you are used to.\nUnderstanding charge rates is important for two main reasons:\n\nJudicious use of resources. KSU is provided to you in-kind by The University of Sydney. It is your responsibility to ensure efficient use of these resources. Selecting the appropriate queue for your job avoids wastage and avoids unnecessary impacts on other users of this national resource.\nEnsuring your job can run. Jobs can only leave the route queue and join the execution queue if sufficient SU are available to the project assuming the job runs for its full requested walltime.\n\nFor example, if your project has 1 KSU and you submit a job script with the following directives:\n#PBS -q hugemem\n#PBS -l ncpus=48\n#PBS -l mem=1470GB\n#PBS -l walltime=12:00:00\nYour job will not join the compute queue - it will be held, showing a status of H when qstat is run. The reason for hold status is that the requested job requires more service units than the project has available.\nYou can view your project budget with the following command:\nnci_account -P &lt;nci-project-code&gt;\nThis will show the total allocated for the current quarter, the amount used, the amount reserved (by running or queued jobs), and the amount available. Any new job you submit MUST request less than the amount available.\nThe required SU available to run the job can be calculated by the formula:\nwalltime-hours * MAX (CPU|MEM) * charge-rate\nwhere MAX is based on the greater value of CPUs or proportion of node memory requested.\nso for the above example:\n12 h * 48 CPU * 3 charge rate = 1728 SU\nIn this case, MAX is based on CPU, since the per-CPU memory request (1470 GB / 48 CPU = 30.625 GB) is less than or equal to the maximum proportion of memory per CPU on the hugemem queue.\nSince 1728 SU is more than the 1 KSU the project has available, the job cannot run. You will need to either:\n\nObtain more KSU\nReconfigure your job to fit under the 1 KSU you have available.\n\nYou might consider reducing walltime, CPUs, change the queue, etc, depending on your job and what you expect are its minimum viable resource requests. You can do this by:\nKilling the job (qdel &lt;jobID&gt;), editing the directives and resubmitting, OR use the qalter PBS command to reduce the resource requests of the held job.\nFor the above example, let’s assume the requested walltime of 12 hours was an extremely conservative estimate and realistically you expect the job should complete in less than 2 hours. You could run this command:\nqalter -l walltime=02:00:00 &lt;jobid&gt; \nThis would reduce the SU for the submitted job to 288 SU and the job would then be picked up by the next scheduling cycle and enter the queue.\n\n\nQueue selection examples\n\nExample 1\nYou have a small job that only uses a single CPU and 2 GB RAM, but will run for a whole day. Which of the queues would be appropriate?\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nnormal, normalbw, express, expressbw. While you could use the express queues, the charge rate is higher so the non-express normal queues would be more economical.\n\n\n\n\n\nExample 2\nYou have a job that requires 384 GB memory and 12 CPU. Which queue would you use?\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nhugemem, with 12 CPU and 384 GB memory, or hugemembw, with 14 CPU as CPUs must be requested in multiples of 7 on this queue.\nWhich would be the better choice?\nIf the job ran for 2 hours, charge rate would be:\n\nhugemem: 12 CPU * 2 h * 3 charge rate = 72 SU\nhugemembw: 14 CPU * 2 h * 1.25 charge rate = 35 SU\n\nHugemem may execute faster with the newer hardware, yet hugemembw may consume less KSU. hugemembw also has more mem per CPU than hugemem (36 GB vs 32 GB). Benchmarking will demonstrate which of these configurations is more suited to your job.\nSee job efficiency for tips to determine the best compute resources for your job.\n\n\n\n\n\nExample 3\nYou have a job that requires 20,000 CPU. Fill in the below directives for this job, including the maximum permissible walltime:\n#PBS -l ncpus=&lt;value&gt;\n#PBS -l mem=&lt;value&gt;\n#PBS -l walltime=&lt;value&gt;\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\n#PBS -l ncpus=20016\n#PBS -l mem=3803040GB\n#PBS -l walltime=05:00:00\nWhy 20,016? When requesting &gt;1 node on Gadi, only whole nodes can be requested. So to reach 20,000 CPU in a single job would require the use of the Cascade Lake normal queue, where the nodes have 48 CPU per node, and this would be 20,000 * 48 = 416.7 nodes, so we need to round up to 417 nodes, which is 417 * 48 = 20,016 CPU.\nWhy 5 hour walltime not 48 hours? As the quantity of CPU requested increases, maximum walltime goes down. This information can be found in the last column on the queue limits table. 5 hours is the maximum amount of walltime allowed for jobs requesting more than 3024 CPUs (63 nodes) in this queue. To request the maximum walltime of 48 hours on this queue, the job must request at most 672 cores (14 nodes).\nIf your job required exactly 20,000 CPU, you would simply provide this hard-coded value to the relevant command. The number of requested KSU to the job can be accessed from the environment variable $PBS_NCPUS.\n\n\n\n\n\nExample 4\nYour job requires GPUs. Which queues could you use?\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\ngpuvolta or dgxa100 queue",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#lack-of-internet-access-on-compute-nodes",
    "href": "notebooks/08_job_script.html#lack-of-internet-access-on-compute-nodes",
    "title": "Running jobs on Gadi",
    "section": "Lack of internet access on compute nodes",
    "text": "Lack of internet access on compute nodes\nThe only Gadi queue with internet access is copyq. This queue is not suitable for running compute tasks. It allows only single-core jobs and has a maximum walltime of 10 hours. Jobs that require up-to-date information retrieval from external servers have a few options:\n\nSplit the job into two parts: a download or web query task submitted to copyq, ensuring that the retrieved data is saved to persistent disk (ie not the local-to-the-node SSD storage that is deleted upon job completion), followed by a a compute job submitted to one of the appropriate compute queues, reading in the required inputs saved from job 1.\nRun the job via ARE, which provides a graphical user interface run on Gadi’s compute queues plus internet access capability.\nUse NCI’s Nirin cloud instead of Gadi.",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#submitting-a-pbs-script",
    "href": "notebooks/08_job_script.html#submitting-a-pbs-script",
    "title": "Running jobs on Gadi",
    "section": "Submitting a PBS script",
    "text": "Submitting a PBS script\nLike on Artemis, the qsub command is used to submit the job to the scheduler. Please visit the NCI Gadi job submission page if you require more details on this.\nAfter your job is submitted, job monitoring and job logs are very similar to your experience on Artemis. Please see job monitoring for more details.",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#interactive-jobs",
    "href": "notebooks/08_job_script.html#interactive-jobs",
    "title": "Running jobs on Gadi",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for jobs that require user input feedback as an analysis progresses, or can be useful for testing commands/tools prior to submitting a full job via a PBS script.\nRunning an interactive job on Gadi is very similar to an Artemis interactive job: you provide the relevant directives on the command line rather than from within a script, and include -I instead of #PBS -q &lt;queue&gt;.\nFor example, to start an interactive job with 4 CPU for 1 hour, enter the following command on the Gadi login node:\nqsub -I -P &lt;nci-project-code&gt; -l walltime=00:01:00,ncpus=4,mem=16GB,storage=&lt;required-storage-paths&gt;,wd\nAfter you enter the command, you will receive a message\nqsub: waiting for job &lt;id&gt;.gadi-pbs to start\nOnce your interactive job has left the queue and started, you will receive a message\nqsub: job &lt;id&gt;.gadi-pbs ready\nNotice that your command prompt has changed, indicating the node ID you are on instead of the login node ID.\nYou can then interactively enter the commands required for your compute task. To terminate the interactive job, enter exit.",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/08_job_script.html#persistent-sessions",
    "href": "notebooks/08_job_script.html#persistent-sessions",
    "title": "Running jobs on Gadi",
    "section": "Persistent sessions",
    "text": "Persistent sessions\nTo support the use of long-running, low CPU and low memory demand processes, NCI provides a persistent sessions service on Gadi. This service is primarily designed for the use of workflow management tools (eg nextflow) that automatically submit and monitor PBS jobs to the Gadi compute queues.\nWorkflow management tools are a unique use case where the ‘head job’ requires internet access (provided through the persistent session) and access to the scheduler to submit a series of chained compute jobs to various queues depending on the unique workflow configuration.\nThis service is NOT designed for computational work, large downloads, or other intensive tasks. These jobs should be submitted to the appropriate PBS queues.",
    "crumbs": [
      "Running a job",
      "Job scripts and submission"
    ]
  },
  {
    "objectID": "notebooks/03_expectations.html",
    "href": "notebooks/03_expectations.html",
    "title": "What does the system expect of its users?",
    "section": "",
    "text": "Gadi is a shared resource and its efficient use not only ensures fair access for all users but also helps minimise the environmental impact of high-performance computing, as systems like Gadi consume significant energy resources. When you are using a system like Gadi, there are potentially hundreds of other users accessing the system at the same time as you. For Gadi to remain efficient and usable, everyone needs to be courteous and use the system with consideration for others.\nTo help you be a good citizen of the NCI HPC community, please review the do’s and don’ts of using Gadi as well as the general tips below.",
    "crumbs": [
      "Introduction to Gadi",
      "Infrastructure expectations"
    ]
  },
  {
    "objectID": "notebooks/03_expectations.html#use-job-queues-appropriately",
    "href": "notebooks/03_expectations.html#use-job-queues-appropriately",
    "title": "What does the system expect of its users?",
    "section": "Use job queues appropriately",
    "text": "Use job queues appropriately\n\nGadi job queues\nGadi queue limits\n\nGadi runs a PBSpro job scheduler that manages the allocation of resources to users. When you submit a job, it is placed in a queue and will run when the requested resources become available. Unlike on Artemis where your job is allocated to a suitable queue based on your resource request, Gadi users need to explicitly request their job is sent to a specific queue. It is important for you to pick a job queue that is appropriate for your job.",
    "crumbs": [
      "Introduction to Gadi",
      "Infrastructure expectations"
    ]
  },
  {
    "objectID": "notebooks/03_expectations.html#responsibly-manage-your-data",
    "href": "notebooks/03_expectations.html#responsibly-manage-your-data",
    "title": "What does the system expect of its users?",
    "section": "Responsibly manage your data",
    "text": "Responsibly manage your data\n\nNCI file management policy\nTransferring data between RDS and Gadi\n\n/scratch is not a safe space for long term data storage. If it has not been accessed in 100 days, it will be subjected to NCI’s clean up policy. If you have a /g/data allocation, this is a better place to store data that you will regularly need as input for your jobs. While gdata is not subjected to purge like scratch, it is not backed up. You should back up all input, job scripts and important output files to RDS. Please follow the data transfer between Gadi and RDS guide for the best ways to do this.",
    "crumbs": [
      "Introduction to Gadi",
      "Infrastructure expectations"
    ]
  },
  {
    "objectID": "notebooks/03_expectations.html#dont-request-more-resources-than-you-need",
    "href": "notebooks/03_expectations.html#dont-request-more-resources-than-you-need",
    "title": "What does the system expect of its users?",
    "section": "Don’t request more resources than you need",
    "text": "Don’t request more resources than you need\n\nBenchmarking tasks on Gadi\n\nDon’t request resources that you won’t need, it will only result in your job and other users jobs being held up, and you wasting your service unit allocation. It can be hard to know what resources a tool needs, and this can vary on different hardware. We suggest the following:\n\nStep 1: Consult the software documentation\n\nOften, developers will outline the minimum amount of RAM (memory) and whether a tool is multi-threaded (e.g. use &gt;1 CPU or GPU)\n\nStep 2: Run a test job using our Gadi benchmarking tool\n\nThis will give you a good idea of how much resources you need to request for your main job.\n\nStep 3: Ask for help",
    "crumbs": [
      "Introduction to Gadi",
      "Infrastructure expectations"
    ]
  },
  {
    "objectID": "notebooks/03_expectations.html#keep-track-of-your-resource-usage",
    "href": "notebooks/03_expectations.html#keep-track-of-your-resource-usage",
    "title": "What does the system expect of its users?",
    "section": "Keep track of your resource usage",
    "text": "Keep track of your resource usage\nRunning jobs on gadi requires users to have sufficient service units (compute hours) available. It is also important to monitor your use of physical disk space and inodes on scratch, home and gdata. Please see the accounting section or the NCI pages below for more details:\n\nMonitor your jobs\nMonitor your project allocation\nWhat does a job cost?\nWhy are my jobs not running?",
    "crumbs": [
      "Introduction to Gadi",
      "Infrastructure expectations"
    ]
  },
  {
    "objectID": "notebooks/03_expectations.html#dont-misuse-the-login-nodes",
    "href": "notebooks/03_expectations.html#dont-misuse-the-login-nodes",
    "title": "What does the system expect of its users?",
    "section": "Don’t misuse the login nodes",
    "text": "Don’t misuse the login nodes\nLogin nodes are for logging in to the system, basic file and directory navigation commands, submitting jobs etc. Login nodes are not for large data transfers, compute tasks, excessive qstat queries, or submitting jobs via high-iteration for loops. Doing so will overload these nodes, causing a slow-down and frustration for all users. NCI monitors login node traffic and inappropriate use will be targeted. Please see the sections on data transfer and parallel jobs for recommended strategies for these tasks.",
    "crumbs": [
      "Introduction to Gadi",
      "Infrastructure expectations"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html",
    "href": "notebooks/06_accounting.html",
    "title": "Accounting",
    "section": "",
    "text": "Gadi does not use the ‘fair share’ policy that Artemis users are familiar with. Gadi uses the concept of a ‘service unit’ (SU) and each project has a finite allocation of SU (often referred to as KSU for 1,000 SU) which they cannot exceed. NCI projects on Gadi also have fixed allocations with scratch, and this includes physical space as well as iNodes. Users are responsible for managing their SU and storage allocations. NCI allocates scratch and iNode limits, and SIH allocates gdata limits.\nIn this section, we will describe NCI accounting of KSU, disk and iNode limits.",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#introduction",
    "href": "notebooks/06_accounting.html#introduction",
    "title": "Accounting",
    "section": "",
    "text": "Gadi does not use the ‘fair share’ policy that Artemis users are familiar with. Gadi uses the concept of a ‘service unit’ (SU) and each project has a finite allocation of SU (often referred to as KSU for 1,000 SU) which they cannot exceed. NCI projects on Gadi also have fixed allocations with scratch, and this includes physical space as well as iNodes. Users are responsible for managing their SU and storage allocations. NCI allocates scratch and iNode limits, and SIH allocates gdata limits.\nIn this section, we will describe NCI accounting of KSU, disk and iNode limits.",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#what-are-the-important-accounting-metrics",
    "href": "notebooks/06_accounting.html#what-are-the-important-accounting-metrics",
    "title": "Accounting",
    "section": "What are the important accounting metrics?",
    "text": "What are the important accounting metrics?\n\nService units: The charge rate per resource hour\nDisk usage: The amount of physical disk space used on scratch, home, and gdata\niNode usage: The number of files and directories stored on scratch, home, and gdata",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#service-units",
    "href": "notebooks/06_accounting.html#service-units",
    "title": "Accounting",
    "section": "Service units",
    "text": "Service units\nFor a detailed description of the NCI SU and why 1 SU does not equal 1 CPU hour, see here\nFor every resource hour you consume on an NCI compute platform, you are charged at a specific rate. As a simple example the normal queue on Gadi has a charge rate of 2, so a job using 1 CPU for 1 hour will be charged 2 SU.\nWe often speak in KSU (1 KSU = 1,000 SU) for simplicity. Under the Sydney Scheme, you can easily request more KSU as you need it from the management portal.\nBefore submitting a job on Gadi, it is important to:\n\nCalculate the amount of SU the job will require\nCheck your available SU to ensure there is sufficient for the job to run\n\n\nCalculate the amount of SU the job will require\nIn the simple example above, we decided that our 1 CPU 1 hour normal queue job would cost 2 SU. There is actually another factor to consider, and that is memory. Each queue has nodes of a specific CPU:memory ratio, and if your job consumes more memory than this ratio allows, you will be charged based on the memory used. From the NCI job costs page:\n“However, some jobs will request less CPUs and more memory. When this happens, you are taking memory away from the other CPUs in the node and will be charged accordingly, as other users can’t access those CPUs while your job is using that memory allocation”\nSo let’s assume our 1 CPU job requests 12 GB memory. Checking the Gadi queue structure page, under the Intel Xeon Cascade Lake dropdown, the normal nodes have the following hardware:\n\n2 x 24-coreIntel Xeon Platinum 8274 (Cascade Lake) 3.2 GHz CPUs per node\n192 GiB RAM per node\n\nFrom here we can determine that the CPU:mem ratio is 48:192 = 1:4 (1 CPU per 4 GB mem). By requesting 12 GB mem for our 1 CPU job, we are using 3 times as much mem per CPU as the ratio governs for this queue. So, we are likewise charged 3 times as much, or in other words, we are charged based on the MEMORY rather than the CPU.\nThe equation for every job run on Gadi is charged using the formula:\nSU = Queue Charge Rate  ✕  Max (NCPUs, Memory Proportion)  ✕  Walltime Used (Hours)\nFor our example, this expands to:\nSU = 2 charge rate X 3 memory proportion X 1 hour\nSU = 6\n\nChallenge 1: calculate this job’s SU cost\nUse the queue limits and queue structure pages to help find the answer.\n#PBS -P MYPROJECT\n#PBS -l walltime=02:00:00\n#PBS -l ncpus=4\n#PBS -l mem=48GB\n#PBS -q normal\n#PBS -W umask=022\n#PBS -l wd\n#PBS -l storage=gdata/MYPROJECT\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nFirst, we need to calculate the CPU:mem ratio of our job, which in this case is 4:48 = 1:12, ie 12 GB mem requested per CPU requested.\nNext, we need to check that against the queue hardware: These nodes have 48 CPU and 192 GB RAM. So the CPU:mem ratio for this queue is 48:192 = 1:4 ie 4 GB RAM per CPU. By requesting 12 GB mem per CPU, we have requested 3 times as much mem per CPU than the queue hardware provides for, so we will be charged based off the mem not the CPU, ie instead of being charged based on 4 CPU, we will be charged for the CPUs assigned to 48 GB RAM which is 12 (48 GB mem requested divided by the queue mem per CPU value of 4).\nFinally, check the charge rate for the normal queue, which is 2 SU per resource hour.\nSU = 2 charge rate X 12 memory proportion X 2 hour\nSU = 48\n\n\n\n\n\nChallenge 2: calculate this job’s SU cost\nUse the queue limits and queue structure pages to help find the answer.\n#PBS -P MYPROJECT\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=280\n#PBS -l mem=2520GB\n#PBS -q normalbw\n#PBS -W umask=022\n#PBS -l wd\n#PBS -l storage=gdata/MYPROJECT\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nFirst, we need to calculate the CPU:mem ratio of our job, which in this case is 280:2520 = 1:9, ie 9 GB mem per CPU.\nNext, we need to check that against the queue hardware: The broadwell queues (except express) have a charge rate of 1.25 SU per resource hour. The directives have requested the normalbw or “normal Broadwell” queue, which has some nodes with 128 GB RAM and some nodes with 256 GB RAM. There is no difference in the charge rate for these nodes, and jobs are placed on either node type depending on the memory per CPU requested and resource availability at the time. These nodes have 28 CPU per node. So the CPU:mem rato for this queue is either 28:128 or 28:256. Since the 256 GB nodes are charged the same as the 128 GB nodes, we go with the higher ratio, 28:256 = 1:9.14, ie 9.14 GB mem per CPU.\nSince we requested a CPU:mem ratio of 280:2520 = 1:9, we have requested less mem per CPU than the queue allows per CPU so our charge rate will be based on CPU not mem.\nSU = 1.25 charge rate X 280 CPU X 4 hour\nSU = 1400\nKSU = 1.4\n\n\n\n\n\n\nCheck your available SU\nOnce you have established your job script/s and calculated your SU cost, you should check that you have sufficient SU budget to run the job.\nUse the below command, replacing the &lt;nci-project-id&gt; with your NCI project code:\nnci_account -P &lt;nci-project-id&gt;\nIn the below screenshot, you can see that project qc03 has used 3.24 SU of its 1 KSU allocation (Grant). There are no SU reserved (reserved SU are SU assigned to jobs currently running or in queue) and there are 996.76 SU still available for the rest of the quarter.\n\nIn order to run a job under this project code, it would need to have a computed job cost of less than or equal to 996.76 SU.\nIf a job is submitted under a project that does not have sufficient SU to cover the expected cost, the job will be held. You can reveal the reason for a held job (status = ‘H’) from the “Comment” included in the qstat -f output:\n\nThis job requires 19.2 KSU yet we know only 966 SU is available to the project. To run this job, you would need to obtain more KSU.\nIf the amount by which your job exceeds your current SU budget is small, you may consider reducing the requested resources to fit under the current budget. You can do this by either killing the job (qdel &lt;jobID&gt;), adjusting the resource requests then resubmitting, or use the qalter PBS command to reduce the resource requests of the held job via the command line. For example, to reduce CPU, mem and walltime:\nqalter -l walltime=02:00:00 -l ncpus=48 -l mem=190GB &lt;jobid&gt; \n\n\nCompleted job SU charged\nAfter your job completes, the SU consumed by the job is reported in the PBS .o job log (&lt;jobname&gt;.o&lt;jobID&gt; unless otherwise changed with the #PBS -o &lt;logname&gt; directive).\nThe charged is based off the walltime used, not the walltime requested.\n\nIn the above example, the job cost 23.94 SU. The job queue is not reported in the log, however this information is of course included in the directives of the job script. This job was run on normalbw which has a charge rate of 1.25 SU, and CPU:mem ratio of 1:9.14. Since the job requested 7 CPU and 63 GB RAM, this is below the mem:CPU ratio so the charge is based off CPU not memory:\nSU = 1.25 charge rate X 7 CPU X 2.74 hours\nSU = 23.94",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#disk",
    "href": "notebooks/06_accounting.html#disk",
    "title": "Accounting",
    "section": "Disk",
    "text": "Disk\nNew projects are assigned 1 TB disk quota and 202 K inode quota on Gadi’s scratch filesystem, and these can be increased with justification by contacting the NCI help desk.\nYou can check your project allocation with the same nci_account -P &lt;nci-project-id&gt; command used to query SU:\n\nThis project has used 4.5 TB of its 10 TB of gdata and 4.06 TB of its 60 TB allocated in scratch.\nAnother way to check disk usage is with the lquota command, which will report filesystem usage for all projects of which you are a member:\n\nNote the extra columns here - you will see that in addition to the amount of disk used and allocated, there are also Limit columns. This is a grace amount of additional resources that your project may use, but only for a short time. This is to try and help jobs complete that may otherwise have exceeded the allocation and failed, causing unnecessary wastage of compute resources. You cannot submit any new jobs under a project code that has exceeded the limit; the jobs will be held (status H) until you have brought your disk usage back under the allocation quota. Your project cannot exceed the limits. If a running job causes the limit to be exceeded, the job will fail. If a running job exceeds the allocation but remains under the limit, the job will continue.\nNCI implements a scratch purge policy, where files not accessed for 100 days are moved into quarantine for 14 days before permanent deletion. Files within quarantine still count towards project quotas.\nIf you are a member of multiple projects, understand that the filepath of the files and not the group ownership of the files dictates the project to which the disk resources are accounted. Please see this NCI update for more information.",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#inodes",
    "href": "notebooks/06_accounting.html#inodes",
    "title": "Accounting",
    "section": "iNodes",
    "text": "iNodes\nAn inode (index node) is a data structure used by Unix-like file systems to store metadata about a file or directory. In simple terms, each file and directory contributes an inode count of 1, ie the inode usage of a filesystem is the count of all files and directories stored on that filesystem. Like physical disk space, a filesystem also has an inode limit, and if they get used up (even if there’s still free disk space), you won’t be able to create new files. This can cause failure of a running job.\nMost users will not need to worry about inode limit. For each TB of scratch or gdata disk assigned to a project, an amount if inode that is suitable for most users is also provided. If however you run tools or jobs which generate large numbers of tiny files, you may need to implement close inode monitoring and management, such as periodically deleting or archiving tiny temp files as they are no longer needed.\nFrom the Gadi resources page:\n\n“Please try to keep the number of files as low as possible as this can affect the I/O performance in your job. Gadi is efficient at handling large scale parallel I/O but performance becomes significantly worse when doing frequent small operations. A main culprit for creating a large amount of files is the Python packaging system conda. Please use pip and the available modules that are already tuned for Gadi to keep file and folder count as low as possible.”\n\nYou can see the inode limits and usage for your project’s scratch and gdata with the nci_account -P &lt;nci-project-id&gt; command:\n\nThis project has been allocated (iAllocation) 4.5 million inodes in scratch, and has used (iUsed) 1.59 million inodes. With ~ 3 million inodes spare, there is plenty to continue work.\nThe lquota command shown previously also reports the inode usage, allocation and limit for all projects of which you are a member. The inode limit functions the same as the disk limit, whereby you can exceed the allocation/quota up to the inode limit, you cannot exceed the inode limit, and you cannot submit new jobs until the project is back under the inode allocation/quota.\nIf you would like to determine the total inode usage of a specific directory and its subdirectories, run this command from within that directory:\nfor d in `find -maxdepth 1 -type d |cut -d\\/ -f2 |sort`; do c=$(find $d |wc -l) ; printf \"$c\\t\\t- $d\\n\" ; done ; printf \"Total: \\t\\t$(find $(pwd) | wc -l)\\n\"",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#disk-and-inode-usage-on-home",
    "href": "notebooks/06_accounting.html#disk-and-inode-usage-on-home",
    "title": "Accounting",
    "section": "Disk and inode usage on /home",
    "text": "Disk and inode usage on /home\nEach user has a quota of 10 GB of storage in their home directory, which is private and backed up. They also have an inode allocation of 10,240 M.\nImportantly, home directories do not have a grace limit like scratch and gdata do! It is critical to actively monitor and manage your home directory usage to avoid preventable job failures.\nThe nci_account and lquota commands do not report usage on home. To view disk and inode quota and usage, use the command quota -s:\n\nThis user has filled 8643 M of physical space in their home directory, and has 109 K inodes used of the total quota of 4295 M inodes. Note there is no ‘grace’ (limit) value in home; the quotas cannot be exceeded.",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/06_accounting.html#key-points",
    "href": "notebooks/06_accounting.html#key-points",
    "title": "Accounting",
    "section": "Key points",
    "text": "Key points\n\nNCI accounts for disk usage, inode usage, and SU\nIt is your responsibility to monitor and manage your usage of these resources as you work\nUse command nci_account -P &lt;nci-project-id&gt; to check SU of a specific project for the current quarter, as well as disk and inode for that project\nUse command lquota to check disk and inode usage and allocation for all projects you are a member of\nUse quota -s to check disk and inode usage in your home directory\nRequests for more SU can be made through the Sydney Scheme management portal\nRequests for more scratch disk, scratch inode, and gdata inode can be made through the NCI help desk, and will require justification\nRequests for more gdata disk can be made by emailing the Sydney Scheme manager, and will require justification",
    "crumbs": [
      "Working on Gadi",
      "Accounting"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html",
    "href": "notebooks/10_job_efficiency.html",
    "title": "Optimising and benchmarking your job",
    "section": "",
    "text": "In this section, we will explore how optimising your workload through splitting/checkpointing jobs and optimising compute resource requests for Gadi queues can help minimise your walltime and use less service units.\nWatch the pre-recorded session",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#introduction",
    "href": "notebooks/10_job_efficiency.html#introduction",
    "title": "Optimising and benchmarking your job",
    "section": "",
    "text": "In this section, we will explore how optimising your workload through splitting/checkpointing jobs and optimising compute resource requests for Gadi queues can help minimise your walltime and use less service units.\nWatch the pre-recorded session",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#what-is-workflow-optimisation",
    "href": "notebooks/10_job_efficiency.html#what-is-workflow-optimisation",
    "title": "Optimising and benchmarking your job",
    "section": "What is workflow optimisation",
    "text": "What is workflow optimisation\nIn short, to optimise a workflow or code is to make it faster and more efficient. This can be achieved through a number of practices:\n\nEfficient resource allocation\n\nSelecting the right number of CPUs, memory and queue for the job\nAvoiding overallocation (wasting resources) or underallocation (causing slow performance)\n\nParallelisation and scaling\n\nSplitting/checkpointing jobs into numerous smaller jobs to enable parallel computing and job resume\nBalancing workload distribution to avoid bottlenecks caused when some cores are idle while others are overloaded\n\nI/O optimisation\n\nSelecting the right filesystem for I/O, for example using compute node SSD for temp files\nUsing efficient file formats\nAvoiding excessive small file creation to prevent inode exhaustion\n\nMemory management\n\nEnsuring a job fits within allocated RAM to avoid swapping to disk\nUsing efficient data structures to reduce memory footprint\n\nSoftware and algorithm optimisation\n\nChoosing the best algorithm for the task\nUsing binaries compiled on Gadi and optimised libraries\nEnabling hardware acceleration with GPU where possible\n\n\nOptimisation is crucial in HPC because it facilitates faster, more efficient, and cost-effective execution of computational workload. As researchers we want fast results but it is also our moral imperative to minimise our carbon footprint while using HPC. We also have a shared responsibility to other users of the system to make efficient use of the resources we reserve for our jobs, and to The University of Sydney, which funds 100% of our compute with NCI.\nIn this section, we will look at one simple approach to optimisation through compute resource benchmarking as a quickstart way of adapting your Artemis workflows to Gadi and selecting the right CPU, mem and queue for your Gadi job. In order to optimise in other ways, for example testing different file formats, I/O filesystems, algorithm types, GPUs, etc, a similar systematic approach could be followed: run the same analysis with the relevant differences, and compare the job performance.\nThere are complex job tracing tools for fine-tuned code optimisation; we will not be covering these, instead focusing on an overall picture of CPU and memory efficiency to guide appropriate resource selection.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#compute-resource-benchmarking-vs-scientific-benchmarking",
    "href": "notebooks/10_job_efficiency.html#compute-resource-benchmarking-vs-scientific-benchmarking",
    "title": "Optimising and benchmarking your job",
    "section": "Compute resource benchmarking vs scientific benchmarking",
    "text": "Compute resource benchmarking vs scientific benchmarking\nPlease note that this section covers compute resource benchmarking and not scientific benchmarking. While compute resource benchmarking focuses on execution speed and efficiency, scientific benchmarking focuses on whether computational results are correct, reproducible, and scientifically valid. Scientific benchmarking is extremely important, but not the focus of this training. Please do undertake scientific benchmarking, but understand that it is not covered within this section. It may be that changing job parameters can also influence the optimum resources required, such that once you have selected the right resources based on benchmarking for one set of parameters, you may need to re-run compute benchmarking for a different set of parameters. This may sound tedious, however if you typically run the same types of analyses frequently, the compute benchmarking you perform can save you both time (walltime, queue time, avoiding job failures from exceeding resources) and SU in the long run.\nIn this section we will focus on selecting the right queue on Gadi, along with the CPU and memory that provides the best trade-off between walltime, CPU efficiency and memory efficiency. By optimising these aspects, we can also indirectly minimise our queue time, as an efficient job is generally a job with a shorter walltime and shorter walltimes contribute to shorter queue times.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#cpu-and-memory-efficiency",
    "href": "notebooks/10_job_efficiency.html#cpu-and-memory-efficiency",
    "title": "Optimising and benchmarking your job",
    "section": "CPU and memory efficiency",
    "text": "CPU and memory efficiency\nEfficiency can refer to CPU efficiency or memory efficiency.\nCPU efficiency can be calculated with the formula:\ncpu_e = cputime / (walltime X cpus_used)\nA CPU efficiency value of 1 indicates perfect CPU utilisation for the duration of the job. This is often achieved for single-core jobs, however typically, as a tool or code is run with multiple threads or at scale across multiple nodes, the CPU efficiency declines. Aim to maintain your workflows above 80% where possible.\nNCI monitors CPU utilisation and repeated execution of jobs with very poor CPU efficiency may be met with an email from the NCI technical team.\nA job with extremely low CPU efficiency may be permissible if it has high memory efficiency. Memory efficiency can be calculated with the formula:\nmem_e = max_mem_used / mem_requested\nBelow is a Gadi job log. All the details required to calculate CPU and memory efficiency are contained within this log:\n\n\nChallenge: calculate CPU efficiency\nFrom the above job log, use the formula cpu_e = cputime / (walltime X cpus_used) to calculate this job’s CPU efficiency.\nDo you think this job efficiently utilised CPU?\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nThe below answer converts CPU time and walltime to minutes. The same result is achieved if you use hours or seconds, as long as you use the same unit across both values.\ncpu_e = cputime / (walltime X cpus_used)\ncpu_e = 177.55 / (164.15 X 7)\ncpu_e = 0.15\nThis job did not make efficient use of the requested 7 CPU.\n\n\n\n\n\nChallenge: calculate memory efficiency\nFrom the above job log, use the formula mem_e = max_mem_used / mem_requested to calculate this job’s memory efficiency.\nDo you think this job efficiently utilised memory?\n\n\n\n\n\n\nNoteAnswer\n\n\n\n\n\nmem_e = max_mem_used / mem_requested\nmem_e = 51.14 / 63\nmem_e = 0.81\nThis job did make efficient use of the requested 63 GB memory.\n\n\n\nReflecting on the efficiencies calculated for this example job, would you consider this job to have requested appropriate resources on Gadi?\nPerfect utilisation of all 7 CPUs would result in a CPU efficiency of 1.0. If only one CPU were used with perfect efficiency, its individual contribution would be 1/7 ≈ 0.14. This job had a CPU efficiency of 0.15. This suggests that the job was a single core job with a high memory requirement. If this job requested only 1 CPU and the same 63 GB memory, the CPU efficiency calculation from the job log would be close to perfect! However, the memory efficiency and total SU charge would have remained identical.\nWhy would the user have requested 7 CPU over 1 CPU for a single-core job? By requesting 7 CPU, they have been protective of the resources they have partitioned for their job. If a request of 1 CPU and 63 GB memory was made, other users may have been able to utilise those other 6 CPU, but only if there was sufficient memory remaining on that node. Some jobs benefit from reserving extra CPUs even if they do not use them, such as memory-intensive jobs that prevent other users from overloading the node.\nNow that we have an understanding of CPU efficiency, memory efficiency, and how to calculate them, we have a means of directly comparing replicate runs of the same analysis task with differing compute resources.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-cpu-memory-and-queue-on-gadi",
    "href": "notebooks/10_job_efficiency.html#benchmarking-cpu-memory-and-queue-on-gadi",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking CPU, memory and queue on Gadi",
    "text": "Benchmarking CPU, memory and queue on Gadi\nCompared to Artemis, Gadi has newer hardware, newer software, and more diverse queue options. A burning question from Artemis users newly migrating to Gadi is how do I know what resources to request for my job?\nIf you have a fair idea of what resources your job required on Artemis and for how long, you can request similar resources on the relevant queue on Gadi, with the expectation that your job will execute more quickly due to the newer hardware and software. Whether you have this prior knowledge or not, it is worthwhile to perform benchmarking on Gadi.\n\nExample Artemis usage log\nConsider the below job log from Artemis:\n\nThis is a fairly long-running job with high CPU and memory utilisation. We could run this job with reasonable confidence on Gadi by applying the same resource requests:\n#PBS -P MYPROJECT\n#PBS -l walltime=24:00:00\n#PBS -l ncpus=16\n#PBS -l mem=96GB\n#PBS -q normal\nYet this requests a CPU:mem ratio of 16:96 = 6, which as we have learnt from the section on accounting is higher than the CPU:mem ratio on the normal queue of 4, so our job would be charged double per resource hour.\nHow else could we structure this to ensure sufficient CPU, memory, and minimise SU?\n\n16 CPU with 4 GB RAM per CPU on the normal queue, total memory request of 64 GB. Would this be sufficient, or would the job die due to inadequate memory? Memory utilisation on Artemis was 100% at 96 GB\n16 CPU with 9 GB RAM per CPU on the normalbw queue. We would have plenty of memory at 144 GB, and a low charge rate of 1.25, but how much slower would our job be on the older Broadwell nodes?\n24 CPU with 4 GB RAM per CPU on the normal queue. We would have the same amount of memory as the Artemis job, but with more CPU. Would the walltime be faster due to this extra CPU, or would it take the same walltime and thus have a lower CPU efficiency?\n\nThese are the questions that we can answer with simple compute resource benchmarking. In this example, we have a good starting point about the amount of resources the job requires. We can test different queues and different CPU and memory combinations that are around these values to obtain the most efficient values, ie those that are the best trade-off between walltime, SU, and efficiency.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-template-scripts",
    "href": "notebooks/10_job_efficiency.html#benchmarking-template-scripts",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking template scripts",
    "text": "Benchmarking template scripts\nTo make this process easier, SIH have a repository of Gadi template benchmarking scripts. This repository contains a pair of scripts designed to test single runs of a command/tool at various CPU and memory settings on different queues. It does require some modification (and carefully use and follow the guide!) to set it up, but once you know how to use this template, it can expedite testing chunks of your workflow to obtain the most efficient (ie optimised) queue and resource requests for the task. Running the gadi_usage_report.pl script from this repository will summarise the resources used by the benchmark jobs into a table that can be viewed or plotted to determine best resources.\nIt is not critical to use this template, but it can be a helpful tool if you have not benchmarked before, or if you benchmark multiple tools/code chunks regularly and want a simple and replicable method.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#tips-for-benchmarking",
    "href": "notebooks/10_job_efficiency.html#tips-for-benchmarking",
    "title": "Optimising and benchmarking your job",
    "section": "Tips for benchmarking",
    "text": "Tips for benchmarking\n\nTest individual parts of your code where possible - ie one command, one tool, one chunk of code\n\nThis enables you to determine which parts of your workflow have differing compute requirements\nParts with differing compute requirements can be allocated to different queues and resources, saving you KSU\n\nDo initial benchmarking on a small subset of your data - ie subsample, reduce sample numbers, reduce permutations, etc\nFollow up with scalability testing: Once you have refined the candidate best resources, re-run the benchmark on a representative subset (ie whole sample, more iterations) and compare the CPU efficiency\n\nIs it as good as the initial test benchmark in terms of CPU and memory efficiency?\nIf so, then go ahead and apply this setting to your full run\nIf not, re-run full benchmarks with the larger test dataset, or dig deeper into what is causing the loss of efficiency at scale\n\nEmbrace the labour of benchmarking!\n\nWhile it may seem like a time-consuming impediment to getting on with analysing your data, it can save you a lot of time and KSU down the track\nBenchmarking will make your analysis faster and use less USyd-funded resources and energy resources\nBenchmarking can prevent avoidable job failures such as a job running out of walltime or memory, which will cost more time and resources to resubmit",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#demo-benchmarking-activity-using-the-template",
    "href": "notebooks/10_job_efficiency.html#demo-benchmarking-activity-using-the-template",
    "title": "Optimising and benchmarking your job",
    "section": "Demo benchmarking activity using the template",
    "text": "Demo benchmarking activity using the template\nThe example Artemis usage log shown above ran the tool bwa-mem to map DNA sequences to a reference sequence. When benchmarking, apply your domain knowledge to determine how to best subset the data to make small test datasets that can provide benchmark results quickly and with low SU cost. For this example, each query string within the input DNA file is mapped independently, so we can simply take a short subset of the file and map that to the reference.\nWhen subsetting, it’s important to finish off benchmarking by completing one full (not subset) run at the best chosen compute resources, to ensure that the benchmark results are consistent when a large input is provided.\nIt is also important to take a meaningful subset. The minimum size may not always be the best; aim to have your jobs run for at least 5 minutes. Starting a job and its various processes has an overhead, and making the subset data too small can cause the compute resource usage summary to be affected by these background processes. For this example, the full input was 433.4 million queries. For a target run time of 5 minutes, this equates to approximately 2 million queries. However, I am so optimistic about the execution speed on Gadi that I will hazard a guess that 2 million queries will be too fast for a decent preliminary benchmark run and we should use 10 million :-)\nAfter subsetting the raw data to the target 2 million entries, next set up the template benchmarking scripts. Working on Gadi, make a directory for the tool to be benchmarked, clone the repository, and rename the scripts\ncd /scratch/&lt;nci-project-id&gt;\nmkdir benchmark-bwa\ncd benchmark-bwa/\ngit clone git@github.com:Sydney-Informatics-Hub/Gadi-benchmarking.git\ncd Gadi-benchmarking/\nmv tool_benchmark_run.sh bwa_benchmark_run.sh\nmv tool_benchmark.pbs bwa_benchmark.pbs \nNext, edit bwa_benchmark_run.sh which is a wrapper script to launch multiple PBS jobs for the same compute task at different resources. Update the run prefix, tool name and tool short name:\n\nThen, review the pre-set queue variables, and determine which CPU values you want to run on. For this case we are interested in the normal and normalbw queues, as the CPU:mem ratio on hugemem does not seem warranted by our prior runs on Artemis. It’s unlikely that the job will run on low CPU values, given the mem usage of 96 GB on Artemis, so adapt the NCPUS arrays for the normal and normalbw code chunks to exclude 1, 2 and 4 CPU for normal and 1 CPU for normalbw:\n\nThen, add the workflow code from the original script to the benchmarking template script named bwa_benchmark.pbs, ensuring to adhere to the output path and prefix requirements described in the comments. Also add your NCI project code at #PBS -P and update the lstorage paths as required for your input data:\n\nMake sure to check for the required software on Gadi. See software for options when your tools/versions aren’t globally installed on Gadi.\nAfter making the required changes to this script, the completed bwa_benchmark.pbs workflow section appears as below. Note the use of ${ncpus} for the BWA and samtools thread parameter values, and the use of ${outdir} and ${outfile_prefix} to name the outputs. These are specified within the run script, and are set up so you can run multiple benchmarks simultaneously without I/O clashes. Also note the slight adjustment to module versions and input file paths, as well as the lack of job array here compared to the Artemis script:\n################################################\n### YOUR SCRIPT HERE \n################################################\n\n# Include all the commands required to run your job\n# Use ${outdir} for output directory path\n# Use ${outfile_prefix} for output file prefix\n\n# Load modules\nmodule load bwa/0.7.17\nmodule load samtools/1.19\nmodule load samblaster/0.1.24\n\nref=./inputs/Reference/hs38DH.fasta\n\nSAMPLE=subset_10M\nfq1=./inputs/${SAMPLE}_R1.fastq.gz\nfq2=./inputs/${SAMPLE}_R2.fastq.gz\n\nbwa mem -M -t ${ncpus} $ref \\\n    -R \"@RG\\tID:${SAMPLE}_1\\tPL:ILLUMINA\\tSM:${SAMPLE}\\tLB:1\\tCN:KCCG\" \\\n    $fq1 $fq2  \\\n    | samblaster -M -e --addMateTags \\\n    -d ${outdir}/${outfile_prefix}.disc.sam \\\n    -s ${outdir}/${outfile_prefix}.split.sam \\\n    | samtools sort -@ ${ncpus} -m 1G -o ${outdir}/${outfile_prefix}.dedup.sort.bam  -\n\n################################################\n### END YOUR SCRIPT\n################################################\nSave, then launch the set of benchmarks on the normal queue and the normalbw queue:\nbash bwa_benchmark_run.sh normal\nbash bwa_benchmark_run.sh normalbw\nThis has submitted 7 identical compute tasks with different CPU, memory and queue resources:\n\nMonitor with qstat, then when all jobs have completed, check the usage summary with the SIH Gadi job usage reporting tool:\ncd PBS_logs/bwa/\nperl gadi_usage_report_v1.1.pl\nThis provides comprehensive job reporting metrics which can be copied to Excel for plotting or review. Extracting a few key columns from the table, and extrapolating walltime and SU to the full-sized sample:\n\nWhich resources would you choose when porting this Artemis job to Gadi?\n\nFastest walltime? 48 CPU on normal queue, but this comes with the lowest CPU efficiency and highest SU charge, which could really add up for multiple full-size samples\nLeast service units? 7 CPU on normal Broadwell queue, this also has the best CPU efficiency of 97% but the second slowest walltime\nTrade-off between efficiency, walltime, and SU? That could be 14 CPU on the normalbw queue, which nicely fits 2 samples per node enabling a good fit for a multi-node parallel job when all samples are analysed\n\nOf note, these benchmarks show a much lower memory usage than the Artemis job. Artemis takes a snapshot of memory usage to report in the log, and it may be that a surge in memory usage was recorded, and that the method in which NCI captures memory use for log reporting differs. This even further highlights the need to benchmark on Gadi, as a job that may appear to require high memory or walltime on Artemis may be sufficiently run with far less on Gadi.\nLikewise, the extrapolated walltime for a full-size sample is also much lower than Artemis. We expect faster execution on Gadi, however this difference (2-7 hours vs 18 hours!) is greater than expected. There may have been some other inefficiencies within the tool versions or perhaps scratch disk contention at the time the job was run on Artemis. This clearly underpins the need to benchmark on the platform you plan to use and also, that the small-scale benchmarks should be followed up with a full-size run where possible.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-at-scale",
    "href": "notebooks/10_job_efficiency.html#benchmarking-at-scale",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking at scale",
    "text": "Benchmarking at scale\nWe are fortunate under the Sydney Scheme to have easy access to generous KSU. Prior to this scheme, NCI users would need to apply for merit-based allocations under Sydney Scheme or NCMAS. NCMAS is highly prestigious and competitive, and applications are required to not only provide benchmarking results, but also to demonstrate scalability by showing that CPU and/or memory efficiency is maintained when the workflow is scaled to multi-node jobs.\nWe typically see some decline in efficiency at scale. The figure below shows a scalability study using the same command as above, on one full-sized sample, running multiple 10 million query chunks in parallel with 6 CPU per task (best efficiency from the benchmark results above). Using this scatter-gather parallelism approach, multiple nodes per sample can be utilised (not something the tool can do natively), making great use of Gadi’s extensive infrastructure.\nFor a job with perfect scalability, we would see the walltime decrease as the number of nodes increased, while the SU and CPU efficiency remained constant. The job below does not demonstrate perfect scalability, as so few do. As expected, we can see a decrease in walltime as more nodes are assigned to this job, and a slight increase in SU and worsening of CPU efficiency. It appears the ‘sweet spot’ is around 3 nodes per sample for this analysis, where the CPU efficiency remains &gt; 80% and a fair compromise between walltime and SU is observed.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-the-best-algorithm-for-the-task",
    "href": "notebooks/10_job_efficiency.html#benchmarking-the-best-algorithm-for-the-task",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking the best algorithm for the task",
    "text": "Benchmarking the best algorithm for the task\nOften, there are multiple tools or algorithms we could choose from to perform a computational task. These may produce identical or sufficiently equivalent results, in which case we would not need to consider scientific benchmarking to decide between the two. In these cases, we may rely on computational benchmarks to dictate our choices. For high computational workloads, the walltime and SU savings obtained by using a computationally efficient tool that is “almost as good” as its gold standard counterpart are well justifiable when reporting methods and results. If however you are only performing the analysis once or a small number of times, the deciding factor on tool choice should be scientific performance, not computational.\nTo compare two tools for the same task, you could use the benchmarking template scripts described above. You would simply take two copies of the template scripts, and run separately for each tool. The scripts have variables for tool name and tool abbreviated name, so you could run the benchmarking within the same directory without filename contention. The compute usage and efficiency metrics across both tools could be compared, and combined with scientific benchmarking to make an informed choice of tool.",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#summary",
    "href": "notebooks/10_job_efficiency.html#summary",
    "title": "Optimising and benchmarking your job",
    "section": "Summary",
    "text": "Summary\n\nCompute resource benchmarking can help you save time (walltime, queue time, failed job time) and SU\nUse Artemis job usage logs you have for your favourite workflows as a starting point for resources\nBenchmark on a range of CPU, memory and queue setups on Gadi\nYou may use the SIH Gadi benchmarking template scripts to help you get started with benchmarking\nBenchmark on a small representative subset of data, and then test at scale. Review and revise resources as you go\nApart from CPU, memory, and queue, there are other important factors to benchmark as relevant to your work, including parallelisation, I/O, data structures, software/tool choice, and scientific benchmarking",
    "crumbs": [
      "Running a job",
      "Benchmarking and efficiency"
    ]
  }
]