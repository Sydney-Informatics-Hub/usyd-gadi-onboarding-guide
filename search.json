[
  {
    "objectID": "notebooks/05_data_transfer.html",
    "href": "notebooks/05_data_transfer.html",
    "title": "Transferring data to and from Gadi",
    "section": "",
    "text": "In this section, we will look at transferring data between the Research Data Store and Gadi.\nWatch the pre-recorded session\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at Data Transfer. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decommission date.\n\n\n\n\n\nMethod\nSuitable data size/type\nPros/Cons\n\n\n\n\nGUI based data transfer client e.g. filezilla or cyberduck\nOnly small files, config scripts etc.\n\nEasy to use\nHost computer can limit transfer speed\n\n\n\nsftp transfer from Gadi terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nUses login node for computation\nCannot restart after interruption\n\n\n\nrsync transfer from Artemis terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nCan restart after interruption\nUses login node for computation\nArtemis only around until August\n\n\n\nsftp copy using copyq\nLarge files and datasets to copy all in one go.\n\nRobust, runs in background\nFaster speed than lftp\nAlways copies all files\n\n\n\nlftp sync using copyq\nLarge datasets with many files and only a few updated with each transfer.\n\nRobust, runs in background\nOnly copies whats changed\nSlower speed than sftp\n\n\n\nArtemis rsync using dtq\nAny large dataset.\n\nRobust, runs in background\nOnly copies whats changed\nArtemis only around until August\n\n\n\nGlobus\nAny file/dataset size.\n\nShould do everything\n\nGUI\nTerminal copy and sync\ncopyq copy and sync\n\nUnknown time-frame for availability"
  },
  {
    "objectID": "notebooks/05_data_transfer.html#introduction",
    "href": "notebooks/05_data_transfer.html#introduction",
    "title": "Transferring data to and from Gadi",
    "section": "",
    "text": "In this section, we will look at transferring data between the Research Data Store and Gadi.\nWatch the pre-recorded session\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at Data Transfer. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decommission date.\n\n\n\n\n\nMethod\nSuitable data size/type\nPros/Cons\n\n\n\n\nGUI based data transfer client e.g. filezilla or cyberduck\nOnly small files, config scripts etc.\n\nEasy to use\nHost computer can limit transfer speed\n\n\n\nsftp transfer from Gadi terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nUses login node for computation\nCannot restart after interruption\n\n\n\nrsync transfer from Artemis terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nCan restart after interruption\nUses login node for computation\nArtemis only around until August\n\n\n\nsftp copy using copyq\nLarge files and datasets to copy all in one go.\n\nRobust, runs in background\nFaster speed than lftp\nAlways copies all files\n\n\n\nlftp sync using copyq\nLarge datasets with many files and only a few updated with each transfer.\n\nRobust, runs in background\nOnly copies whats changed\nSlower speed than sftp\n\n\n\nArtemis rsync using dtq\nAny large dataset.\n\nRobust, runs in background\nOnly copies whats changed\nArtemis only around until August\n\n\n\nGlobus\nAny file/dataset size.\n\nShould do everything\n\nGUI\nTerminal copy and sync\ncopyq copy and sync\n\nUnknown time-frame for availability"
  },
  {
    "objectID": "notebooks/05_data_transfer.html#where-to-put-project-files-on-gadi",
    "href": "notebooks/05_data_transfer.html#where-to-put-project-files-on-gadi",
    "title": "Transferring data to and from Gadi",
    "section": "Where to put project files on Gadi",
    "text": "Where to put project files on Gadi\nOn Gadi (like on Artemis) you have access to a number of different storage areas for project files and data. Check the NCI User Guides for more detailed information. Here we provide a brief introduction for those familiar with Artemis.\nEach space is intended for use in a specific way:\n\n/home\nYour home space (/home/&lt;nci_user_id&gt;) is owned by you and has 10 GiB of available space. This cannot be increased.\nIt works similarly to your /home space on Artemis and should only be used to store things like program code, batch scripts or software configuration information. Note that as space is extremely limited here it is unadvisable to use this space for storing project data.\n\n\n/scratch\nYour scratch space (/scratch/&lt;project&gt;) is owned by your project and has 1 TiB of available space which can be increased upon request to NCI. It is roughly equivalent to /scratch on Artemis.\nData are not backed up and files not accessed for 100 days will be quarantined for 14 days and then removed (See here for instructions for removing files from quarantine).\nScratch should be used for temporary files associated with a job that has large data IO and not for longer term storage. Always ensure any data you need from a job that is left in /scratch is promptly backed up to the Research Data Store.\n\n\n/g/data\nYour /g/data space (/g/data/&lt;project&gt;) is owned by your project and has the available space allocated by the Sydney Scheme Manager.\nThe files on /g/data are not backed up but they will persist there for the lifetime of your project.\n/g/data is intended to be used to store longer term files that are regularly used by your project during its lifetime. Always ensure your data is regularly backed up from here to the Research Data Store.\n/g/data may be accessed directly from PBS job scripts by using the -lstorage PBS directive.\nTo check the amount of disk space you have available in the data areas listed above you can type the command lquota at the Gadi prompt."
  },
  {
    "objectID": "notebooks/05_data_transfer.html#research-data-store-rds",
    "href": "notebooks/05_data_transfer.html#research-data-store-rds",
    "title": "Transferring data to and from Gadi",
    "section": "Research data store (RDS)",
    "text": "Research data store (RDS)\nThe RDS is NOT being decommissioned along with Artemis HPC. Any RDS projects you currently have will persist on RDS. It is your responsibility to backup any data on Artemis filesystems (/home, /scratch, /project) that you wish to keep prior to the decommission date of August 29 2025. For information on how to go about this see the SIH Artemis Training Series.\nIn this section, we will mainly focus on how to transfer data between Gadi HPC and RDS. You should be able follow similar methods for copying data between your own laptop/server and Gadi."
  },
  {
    "objectID": "notebooks/05_data_transfer.html#data-transfer-options",
    "href": "notebooks/05_data_transfer.html#data-transfer-options",
    "title": "Transferring data to and from Gadi",
    "section": "Data transfer options",
    "text": "Data transfer options\nDepending on the size and complexity of the data you are transferring you have multiple options available:\n\nFor small transfers (&lt;1GB) you can use a GUI based data transfer client such as filezilla or cyberduck.\nFor mid sized transfers up to tens of GB you can use terminal based transfer.\nFor large transfers you should use the data transfer queue options on either Gadi (copyq) or Artemis (dtq).\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data transfer to and from Gadi should be made using the “Data Mover Node” at gadi-dm.nci.org.au where possible rather than the login nodes. This ensures that data transfer will not consume otherwise limited resources on the login nodes.\n\n\n\nGlobus - COMING SOON\nIn the coming months, Globus will be available for simplified and efficient data transfer. We will provide training and materials on this once available.\nIn the meantime, the below options are available, and detailed examples for each method are provided in the subsequent sections.\n\n\nTransfer using RDS mapped network drive and data transfer client\nFor smaller files or datasets, for example a set of scripts that you are going to run, you can map your RDS project as a network drive and transfer the data to Gadi via an intermediate data transfer client GUI such as filezilla or cyberduck.\nWhile simple to use, these are not recommended for large data transfers, as the local computer becomes a bottleneck and they are generally not resumable after interruption. Faster speeds will be obtained if you are on campus, but still this method may be prohibitively slow for larger datasets.\n\n\n\n\n\n\nExample using cyberduck\n\n\n\nThe following are instructions using the cyberduck data transfer client. The process using filezilla is similar with the same username and server IP address as shown in this example.\nTo mount your RDS drive in either Windows or MacOS, please follow the instructions described here. You should have a File Explorer (Windows) or Finder (MacOS) window open and displaying the files and folders in your RDS project directory.\nNext download cyberduck from https://cyberduck.io and open it and connect to Gadi:\n\nClick on the Open Connection icon at the top of the window.\nSelect SFTP (SSH File Transfer Protocol) from the drop-down menu at the top of box.\nIn the Server field, enter gadi-dm.nci.org.au.\nIn the Username field, enter your NCI Username.\nIn the Password field, enter your NCI password.\nClick Connect.\nIf an Unknown fingerprint box appears, click the Always check box in the lower-left hand corner, then click Allow.\n\nIf you have successfully logged in, you will see a directory listing of /home/&lt;user_login&gt;. You can browse to your project folder either in /scratch or /g/data by pressing Ctrl + g and then typing /scratch/&lt;project_id&gt; or /g/data/&lt;project_id&gt;.\nYou can then transfer data to and from RDS and NCI by dragging and dropping files between your computer’s file explorer and the Cyberduck window.\n\n\n\n\nTransfer from Gadi/Artemis terminal to/from RDS\nYou can use commands in your terminal application (Mac and linux: Terminal, Windows: Windows Terminal or Powershell equivalent) as an alternative to graphical applications.\nSince the connection will be terminated if your computer sleeps, terminal crashes, network drops out etc, this method is not particularly robust for large transfers. For these rather use the queue based methods (either copyq on Gadi or dtq on Artemis) described below.\n\nTransfers from a terminal on Gadi\nDue to stringent security settings around Artemis and RDS, familiar commands like rsync or scp cannot be initiated from NCI Gadi login nodes. Instead you have to use commands like sftp to copy the data.\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that running these commands on the login nodes is not the recommended way to transfer research data to Gadi! For smaller downloads, this is OK, but for normal purposes the use of Gadi’s copyq and data mover nodes are the appropriate tools.\n\n\n\n\n\n\n\n\nHow to transfer data from RDS to Gadi (and vice-versa) from a Gadi login.\n\n\n\nTo transfer data between RDS and Gadi from the Gadi login shell:\n\nOpen a terminal (using the ‘Terminal’ app on MacOS or the ‘Command Prompt’ app on Windows and then log into Gadi using ssh:\n\nssh &lt;nci_user_id&gt;@gadi.nci.org.au\nYou may be prompted to enter your NCI password at this point.\n\nGet the data from RDS, to a specific location on Gadi, e.g:\n\nsftp -r &lt;your_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder&gt; &lt;location on Gadi&gt;\nFor example if I wanted to copy data from my Training project on RDS in the folder MyData to Gadi in the scratch space for my NCI project named qc03:\nsftp -r &lt;my_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-Training/MyData /scratch/qc03/MyData\nreplace /rds in the above with /project or /scratch for your preferred source folder or file.\nIf you want to copy the other way around (ie. from Gadi /scratch to RDS) use\nsftp &lt;your_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder&gt; &lt;&lt;&lt; $\"put -r &lt;location on Gadi&gt;\"\n\n\n\n\nTransfers from a terminal on Artemis using rsync\n\n\n\n\n\n\nWarning\n\n\n\nThis option is only available prior to the decommission of Artemis on 29 August 2025 - after that date you will have to use either GLOBUS (preferred), or copy data when logged into the Gadi terminal (either at the login shell or using copyq scripts).\n\n\nWhen logged into Artemis you can use the rsync or scp command to copy data directly to/from Gadi, since Gadi allows the kind of secure connection that these commands require.\nUsing rsync will allow you to sync data between RDS and Gadi, this means that only files that have been updated since the last transfer will be copied. This will allow small changes to large datasets to be transferred quickly.\n\n\n\n\n\n\nTransfer from direct connection to RDS\n\n\n\nYou can also initiate the transfer in a terminal from a connection via ssh to research-data-int.sydney.edu.au (on campus or USyd VPN) and using the rsync method described here - just replace hpc.sydney.edu.au with research-data-int.sydney.edu.au in step 1.\n\n\n\n\n\n\n\n\nHow to transfer from RDS to Gadi (and vice-versa) at an Artemis terminal using rsync\n\n\n\nTo transfer data between RDS and Gadi from the Artemis login shell:\n\nOpen a terminal (using the ‘Terminal’ app on MacOS or the ‘Command Prompt’ app on Windows and then log into Artemis using ssh:\n\nssh &lt;your_unikey&gt;@hpc.sydney.edu.au\nYou may be prompted to enter your password at this stage.\n\nCopy the data from rds, to a specific location on Gadi, e.g:\n\nrsync -rtlPvz /rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder in RDS&gt; &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;Destination on Gadi&gt;\nFor example if I wanted to sync data from my Training project on RDS in the folder MyData to Gadi in the scratch space for my NCI project named qc03:\nrsync -rtlPvz /rds/PRJ-Training/MyData &lt;nci_user_id&gt;@gadi-dm.nci.org.au:/scratch/qc03/MyData\n\nYou will be prompted for the password associated with your username on Gadi and the transfer will commence.\n\nIf you want to copy the other way around (ie. from Gadi /scratch to RDS) then simply reverse the order of the above command in step 2, e.g:\nrsync -rtlPvz &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;Source on Gadi&gt; /rds/PRJ-&lt;Project Short ID&gt;/&lt;Destination on RDS&gt;\n\n\n\n\nUsing tmux to run your job in the background\n\n\n\n\n\n\nWarning\n\n\n\nWhile this method can help you run your copy job in the background over a long period of time, it is recommended to rather use the copyq transfer method described below for large file transfers, as running jobs on the login node can overwhelm their scant resources.\n\n\nYou can run all of the above copy commands in a tmux session which can be detached to the background allowing you to log out of Gadi and switch off your computer while the copy still runs in the background.\nTo do this, after logging in (step 1. above) enter the command tmux at the prompt. This will send you into a tmux terminal session, then inside the tmux session enter the copy command (step 2. above) and the copy will start. Then while the copy command is running, enter &lt;ctrl&gt;-b then d to detach the tmux session and return to the login prompt. You can now log out of Gadi while your copy job is running in the background on Gadi. To check its status simply log back into Gadi in a terminal (step 1. above) and enter the command tmux attach. This will re-attach your running tmux session and you can investigate its output to check if its done. When things are finished you can exit the running tmux session by typing exit inside it.\nFor more info about tmux check here.\n\n\n\nTransfer using sftp or lftp from Gadi copyq\nThe data transfer queue on Gadi is called copyq. This is comparable to the data transfer queue on Artemis dtq. Data transfer methods/scripts that you used to put data onto Artemis for example from the web via wget or from another server should be easily portable to use on Gadi’s copyq.\nPlease note that the compute nodes on Gadi do not have internet access like the Artemis compute nodes do, so all required data must first be downloaded before submitting a compute job that requires the data.\nDue to stringent security settings around Artemis and RDS, commands like rsync or scp cannot be initiated from NCI Gadi login nodes or copyq. To initiate the transfer from Gadi, sftp or lftp must be used. In the not too distant future Globus will become available for data transfer and then that will be the preferred method for transferring data to and from Gadi.\n\nHow to set up SSH keys for passwordless data transfer\nIf you are transferring data directly for example scp on the command line or via a transfer client on your local computer, entering a password to initiate the transfer is straightforward. If however you want to transfer via a job submitted to either copyq or dtq, you will need to set up SSH keys first, or else your script will halt while it waits fro a password to be entered.\nYou only need to set this up once.\nSSH key pairs are used for secure communication between two systems. The pair consists of a private key and a public key. The private key should remain private and only be known by the user. It is stored securely on the user’s computer. The public key can be shared with any system the user wants to connect to. It is added to the remote system’s authorized keys. When a connection is attempted, the remote system uses the public key to create a message for the user’s system.\nThere are many general guides for this online, for example this one. For step-by-step instructions on how to set up keys between Gadi and RDS, expand the drop down below.\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\nFollow the below steps carefully to set up SSH keys between RDS and Gadi. Note, you only need to do this once.\n\nLog into Gadi with your chosen method, e.g:\n\nssh ab1234@gadi.nci.org.au\n\nMove to your home directory:\n\ncd ~\n\nMake a .ssh directory, if you don’t already have one:\n\nmkdir -p .ssh \n\nSet suitable permissions for the .ssh directory and move into it:\n\nchmod 700 .ssh\ncd .ssh\n\nGenerate SSH key pair:\n\nssh-keygen\nHit enter when prompted, saving the key in ~/.ssh/id_rsa and enter for NO passphrase. A public key will be located in ~/.ssh/id_rsa.pub and a private key in ~/.ssh/id_rsa.\n\nSet suitable permissions for the keys:\n\nchmod 600 id_rsa\nchmod 644 id_rsa.pub\n\nMake an authorized_keys file if you don’t already have one that can be transferred to USyd’s Artemis/RDS system:\n\ntouch ~/.ssh/authorized_keys\n\nCopy the contents of the public key file (~/.ssh/id_rsa.pub) to the authorized_keys file to be transferred to USyd’s Artemis/RDS system:\n\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n\nSet permissions for the authorized_keys file to be transferred to USyd’s Artemis/RDS system:\n\nchmod 600 ~/.ssh/authorized_keys\n\nConnect to USyd’s Artemis/RDS system using lftp and your unikey:\n\nlftp sftp://&lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nProvide your password when prompted. Then make and move into a .ssh directory if you don’t already have one:\nmkdir -p ~/.ssh\ncd ~/.ssh\n\nTransfer the authorized_keys file from Gadi to USyd’s Artemis/RDS system:\n\nput authorized_keys\nDoing this will transfer authorized_keys on Gadi to your current directory. With lftp, it will look for the file relative to where you launched lftp. You can check where you are on Gadi using:\nlocal pwd\n\nExit your lftp connection to USyd’s Artemis/RDS system ctrl + d and test the passwordless connection:\n\nsftp &lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nThis time, you shouldn’t be prompted for a password. You can proceed to transfer data between Gadi and USyd’s Artemis/RDS system now on the copyq.\nIf you get the error “Fatal error: Host key verification failed” you may have to get an “ssh fingerprint” first. Do this by sending an ssh request to the RDS with:\nssh &lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nAccept that you trust the connection and enter your passowrd. The connection will then close with the following message:\nThis service allows sftp connections only.\nConnection to research-data-ext.sydney.edu.au closed.\nBut now try lftp connection again!\n\n\n\n\n\nTemplate copyq scripts for transferring data with sftp\nThe scripts below use sftp to transfer data between RDS and Gadi on the Gadi copyq. sftp can transfer whole files and directories but must copy all of your data every time, it cannot only copy modified files like rsync can. This makes it considerably slower for copying large datasets where only minor changes have been made during a run. An alternative command lftp can behave like rsync but is slower to transfer than sftp. We also provide a template lftp script below.\nCopies of these scripts have been placed in /scratch/qc03/data-transfer-scripts/gadi-scripts. You can make a copy of these scripts to your /scratch/&lt;nci-project-code&gt; or /home/&lt;nci-user-id&gt; workspace on Gadi and edit (for example using nano &lt;script&gt;), by replacing the names described in the header to suit your needs.\nThere are two scripts:\n\nfrom_gadi_to_rds.pbs is used to transfer a file or folder from Gadi to RDS\nfrom_rds_to_gadi.pbs is used to transfer a file or folder from RDS to Gadi\n\n\n\n\n\n\n\nTransfer from Gadi to RDS\n\n\n\n\n\nfrom_gadi_to_rds.pbs script, replace variables in &lt;brackets&gt; as described:\n#!/bin/bash\n\n# Transfer from Gadi to RDS\n#\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;      : Your USyd unikey\n# &lt;rds_project&gt; : Your RDS project name\n# &lt;local_path&gt;  : The local file or folder you want to copy\n# &lt;remote_path&gt; : The location on RDS to put your folder\n# &lt;nci_project&gt; : Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\nremote_path=/rds/PRJ-&lt;rds_project&gt;/&lt;remote_path&gt;\n\n# NOTE: Add a trailing slash (/) to local_path if you don't want to create the\n# parent directory at the destination.\nlocal_path=&lt;local_path&gt;\n\nsftp ${remote_user}@${remote_host}:${remote_path} &lt;&lt;&lt; $\"put -r ${local_path}\"\n\n\n\n\n\n\n\n\n\nTransfer from RDS to Gadi\n\n\n\n\n\nfrom_rds_to_gadi.pbs script, replace variables in &lt;brackets&gt; as described:\n#!/bin/bash\n\n# Transfer a folder from RDS to Gadi\n# This will recreate your RDS path (/rds/PRJ-&lt;rds_project&gt;)\n# on Gadi in /scratch/&lt;nci_project&gt;\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;      : Your USyd unikey\n# &lt;rds_project&gt; : Your RDS project name\n# &lt;remote_path&gt; : The location on RDS of your file ot directory to copy\n# &lt;local_path&gt;  : The name of the folder to copy to\n# &lt;nci_project&gt; : Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\n# RDS:\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\n\n# NOTE: Add a trailing slash (/) to remote_path if you don't want to create the\n# parent directory at the destination. \nremote_path=/rds/PRJ-&lt;rds_project&gt;/&lt;remote_path&gt;\n\n# Gadi:\ndest_path=/scratch/&lt;nci_project&gt;/&lt;local_path&gt;\n\n# Copy folder with sftp\nsftp -r ${remote_user}@${remote_host}:${remote_path} ${dest_path}\n\n\n\nHere is an example showing you how to transfer a folder called MyData in the RDS project Training to some scratch space in Gadi owned by project aa00.\n\n\n\n\n\n\nExample copyq transfer from RDS to Gadi\n\n\n\nLog into Gadi and change directory to your project space and make a folder for your workspace:\n# Using an example username tm0000\nssh tm0000@gadi.nci.org.au\n\ncd /scratch/aa00\n\n# Make a folder called workspace in /scratch/aa00/tm0000\nmkdir -p /scratch/aa00/tm0000/workspace\nCopy the required data transfer script template from /scratch/qc03 to your newly made workspace. In this case we are copying from RDS to Gadi so we use the from_rds_to_gadi.pbs script. You can also cut and paste the template script into your editor from above and save the edited script to your workspace.\ncp /scratch/qc03/data-transfer-scripts/gadi-scripts/from_rds_to_gadi.pbs /scratch/aa00/tm0000/workspace\nThen follow the script and move to that workspace and open the script in an editor (in this example we’ll use the nano editor):\ncd /scratch/aa00/tm0000/workspace\n\nnano from_rds_to_gadi.pbs\nYou need to edit the script by replacing all the variables marked with &lt;&gt; described in the script header and fill in the following details before using it:\nIn the # PBS variables part of the script:\n\nProvide the -P variable by replacing &lt;nci_project&gt; with your NCI project code. In this example aa00.\nIncrease the walltime if you are transferring large files, the limit on this queue is 10 hours.\nAlter -lstorage=scratch/&lt;project&gt; as required. If you also need to access g/data, you can change this to scratch/&lt;project&gt;+g/data/&lt;project&gt;. In this example we’ll just use scratch/aa00\n\nIn the body of the script:\n\nProvide the remote_user variable by replacing &lt;unikey&gt; with your USyd unikey.\nProvide the remote_path variable by replacing &lt;rds_project&gt; and &lt;local_path&gt; with your RDS project name and path to the file or directory you want to transfer. In this example we use remote_path=/rds/PRJ-Training/MyData\n\nHere is what the script will look like when correctly edited for this example:\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/aa00\n\n# RDS:\nremote_user=tm0000     #Example unikey tm0000\nremote_host=research-data-ext.sydney.edu.au\n\n# NOTE: Add a trailing slash (/) to remote_path if you don't want to create the\n# parent directory at the destination. \nremote_path=/rds/PRJ-Training/MyData\n\n# Gadi:\n# This will create /scratch/aa00/MyData if transferring a folder and it doesn't already exist.\ndest_path=/scratch/aa00/MyData\n\n# Copy with sftp\nsftp -r ${remote_user}@${remote_host}:${remote_path} ${dest_path}\nWhen you have finished editing the script save it (using &lt;ctrl&gt;-x and answering y at the prompt if using nano as your editor)\nRun the transfer script\nOnce you have customised the script, you can submit it to the copyq on Gadi. Run the script from the directory where you saved it:\nqsub from_rds_to_gadi.pbs\nThis can be a nerve-wracking process, especially if you are transferring large files. You can check the status of your job on Gadi using:\nqstat -Esw\nOnce it says R (running), you can confirm it is going to where you want on RDS/Artemis or Gadi by logging into the system and checking for the presence of the file/directory in its expected location using:\nls MyData\n\n\n\n\nConfirm transfers after competion\nTo confirm the transfer was successful, you’ll need to check your job logs. These are located in the same directory as your script and are named transfer.o&lt;jobid&gt;.\nHowever, this doesn’t guarantee the integrity of the files. You should check the files themselves to ensure they are intact. You can do this using md5checksums. See this SIH tidbits blogpost about how to use these. You’ll need to create md5checksums for the original files if they don’t already exist and compare them after transfer.\n\n\nTemplate copyq scripts for syncing data with lftp\nIf you have a large project with many files and only a few of them are modified at any time you can use the mirror command in lftp to only sync the modified files when you are backing up data to RDS. The lftp - mirror command transfers data at a slower speed than sftp but will significantly speed up the backup of data since it generally only needs to copy a small subset of all of the data on Gadi if only a few files have changed since the last sync.\nBelow is a template .pbs script that can be used to sync files between Gadi and RDS, you can copy it to your workspace and modify it as needed similarly to the example above. You can also find this script on Gadi in /scratch/qc03/data-transfer-scripts/gadi-scripts/sync_gadi_to_rds.pbs.\n\n\n\n\n\n\nWarning\n\n\n\nNote that when using this script to copy a folder you should ensure the target directory exists on RDS first, otherwise the parent directory will not be synced with the data.\n\n\n\n\n\n\n\n\nSync between Gadi and RDS\n\n\n\n\n\n#!/bin/bash\n\n# Mirror directory from Gadi to RDS\n#\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;:       Your USyd unikey\n# &lt;rds_project&gt;:  Your RDS project name\n# &lt;path-on-rds&gt;:  The location on RDS to put your directory\n# &lt;path-on-gadi&gt;: The directory on Gadi to mirror to rds\n# &lt;nci_project&gt;:  Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\nremote_path=/rds/PRJ-&lt;rds-project&gt;/&lt;path-on-rds&gt;\n\nsource_path=&lt;path-on-gadi&gt;\n\noutfile=\"${PBS_O_WORKDIR}/${PBS_JOBNAME}_${PBS_JOBID}.log\"\necho \"Writing log to: ${outfile}\"\ntouch $outfile\n\nlftp -u ${remote_user}, sftp://${remote_host} &lt;&lt;EOF\nset xfer:log true\nset xfer:log-file \"${outfile}\"\nmirror -p --verbose -R ${source_path} ${remote_path}\nexit\nEOF\n\n\n\n\n\nSuggested workflow for copying data between RDS and Gadi and keeping it up-to-date\nThe sftp copy method and lftp sync methods described above both have their pros and cons:\n\nsftp has a faster transfer speed but can only copy all your files in bulk when it is run.\nlftp has a slower transfer speed but it can sync only the subset of files that have changed.\n\nBecause of this we suggest users be selective about which method they use based on their needs.\nFor example a simple workflow for large projects that have a lot of data and many files in a folder, only a few of which are changed between backups to RDS would be:\n\nInitially use the sftp based from_rds_to_gadi.pbs script to bulk copy your data from RDS to your project space on Gadi, since this is faster for bulk transfers.\nSubsequently use the lftp based sync_gadi_to_rds.pbs to sync smaller files (e.g. output logs and data) back to RDS, without having to re-copy the bulk of the data back to RDS.\n\n\n\n\nTransfer using rsync from Artemis dtq\n\n\n\n\n\n\nWarning\n\n\n\nThis option is only available prior to the decommission of Artemis on 29 August 2025 - after that date you will have to use either GLOBUS (preferred), or copy data when logged into the Gadi terminal (either at the login shell or using copyq scripts).\n\n\nFor transfer of large files directly from Artemis to Gadi, the use of resumable rsync is recommended (see script below). The transfer can be initiated using Artemis dtq and using Gadi’s data mover node: gadi-dm.nci.org.au. The below template script can be used with Artemis’ dtq using rsync.\nFor further info about copying data from Artemis dtq can be found in the SIH Artemis Training Series.\n\n\n\n\n\n\nTemplate Artemis data transfer script using rsync\n\n\n\n\n\n#!/bin/bash\n\n# This is an Artemis data transfer script\n\n#PBS -P &lt;project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q dtq\n\n# NOTE: Add a trailing slash (/) to source_path if you don't want to create the\n# parent directory at the destination. \nsource_path=/rds/PRJ-&lt;project&gt;/&lt;path&gt;/&lt;file&gt;\n\ndestination=&lt;user&gt;@gadi-dm.nci.org.au\ndestination_path=&lt;path-on-gadi&gt;\n\nwhile [ 1 ]\ndo\n        echo Transferring ${source_path} to ${destination}:${destination_path}\n        rsync -rtlPvz --append-verify ${source_path} ${destination}:${destination_path}\n\n        if [ \"$?\" = \"0\" ]\n        then\n                echo \"Rsync completed normally\"\n                dt=$(date)\n                echo Finished transferring at $dt\n        exit\n        else\n                echo \"Rsync failure. Backing off and retrying in 3 minutes\"\n                sleep 180\n        fi\ndone"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html",
    "href": "notebooks/10_job_efficiency.html",
    "title": "Optimising and benchmarking your job",
    "section": "",
    "text": "The main challenges users may face adapting Artemis workflows to Gadi are:\n\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will broadly address the first two challenges on this list, exploring how optimising your workload through splitting/checkpointing jobs and optimising compute resource requests for Gadi queues can help minimise your walltime and use less KSU.\nFor the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#introduction",
    "href": "notebooks/10_job_efficiency.html#introduction",
    "title": "Optimising and benchmarking your job",
    "section": "",
    "text": "The main challenges users may face adapting Artemis workflows to Gadi are:\n\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will broadly address the first two challenges on this list, exploring how optimising your workload through splitting/checkpointing jobs and optimising compute resource requests for Gadi queues can help minimise your walltime and use less KSU.\nFor the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#what-is-workflow-optimisation",
    "href": "notebooks/10_job_efficiency.html#what-is-workflow-optimisation",
    "title": "Optimising and benchmarking your job",
    "section": "What is workflow optimisation",
    "text": "What is workflow optimisation\nIn short, to optimise a workflow or code is to make it faster and more efficient. This can be achieved through a number of practices:\n\nEfficient resource allocation\n\nSelecting the right number of CPUs, memory and queue for the job\nAvoiding overallocation (wasting resources) or underallocation (causing slow performance)\n\nParallelisation and scaling\n\nSplitting/checkpointing jobs into numerous smaller jobs to enable parallel computing and job resume\nBalancing workload distribution to avoid bottlenecks caused when some cores are idle while others are overloaded\n\nI/O optimisation\n\nSelecting the right filesystem for I/O, for example using compute node SSD for temp files\nUsing efficient file formats\nAvoiding excessive small file creation to prevent inode exhaustion\n\nMemory management\n\nEnsuring a job fits within allocated RAM to avoid swapping to disk\nUsing efficient data structures to reduce memory footprint\n\nSoftware and algorithm optimisation\n\nChoosing the best algorithm for the task\nUsing binaries compiled on Gadi and optimised libraries\nEnabling hardware acceleration with GPU where possible\n\n\nOptimisation is crucial in HPC because it facilitates faster, more efficient, and cost-effective execution of computational workload. As researchers we want fast results but it is also our moral imperative to minimise our carbon footprint while using HPC. We also have a shared responsibility to other users of the system to make efficient use of the resources we reserve for our jobs, and to The University of Sydney, which funds 100% of our compute with NCI.\nIn this section, we will look at one simple approach to optimisation through compute resource benchmarking as a quickstart way of adapting your Artemis workflows to Gadi and selecting the right CPU, mem and queue for your Gadi job. In order to optimise in other ways, for example testing different file formats, I/O flesystems, algorithm types, GPUs, etc, a similar systematic approach could be followed: run the same analysis with the relevant differences, and compare the job performance.\nThere are complex job tracing tools for fine-tuned code optimisation; we will not be covering these, instead focusing on an overall picture of CPU and memory efficiency to guide appropriate resource selection."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#compute-resource-benchmarking-vs-scientific-benchmarking",
    "href": "notebooks/10_job_efficiency.html#compute-resource-benchmarking-vs-scientific-benchmarking",
    "title": "Optimising and benchmarking your job",
    "section": "Compute resource benchmarking vs scientific benchmarking",
    "text": "Compute resource benchmarking vs scientific benchmarking\nPlease note that this section covers compute resource benchmarking and not scientific benchmarking. While compute resource benchmarking focuses on execution speed and efficiency, scientific benchmarking focuses on whether computational results are correct, reproducible, and scientifically valid. Scientific benchmarking is extremely important, but not the focus of this training. Please do undertake scientific benchmarking, but understand that it is not covered within this section. It may be that changing job parameters can also influence the optimum resources required, such that once you have selected the right resources based on benchmarking for one set of parameters, you may need to re-run compute benchmarking for a different set of parameters. This may sound tedious, however if you typically run the same types of analyses frequently, the compute benchmarking you perform can save you both time (walltime, queue time, avoiding job failures from exceeding resources) and SU in the long run.\nIn this section we will focus on selecting the right queue on Gadi, along with the CPU and memory that provides the best trade-off between walltime, CPU efficiency and memory efficiency. By optimising these aspects, we can also indirectly minimise our queue time, as an efficient job is generally a job with a shorter walltime and shorter walltimes contribute to shorter queue times."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#cpu-and-memory-efficiency",
    "href": "notebooks/10_job_efficiency.html#cpu-and-memory-efficiency",
    "title": "Optimising and benchmarking your job",
    "section": "CPU and memory efficiency",
    "text": "CPU and memory efficiency\nEfficiency can refer to CPU efficiency or memory efficiency.\nCPU efficiency can be calculated with the formula:\ncpu_e = cputime / (walltime X cpus_used)\nA CPU efficiency value of 1 indicates perfect CPU utilisation for the duration of the job. This is often achieved for single-core jobs, however typically, as a tool or code is run with multiple threads or at scale across multiple nodes, the CPU efficiency declines. Aim to maintain your workflows above 80% where possible.\nNCI monitors CPU utilisation and repeated execution of jobs with very poor CPU efficiency may be met with an email from the NCI technical team.\nA job with extremely low CPU efficiency may be permissable if it has high memory efficiency. Memory efficiency can be calculated with the formula:\nmem_e = max_mem_used / mem_requested\nBelow is a Gadi job log. All the details required to calculate CPU and memory efficiency are contained within this log:\n\n\nChallenge: calculate CPU efficiency\nFrom the above job log, use the formula cpu_e = cputime / (walltime X cpus_used) to calculate this job’s CPU efficiency.\nDo you think this job efficiently utilised CPU?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe below answer converts CPU time and walltime to minutes. The same result is achieved if you use hours or seconds, as long as you use the same unit across both values.\ncpu_e = cputime / (walltime X cpus_used)\ncpu_e = 177.55 / (164.15 X 7)\ncpu_e = 0.15\nThis job did not make efficient use of the requested 7 CPU.\n\n\n\n\n\nChallenge: calculate memory efficiency\nFrom the above job log, use the formula mem_e = max_mem_used / mem_requested to calculate this job’s memory efficiency.\nDo you think this job efficiently utilised memory?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmem_e = max_mem_used / mem_requested\nmem_e = 51.14 / 63\nmem_e = 0.81\nThis job did make efficient use of the requested 63 GB memory.\n\n\n\nReflecting on the efficiencies calculated for this example job, would you consider this job to have requested appropriate resources on Gadi?\nPerfect utilisation of all 7 CPUs would result in a CPU efficiency of 1.0. If only one CPU were used with perfect efficiency, its individual contribution would be 1/7 ≈ 0.14. This job had a CPU efficiency of 0.15. This suggests that the job was a single core job with a high memory requirement. If this job requested only 1 CPU and the same 63 GB memory, the CPU efficiency calculation from the job log would be close to perfect! However, the memory efficiency and total SU charge would have remained identical.\nWhy would the user have requested 7 CPU over 1 CPU for a single-core job? By requesting 7 CPU, they have been protective of the resources they have partitioned for their job. If a request of 1 CPU and 63 GB memory was made, other users may have been able to utilise those other 6 CPU, but only if there was sufficient memory remaining on that node. Some jobs benefit from reserving extra CPUs even if they do not use them, such as memory-intensive jobs that prevent other users from overloading the node.\nNow that we have an understanding of CPU efficiency, memory efficiency, and how to calculate them, we have a means of directly comparing replicate runs of the same analysis task with differing compute resources."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-cpu-memory-and-queue-on-gadi",
    "href": "notebooks/10_job_efficiency.html#benchmarking-cpu-memory-and-queue-on-gadi",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking CPU, memory and queue on Gadi",
    "text": "Benchmarking CPU, memory and queue on Gadi\nCompared to Artemis, Gadi has newer hardware, newer software, and more diverse queue options. A burning question from Artemis users newly migrating to Gadi is how do I know what resources to request for my job?\nIf you have a fair idea of what resources your job required on Artemis and for how long, you can request similar resources on the relevant queue on Gadi, with the expectation that your job will execute more quickly due to the newer hardware and software. Whether you have this prior knowledge or not, it is worthwhile to perform benchmarking on Gadi.\n\nExample Artemis usage log\nConsider the below job log from Artemis:\n\nThis is a fairly long-running job with high CPU and memory utilisation. We could run this job with reasonable confidence on Gadi by applying the same resource requests:\n#PBS -P MYPROJECT\n#PBS -l walltime=24:00:00\n#PBS -l ncpus=16\n#PBS -l mem=96GB\n#PBS -q normal\nYet this requests a CPU:mem ratio of 16:96 = 6, which as we have learnt from the section on accounting is higher than the CPU:mem ratio on the normal queue of 4, so our job would be charged double per resource hour.\nHow else could we structure this to ensure sufficient CPU, memory, and minimise SU?\n\n16 CPU with 4 GB RAM per CPU on the normal queue, total memory request of 64 GB. Would this be sufficient, or would the job die due to inadequate memory? Memory utilisation on Artemis was 100% at 96 GB\n16 CPU with 9 GB RAM per CPU on the normalbw queue. We would have plenty of memory at 144 GB, and a low charge rate of 1.25, but how much slower would our job be on the older Broadwell nodes?\n24 CPU with 4 GB RAM per CPU on the normal queue. We would have the same amount of memory as the Artemis job, but with more CPU. Would the walltime be faster due to this extra CPU, or would it take the same walltime and thus have a lower CPU efficiency?\n\nThese are the questions that we can answer with simple compute resource benchmarking. In this example, we have a good starting point about the amount of resources the job requires. We can test different queues and different CPU and memory combinations that are around these values to obtain the most efficient values, ie those that are the best trade-off between walltime, SU, and efficiency."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-template-scripts",
    "href": "notebooks/10_job_efficiency.html#benchmarking-template-scripts",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking template scripts",
    "text": "Benchmarking template scripts\nTo make this process easier, SIH have a repository of Gadi template benchmarking scripts. This repository contains a pair of scripts designed to test single runs of a command/tool at various CPU and memory settings on different queues. It does require some modification (and carefully use and follow the guide!) to set it up, but once you know how to use this template, it can expedite testing chunks of your workflow to obtain the most efficient (ie optimised) queue and resource requests for the task. Running the gadi_usage_report.pl script from this repository will summarise the resources used by the benchmark jobs into a table that can be viewed or plotted to determine best resoruces.\nIt is not critical to use this template, but it can be a helpful tool if you have not benchmarked before, or if you benchmark multiple tools/code chunks regularly and want a simple and replicable method."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#tips-for-benchmarking",
    "href": "notebooks/10_job_efficiency.html#tips-for-benchmarking",
    "title": "Optimising and benchmarking your job",
    "section": "Tips for benchmarking",
    "text": "Tips for benchmarking\n\nTest individual parts of your code where possible - ie one command, one tool, one chunk of code\n\nThis enables you to determine which parts of your workflow have differeing compute requirements\nParts with differing compute requirements can be allocated to different queues and resources, saving you KSU\n\nDo initial benchmarking on a small subset of your data - ie subsample, reduce sample numbers, reduce permutations, etc\nFollow up with scalability testing: Once you have refined the candidate best resources, re-run the benchmark on a representative subset (ie whole sample, more iterations) and compare the CPU efficiency\n\nIs it as good as the initial test benchmark in terms of CPU and memory efficiency?\nIf so, then go ahead and apply this setting to your full run\nIf not, re-run full benchmarks with the larger test dataset, or dig deeper into what is causing the loss of efficiency at scale\n\nEmbrace the labour of benchmarking!\n\nWhile it may seem like a time-consuming impediment to getting on with analysing your data, it can save you a lot of time and KSU down the track\nBenchmarking will make your analysis faster and use less USyd-funded resources and energy resources\nbenchmarking can prevent avoidable job failures such as a job runing out of walltime or memory, which will cost more time and resources to resubmit"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#demo-benchmarking-activity-using-the-template",
    "href": "notebooks/10_job_efficiency.html#demo-benchmarking-activity-using-the-template",
    "title": "Optimising and benchmarking your job",
    "section": "Demo benchmarking activity using the template",
    "text": "Demo benchmarking activity using the template\nThe example Artemis usage log shown above ran the tool bwa-mem to map DNA sequences to a reference sequence. When benchmarking, apply your domain knowledge to determine how to best subset the data to make small test datasets that can provide benchmark results quickly and with low SU cost. For this example, each query string within the input DNA file is mapped independently, so we can simply take a short subset of the file and map that to the reference.\nWhen subsetting, it’s important to finish off benchmarking by completing one full (not subset) run at the best chosen compute resources, to ensure that the benchmark results are consistent when a large input is provided.\nIt is also important to take a meaningful subset. The minimum size may not always be the best; aim to have your jobs run for at least 5 minutes. Starting a job and its various processes has an overhead, and making the subset data too small can cause the compute resource usage summary to be affected by these background processes. For this example, the full input was 433.4 million queries. For a target run time of 5 minutes, this equates to approximately 2 million queries. However, I am so optimistic about the execution speed on Gadi that I will hazard a guess that 2 million queries will be too fast for a decent preliminary benchmark run and we should use 10 million :-)\nAfter subsetting the raw data to the target 2 million entries, next set up the template benchmarking scripts. Working on Gadi, make a directory for the tool to be benchmarked, clone the repository, and rename the scripts\ncd /scratch/&lt;nci-project-id&gt;\nmkdir benchmark-bwa\ncd benchmark-bwa/\ngit clone git@github.com:Sydney-Informatics-Hub/Gadi-benchmarking.git\ncd Gadi-benchmarking/\nmv tool_benchmark_run.sh bwa_benchmark_run.sh\nmv tool_benchmark.pbs bwa_benchmark.pbs \nNext, edit bwa_benchmark_run.sh which is a wrapper script to launch multiple PBS jobs for the same compute task at different resources. Update the run prefix, tool name and tool short name:\n\nThen, review the pre-set queue variables, and determine which CPU values you want to run on. For this case we are interested in the normal and normalbw queues, as the CPU:mem ratio on hugemem does not seem warranted by our prior runs on Artemis. It’s unlikely that the job will run on low CPU values, given the mem usage of 96 GB on Artemis, so adapt the NCPUS arrays for the normal and normalbw code chunks to exclude 1, 2 and 4 CPU for normal and 1 CPU for normalbw:\n\nThen, add the workflow code from the original script to the benchmarking template script named bwa_benchmark.pbs, ensuring to adhere to the output path and prefix requirements described in the comments. Also add your NCI project code at #PBS -P and update the lstorage paths as required for your input data:\n\nMake sure to check for the required software on Gadi. See software for options when your tools/versions aren’t globally installed on Gadi.\nAfter making the required changes to this script, the completed bwa_benchmark.pbs workflow section appears as below. Note the use of ${ncpus} for the BWA and samtools thread parameter values, and the use of ${outdir} and ${outfile_prefix} to name the outputs. These are specified within the run script, and are set up so you can run multiple benchmarks simultaneously without I/O clashes. Also note the slight adjustment to module versions and input file paths, as well as the lack of job array here compared to the Artemis script:\n################################################\n### YOUR SCRIPT HERE \n################################################\n\n# Include all the commands required to run your job\n# Use ${outdir} for output directory path\n# Use ${outfile_prefix} for output file prefix\n\n# Load modules\nmodule load bwa/0.7.17\nmodule load samtools/1.19\nmodule load samblaster/0.1.24\n\nref=./inputs/Reference/hs38DH.fasta\n\nSAMPLE=subset_10M\nfq1=./inputs/${SAMPLE}_R1.fastq.gz\nfq2=./inputs/${SAMPLE}_R2.fastq.gz\n\nbwa mem -M -t ${ncpus} $ref \\\n    -R \"@RG\\tID:${SAMPLE}_1\\tPL:ILLUMINA\\tSM:${SAMPLE}\\tLB:1\\tCN:KCCG\" \\\n    $fq1 $fq2  \\\n    | samblaster -M -e --addMateTags \\\n    -d ${outdir}/${outfile_prefix}.disc.sam \\\n    -s ${outdir}/${outfile_prefix}.split.sam \\\n    | samtools sort -@ ${ncpus} -m 1G -o ${outdir}/${outfile_prefix}.dedup.sort.bam  -\n\n################################################\n### END YOUR SCRIPT\n################################################\nSave, then launch the set of benchmarks on the normal queue and the normalbw queue:\nbash bwa_benchmark_run.sh normal\nbash bwa_benchmark_run.sh normalbw\nThis has submitted 7 identical compute tasks with different CPU, memory and queue resources:\n\nMonitor with qstat, then when all jobs have completed, check the usage summary with the SIH Gadi job usage reporting tool:\ncd PBS_logs/bwa/\nperl gadi_usage_report_v1.1.pl\nThis provides comprehensive job reporting metrics which can be copied to Excel for plotting or review. Extracting a few key columns from the table, and extrapolating walltime and SU to the full-sized sample:\n\nWhich resources would you choose when porting this Artemis job to Gadi?\n\nFastest walltime? 48 CPU on normal queue, but this comes with the lowest CPU efficiency and highest SU charge, which could really add up for multiple full-size samples\nLeast service units? 7 CPU on normal broadwell queue, this also has the best CPU efficiency of 97% but the second slowest walltime\nTrade-off between efficiency, walltime, and SU? That could be 14 CPU on the normalbw queue, which nicely fits 2 samples per node enabling a good fit for a multi-node parallel job when all samples are analysed\n\nOf note, these benchmarks show a much lower memory usage than the Artemis job. Artemis takes a snapshot of memory usage to report in the log, and it may be that a surge in memory usage was recorded, and that the method in which NCI captures memory use for log reporting differs. This even further highlights the need to benchmark on Gadi, as a job that may appear to require high memory or walltime on Artemis may be sufficiently run with far less on Gadi.\nLikewise, the extrapolated walltime for a full-size sample is also much lower than Artemis. We expect faster execution on Gadi, however this difference (2-7 hours vs 18 hours!) is greater than expected. There may have been some other inefficiencies within the tool versions or perhaps scratch disk contention at the time the job was run on Artemis. This clearly underpins the need to benchmark on the platform you plan to use and also, that the small-scale benchmarks should be followed up with a full-size run where possible."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-at-scale",
    "href": "notebooks/10_job_efficiency.html#benchmarking-at-scale",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking at scale",
    "text": "Benchmarking at scale\nWe are fortunate under the Sydney Scheme to have easy access to generous KSU. Prior to this scheme, NCI users would need to apply for merit-based allocations under Sydney Scheme or NCMAS. NCMAS is highly prestigious and competitive, and applications are required to not only provide benchmarking results, but also to demonstrate scalability by showing that CPU and/or memory efficiency is maintained when the worflow is scaled to multi-node jobs.\nWe typically see some decline in efficiency at scale. The figure below shows a scalability study using the same command as above, on one full-sized sample, running multiple 10 million query chunks in parallel with 6 CPU per task (best efficiency from the benchmark results above). Using this scatter-gather parallelism approach, multiple nodes per sample can be utilised (not something the tool can do natively), making great use of Gadi’s extensive infrastructure.\nFor a job with perfect scalability, we would see the walltime descrease as the number of nodes increased, while the SU and CPU efficiency remained constant. The job below does not demonstrate perfect scalability, as so few do. As expected, we can see a decrease in walltime as more nodes are assigned to this job, and a slight increase in SU and worsening of CPU efficiency. It appears the ‘sweet spot’ is around 3 nodes per sample for this analysis, where the CPU efficiency remains &gt; 80% and a fair compromise between walltime and SU is observed."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-the-best-algorithm-for-the-task",
    "href": "notebooks/10_job_efficiency.html#benchmarking-the-best-algorithm-for-the-task",
    "title": "Optimising and benchmarking your job",
    "section": "Benchmarking the best algorithm for the task",
    "text": "Benchmarking the best algorithm for the task\nOften, there are multiple tools or algorithms we could choose from to perform a computational task. These may produce identical or sufficiently equivalent results, in which case we would not need to consider scientific benchmarking to decide between the two. In these cases, we may rely on computational benchmarks to dictate our choices. For high computational workloads, the walltime and SU savings obtained by using a computationally efficient tool that is “almost as good” as its gold standard counterpart are well justifiable when reporting methods and results. If however you are only performing the analysis once or a small number of times, the deciding factor on tool choice should be scientific performance, not computational.\nTo compare two tools for the same task, you could use the benchmarking template scripts desribed above. You would simply take two copies of the template scripts, and run separately for each tool. The scripts have variables for tool name and tool abbreviated name, so you could run the benchmarking within the same directory without filename contention. The compute usage and efficiency metrics across both tools could be compared, and combined with scientific benchmarking to make an informed choice of tool."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#summary",
    "href": "notebooks/10_job_efficiency.html#summary",
    "title": "Optimising and benchmarking your job",
    "section": "Summary",
    "text": "Summary\n\nCompute resource benchmarking can help you save time (walltime, queue time, failed job time) and SU\nUse Artemis job usage logs you have for your favourite workflows as a starting point for resources\nBenchmark on a range of CPU, memory and queue setups on Gadi\nYou may use the SIH Gadi benchmarking template scripts to help you get started with benchmarking\nBenchmark on a small representative subset of data, and then test at scale. Review and revise resources as you go\nApart from CPU, memory, and queue, there are other important factors to benchmark as relevant to your work, including parallelisation, I/O, data structures, software/tool choice, and scientific benchmarking"
  },
  {
    "objectID": "notebooks/06_accounting.html",
    "href": "notebooks/06_accounting.html",
    "title": "Accounting",
    "section": "",
    "text": "The main challenges users may face adapting Artemis workflows to Gadi are:\n\nUnderstanding NCI accounting of KSU, disk and iNode limits\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/06_accounting.html#introduction",
    "href": "notebooks/06_accounting.html#introduction",
    "title": "Accounting",
    "section": "",
    "text": "The main challenges users may face adapting Artemis workflows to Gadi are:\n\nUnderstanding NCI accounting of KSU, disk and iNode limits\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/06_accounting.html#what-are-the-important-accounting-metrics",
    "href": "notebooks/06_accounting.html#what-are-the-important-accounting-metrics",
    "title": "Accounting",
    "section": "What are the important accounting metrics?",
    "text": "What are the important accounting metrics?\n\nService units: The charge rate per resource hour\nDisk usage: The amount of physical disk space used on scratch, home, and gdata\niNode usage: The number of files and directories stored on scratch, home, and gdata"
  },
  {
    "objectID": "notebooks/06_accounting.html#service-units",
    "href": "notebooks/06_accounting.html#service-units",
    "title": "Accounting",
    "section": "Service units",
    "text": "Service units\nFor a detailed description of the NCI SU and why 1 SU does not equal 1 CPU hour, see here\nFor every resource hour you consume on an NCI compute platform, you are charged at a specific rate. As a simple example the normal queue on Gadi has a charge rate of 2, so a job using 1 CPU for 1 hour will be charged 2 SU.\nWe often speak in KSU (1 KSU = 1,000 SU) for simplicity. Under the Sydney Scheme, you can easily request more KSU as you need it from the management portal.\nBefore submitting a job on Gadi, it is important to:\n\nCalculate the amount of SU the job will require\nCheck your available SU to ensure there is sufficient for the job to run\n\n\nCalculate the amount of SU the job will require\nIn the simple example above, we decided that our 1 CPU 1 hour normal queue job would cost 2 SU. There is actually another factor to consider, and that is memory. Each queue has nodes of a specific CPU:memory ratio, and if your job consumes more memory than this ratio allows, you will be charged based on the memory used. From the NCI job costs page:\n“However, some jobs will request less CPUs and more memory. When this happens, you are taking memory away from the other CPUs in the node and will be charged accordingly, as other users can’t access those CPUs while your job is using that memory allocation”\nSo let’s assume our 1 CPU job requests 12 GB memory. Checking the Gadi queue structure page, under the Intel Xeon Cascade Lake dropdown, the normal nodes have the following hardware:\n\n2 x 24-coreIntel Xeon Platinum 8274 (Cascade Lake) 3.2 GHz CPUs per node\n192 GiB RAM per node\n\nFrom here we can determine that the CPU:mem ratio is 48:192 = 1:4 (1 CPU per 4 GB mem). By requesting 12 GB mem for our 1 CPU job, we are using 3 times as much mem per CPU as the ratio governs for this queue. So, we are likewise charged 3 times as much, or in other words, we are charged based on the MEMORY rather than the CPU.\nThe equation for every job run on Gadi is charged using the formula:\nSU = Queue Charge Rate  ✕  Max (NCPUs, Memory Proportion)  ✕  Walltime Used (Hours)\nFor our example, this expands to:\nSU = 2 charge rate X 3 memory proportion X 1 hour\nSU = 6\n\nChallenge 1: calculate this job’s SU cost\nUse the queue limits and queue structure pages to help find the answer.\n#PBS -P MYPROJECT\n#PBS -l walltime=02:00:00\n#PBS -l ncpus=4\n#PBS -l mem=48GB\n#PBS -q normal\n#PBS -W umask=022\n#PBS -l wd\n#PBS -l storage=gdata/MYPROJECT\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFirst, we need to calculate the CPU:mem ratio of our job, which in this case is 4:48 = 1:12, ie 12 GB mem requested per CPU requested.\nNext, we need to check that against the queue hardware: These nodes have 48 CPU and 192 GB RAM. So the CPU:mem ratio for this queue is 48:192 = 1:4 ie 4 GB RAM per CPU. By requesting 12 GB mem per CPU, we have requested 3 times as much mem per CPU than the queue hardware provides for, so we will be charged based off the mem not the CPU, ie instead of being charged based on 4 CPU, we will be charged for the CPUs assigned to 48 GB RAM which is 12 (48 GB mem requested dividided by the queue mem per CPU value of 4).\nFinally, check the charge rate for the normal queue, which is 2 SU per resource hour.\nSU = 2 charge rate X 12 memory proportion X 2 hour\nSU = 48\n\n\n\n\n\nChallenge 2: calculate this job’s SU cost\nUse the queue limits and queue structure pages to help find the answer.\n#PBS -P MYPROJECT\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=280\n#PBS -l mem=2520GB\n#PBS -q normalbw\n#PBS -W umask=022\n#PBS -l wd\n#PBS -l storage=gdata/MYPROJECT\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFirst, we need to calculate the CPU:mem ratio of our job, which in this case is 280:2520 = 1:9, ie 9 GB mem per CPU.\nNext, we need to check that against the queue hardware: The broadwell queues (except express) have a charge rate of 1.25 SU per resource hour. The directives have requested the normalbw or “normal broadwell” queue, which has some nodes with 128 GB RAM and some nodes with 256 GB RAM. There is no difference in the charge rate for these nodes, and jobs are placed on either node type depending on the memory per CPU requested and resource availability at the time. These nodes have 28 CPU per node. So the CPU:mem rato for this queue is either 28:128 or 28:256. Since the 256 GB nodes are charged the same as the 128 GB nodes, we go with the higher ratio, 28:256 = 1:9.14, ie 9.14 GB mem per CPU.\nSince we requested a CPU:mem ratio of 280:2520 = 1:9, we have requested less mem per CPU than the queue allows per CPU so our charge rate will be based on CPU not mem.\nSU = 1.25 charge rate X 280 CPU X 4 hour\nSU = 1400\nKSU = 1.4\n\n\n\n\n\n\nCheck your available SU\nOnce you have established your job script/s and calculated your SU cost, you should check that you have sufficient SU budget to run the job.\nUse the below command, replacing the &lt;nci-project-id&gt; with your NCI project code:\nnci_account -P &lt;nci-project-id&gt;\nIn the below screenshot, you can see that project qc03 has used 3.24 SU of its 1 KSU allocation (Grant). There are no SU reserved (reserved SU are SU assigned to jobs currently running or in queue) and there are 996.76 SU still available for the rest of the quarter.\n\nIn order to run a job under this project code, it would need to have a computed job cost of less than or equal to 996.76 SU.\nIf a job is submitted under a project that does not have sufficient SU to cover the expected cost, the job will be held. You can reveal the reason for a held job (status = ‘H’) from the “Comment” included in the qstat -f output:\n\nThis job requires 19.2 KSU yet we know only 966 SU is available to the project. To run this job, you would need to obtain more KSU.\nIf the amount by which your job exceeds your current SU budget is small, you may consider reducing the requested resources to fit under the current budget. You can do this by either killing the job (qdel &lt;jobID&gt;), adjusting the resource requests then resubmitting, or use the qalter PBS command to reduce the resource requests of the held job via the command line. For example, to reduce CPU, mem and walltime:\nqalter -l walltime=02:00:00 -l ncpus=48 -l mem=190GB &lt;jobid&gt; \n\n\nCompleted job SU charged\nAfter your job completes, the SU consumed by the job is reported in the PBS .o job log (&lt;jobname&gt;.o&lt;jobID&gt; unless otherwise changed with the #PBS -o &lt;logname&gt; directive).\nThe charged is based off the walltime used, not the walltime requested.\n\nIn the above example, the job cost 23.94 SU. The job queue is not reported in the log, however this information is of course included in the directives of the job script. This job was run on normalbw which has a charge rate of 1.25 SU, and CPU:mem ratio of 1:9.14. Since the job requested 7 CPU and 63 GB RAM, this is below the mem:CPU ratio so the charge is based off CPU not memory:\nSU = 1.25 charge rate X 7 CPU X 2.74 hours\nSU = 23.94"
  },
  {
    "objectID": "notebooks/06_accounting.html#disk",
    "href": "notebooks/06_accounting.html#disk",
    "title": "Accounting",
    "section": "Disk",
    "text": "Disk\nNew projects are assigned 1 TB disk quota and 202 K inode quota on Gadi’s scratch filesystem, and these can be increased with justification by contacting the NCI help desk.\nYou can check your project allocation with the same nci_account -P &lt;nci-projet-id&gt; command used to query SU:\n\nThis project has used 4.5 TB of its 10 TB allocated in scratch.\nAnother way to check disk usage is with the lquota command, which will report filesystem usage for all projects of which you are a member:\n\nNote the extra columns here - you will see that in addition to the amount of disk used and allocated, there are also Limit columns. This is a grace amount of additional resources that your project may use, but only for a short time. This is to try and help jobs complete that may otherwise have exceeded the allocation and failed, causing unecessary wastage of compute resources. You cannot submit any new jobs under a project code that has exceeded the limit; the jobs will be held (status H) until you have brought your disk usage back under the allocation quota. Your project cannot exceed the limits. If a running job causes the limit to be exceeded, the job will fail. If a running job exceeds the allocation but remains under the limit, the job will continue.\nNCI implements a scratch purge policy, where files not accessed for 100 days are moved into quarantine for 14 days before permanent deletion. Files within quarantine still count towards project quotas.\nIf you are a member of multiple projects, understand that the filepath of the files and not the group ownership of the files dictates the project to which the disk resources are accounted. Please see this NCI update for more information."
  },
  {
    "objectID": "notebooks/06_accounting.html#inodes",
    "href": "notebooks/06_accounting.html#inodes",
    "title": "Accounting",
    "section": "iNodes",
    "text": "iNodes\nAn inode (index node) is a data structure used by Unix-like file systems to store metadata about a file or directory. In simple terms, each file and directory contributes an inode count of 1, ie the inode usage of a filesystem is the count of all files and directories stored on that filesystem. Like physical disk space, a filesystem also has an inode limit, and if they get used up (even if there’s still free disk space), you won’t be able to create new files. This can cause failure of a running job.\nMost users will not need to worry about inode limit. For each TB of scratch or gdata disk assigned to a project, an amount if inode that is suitable for most users is also provided. If however you run tools or jobs which generate large numbers of tiny files, you may need to implement close inode monitoring and management, such as periodically deleting or archiving tiny temp files as they are no longer needed.\nFrom the Gadi resources page:\n“Please try to keep the number of files as low as possible as this can affect the I/O performance in your job. Gadi is efficient at handling large scale parallel I/O but performance becomes significantly worse when doing frequent small operations. A main culprit for creating a large amount of files is the Python packaging system conda. Please use pip and the available modules that are already tuned for Gadi to keep file and folder count as low as possible.”\nYou can see the inode limits and usage for your project’s scratch and gdata with the nci_account -P &lt;nci-project-id&gt; command:\n\nThis project has been allocated (iAllocation) 4.5 million inodes in scratch, and has used (iUsed) 1.59 million inodes. With ~ 3 million inodes spare, there is plenty to continue work.\nThe lquota command shown previously also reports the inode usage, allocation and limit for all projects of which you are a member. The inode limit functions the same as the disk limit, whereby you can exceed the allocation/quota up to the inode limit, you cannot exceed the inode limit, and you cannot submit new jobs until the project is back under the inode allocation/quota.\nIf you would like to determine the total inode usage of a specific directory and its subdirectories, run this command from within that directory:\nfor d in `find -maxdepth 1 -type d |cut -d\\/ -f2 |sort`; do c=$(find $d |wc -l) ; printf \"$c\\t\\t- $d\\n\" ; done ; printf \"Total: \\t\\t$(find $(pwd) | wc -l)\\n\""
  },
  {
    "objectID": "notebooks/06_accounting.html#disk-and-inode-usage-on-home",
    "href": "notebooks/06_accounting.html#disk-and-inode-usage-on-home",
    "title": "Accounting",
    "section": "Disk and inode usage on /home",
    "text": "Disk and inode usage on /home\nEach user has a quota of 10 GB of storage in their home directory, which is private and backed up. They also have an inode allocation of 10,240 M.\nImportantly, home directories do not have a grace limit like scratch and gdata do! It is critical to actively monitor and manage your home directory usage to avoid preventable job failures.\nThe nci_account and lquota commands do not report usage on home. To view disk and inode quota and usage, use the command quota -s:\n\nThis user has filled 8643 M of physical space in their home directory, and has 109 K inodes used of the total quota of 4295 M inodes. Note there is no ‘grace’ (limit) value in home; the quotas cannot be exceeded."
  },
  {
    "objectID": "notebooks/06_accounting.html#key-points",
    "href": "notebooks/06_accounting.html#key-points",
    "title": "Accounting",
    "section": "Key points",
    "text": "Key points\n\nNCI accounts for disk usage, inode usage, and SU\nIt is your responsibility to monitor and manage your usage of these resources as you work\nUse command nci_account -P &lt;nci-project-id&gt; to check SU of a specific project for the current quarter, as well as disk and inode for that project\nUse command lquota to check disk and inode usage and allocation for all projects you are a member of\nUse quota -s to check disk and inode usage in your home directory\nRequests for more SU can be made through the Sydney Scheme management portal\nRequests for more scratch disk and scratch inode can be made through the NCI help desk, and will require justification\nRequests for more gdata disk and gdata inode can be made by emailing the Sydney Scheme manager, and will require justification"
  },
  {
    "objectID": "notebooks/03_expectations.html",
    "href": "notebooks/03_expectations.html",
    "title": "What does the system expect of its users?",
    "section": "",
    "text": "Gadi is a shared resource and its efficient use not only ensures fair access for all users but also helps minimise the environmental impact of high-performance computing, as systems like Gadi consume significant energy resources. When you are using a system like Gadi, there are potentially hundreds of other users accessing the system at the same time as you. For Gadi to remain efficient and usable, everyone needs to be courteous and use the system with consideration for others.\nTo help you be a good citizen of the NCI HPC community, please review the do’s and don’ts of using Gadi as well as the general tips below."
  },
  {
    "objectID": "notebooks/03_expectations.html#use-job-queues-appropriately",
    "href": "notebooks/03_expectations.html#use-job-queues-appropriately",
    "title": "What does the system expect of its users?",
    "section": "Use job queues appropriately",
    "text": "Use job queues appropriately\n\nGadi job queues\nGadi queue limits\n\nGadi runs a PBSpro job scheduler that manages the allocation of resources to users. When you submit a job, it is placed in a queue and will run when the requested resources become available. Unlike on Artemis where your job is allocated to a suitable queue based on your resource request, Gadi users need to explicitly request their job is sent to a specific queue. It is important for you to pick a job queue that is appropriate for your job."
  },
  {
    "objectID": "notebooks/03_expectations.html#responsibly-manage-your-data",
    "href": "notebooks/03_expectations.html#responsibly-manage-your-data",
    "title": "What does the system expect of its users?",
    "section": "Responsibly manage your data",
    "text": "Responsibly manage your data\n\nNCI file management policy\nTransferring data between RDS and Gadi\n\n/scratch is not a safe space for long term data storage. If it has not been accessed in 100 days, it will be subjected to NCI’s clean up policy. If you have a /g/data allocation, this is a better place to store data that you will regularly need as input for your jobs. While gdata is not subjected to purge like scratch, it is not backed up. You should back up all input, job scripts and important output files to RDS. Please follow the data transfer between Gadi and RDS guide for th best ways to do this."
  },
  {
    "objectID": "notebooks/03_expectations.html#dont-request-more-resources-than-you-need",
    "href": "notebooks/03_expectations.html#dont-request-more-resources-than-you-need",
    "title": "What does the system expect of its users?",
    "section": "Don’t request more resources than you need",
    "text": "Don’t request more resources than you need\n\nBenchmarking tasks on Gadi\n\nDon’t request resources that you won’t need, it will only result in your job and other users jobs being held up, and you wasting your service unit allocation. It can be hard to know what resources a tool needs, and this can vary on different hardware. We suggest the following:\n\nStep 1: Consult the software documentation\n\nOften, developers will outline the minimum amount of RAM (memory) and whether a tool is multi-threaded (e.g. use &gt;1 CPU or GPU)\n\nStep 2: Run a test job using our Gadi benchmarking tool\n\nThis will give you a good idea of how much resources you need to request for your main job.\n\nStep 3: Ask for help"
  },
  {
    "objectID": "notebooks/03_expectations.html#keep-track-of-your-resource-usage",
    "href": "notebooks/03_expectations.html#keep-track-of-your-resource-usage",
    "title": "What does the system expect of its users?",
    "section": "Keep track of your resource usage",
    "text": "Keep track of your resource usage\nRunning jobs on gadi requires users to have sufficient service units (compute hours) available. It is also important to monitor your use of physical disk space and inodes on scratch, home and gdata. Please see the accounting section or the NCI pages below for more details:\n\nMonitor your jobs\nMonitor your project allocation\nWhat does a job cost?\nWhy are my jobs not running?"
  },
  {
    "objectID": "notebooks/03_expectations.html#dont-misuse-the-login-nodes",
    "href": "notebooks/03_expectations.html#dont-misuse-the-login-nodes",
    "title": "What does the system expect of its users?",
    "section": "Don’t misuse the login nodes",
    "text": "Don’t misuse the login nodes\nLogin nodes are for logging in to the system, basic file and directory navigation commands, submitting jobs etc. Login nodes are not for large data transfers, compute tasks, excessive qstat queries, or submitting jobs via high-iteration for loops. Doing so will overload these nodes, causing a slow-down and frustration for all users. NCI monitors login node traffic and inappropriate use will be targeted. Please see the sections on data transfer and parallel jobs for recommended strategies for these tasks."
  },
  {
    "objectID": "notebooks/11_software.html",
    "href": "notebooks/11_software.html",
    "title": "Software options on Gadi",
    "section": "",
    "text": "global apps\nspecialised environments\nself-installed tools\nsingularity containers"
  },
  {
    "objectID": "notebooks/11_software.html#global-apps",
    "href": "notebooks/11_software.html#global-apps",
    "title": "Software options on Gadi",
    "section": "Global apps",
    "text": "Global apps\nOn Gadi, there are shared (global) apps that are installed and managed by the system administrators. On Artemis, these were in /usr/local/ directory, on Gadi they are in /apps/ directory.\nYou can use the same module commands that you are familiar with on Artemis to query and load apps on Gadi.\n# List all global apps starting with 'p':\nls  /apps/p*\n# List all modules for python3:\nmodule avail python3\n# Load a specific version of python3:\nmodule load python3/3.12.1\nEach global app has a default version, so if you run without specifying a version, the default version will be loaded. While this is OK in some circumstances, it is typically recommended to specify the version you know works for your code. Default versons of global apps will change over time without warning, so reproducibility and functionality is best maintained by explicitly stating the version when you load a module within your script."
  },
  {
    "objectID": "notebooks/11_software.html#software-groups-and-specialised-environments",
    "href": "notebooks/11_software.html#software-groups-and-specialised-environments",
    "title": "Software options on Gadi",
    "section": "Software groups and specialised environments",
    "text": "Software groups and specialised environments\nNCI provides a range of software groups and specialised environments for different resarch fields. Before attempting to self-install a tool, you should check if the tool you need is avalable through global apps or one of these environments.\nPlease visit the NCI pages for more details:\n\nAI/machine learning\ndata analysis\nbioinformatics and genomics\nclimate and weather\nearth observation and environment\ngeophysics\nquantum computing\nvisualisation"
  },
  {
    "objectID": "notebooks/11_software.html#self-installed-tools",
    "href": "notebooks/11_software.html#self-installed-tools",
    "title": "Software options on Gadi",
    "section": "Self-installed tools",
    "text": "Self-installed tools\nUnlike Artemis, request of new apps to be installed are not always agreed to. NCI limits global apps to those with a high user base. This is to ensure good maintenance and curation of global apps.\nUsers are encouraged to either self-install apps from source into their /home or /g/data locations, or (recommended) use singularity containers. Installing into /scratch is not recommended due to the 100-day file purge policy. Install into /g/data is ideal when other members of your project need to use the same tool.\nNCI may provide support for users through the self-install process; to request assistance, please email your detailed request including what software tool and version you are attempting to install and describe the issues you are having with the installation.\nOnce a tool has been installed, you can make that tool available to the module commands, by following the steps described here. This is not essential, but can be helpful when managing tools that multiple group members will use.\nTo avoid the burden of installing software that is not available through global apps or specialised environments, the use of singularity containers is recommended."
  },
  {
    "objectID": "notebooks/11_software.html#running-singularity-containers-on-gadi",
    "href": "notebooks/11_software.html#running-singularity-containers-on-gadi",
    "title": "Software options on Gadi",
    "section": "Running singularity containers on Gadi",
    "text": "Running singularity containers on Gadi\nSingularity can be used to execute containerised applications on Gadi. It is installed as a global app:\nmodule load singularity\nsingularity version\n# 3.11.3\nNote that the singularity project was recently migrated to apptainer. Please continue to run singularity commands on Gadi for now.\nSingularity (Apptainer) is a “container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Apptainer on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don’t have to worry about how to install all the software you need on each different operating system.” (from apptainer.org)\nThere are numerous container repositories, for example Docker Hub or quay.io. You can search these repositories, or build your own container if the tool or tool version you need is not yet available.\nSeqera have greatly simplifed the process of building custom containers with their build-your-own container tool! Simply search for the tool(s) you want in your container, and click Get Container. This tool will manage the build for you, and host the created container file.\n\nExample\nLet’s assume you want to use the tool FoldSeek. Below are the steps to search for, obtain, test, and use this container in a PBS job script.\n\nRun module load singularity in your Gadi terminal (or copyq container download script)\nVisit quay.io and search for this tool by typing the tool name into the search bar at the top right of the page. There may be multiple containers available. These may reflect different contributors, different tool versions, etc. Look for containers with recent updates and a high star rating where possible.\nSelect the biocontainers container, and then select the Tags page from the icons on the left (options are Information, Tags, Tag history).\nOn the far right of the most recent tag, select the Fetch tag icon and then change image format to Docker Pull (by tag).\nCopy the command. We need to made some changes to the command before we execute it.\nPaste the command into your terminal (or copyq container download script) and change docker to singularity and add prefix docker:// to the contaner path, as shown below:\n\n# Default command:\ndocker pull quay.io/biocontainers/foldseek:10.941cd33--h5021889_1\n# Change to:\nsingularity pull docker://quay.io/biocontainers/foldseek:10.941cd33--h5021889_1\n\nRun the pull command (note: you need to have the singularity module loaded in your terminal or download script). This will download the docker container to a Singularity Image File (.sif). Most containers are lightweight enough to pull directly on the login node. If a container is bulky and slow to download (or you need many containers), you may need to submit the download as a copyq job.\nTest the container with a basic help or version command. When running a container, typical usage involves the following command structure:\n\nsingularity  exec &lt;container&gt; &lt;command&gt; [args]\nSo everything after the container name is the same as you would run when using a locally installed version of the tool. For foldseek, we would use foldseek version to view the tool verison or foldseek help to view the help menu. To run these commands via the container:\nsingularity exec foldseek_10.941cd33--h5021889_1.sif foldseek version\nsingularity exec foldseek_10.941cd33--h5021889_1.sif foldseek help\nNote that the version of foldseek corresponds to the tag name: in this case, v. 10.941cd33.\n\nTest run the full tool command that you need to use in your analysis. Where possible, use a small subset of your data to test the command (as you would routinely do for new software). You may need to use the interactive job queue or a compute job script for testing if the test command exceeds resource limits on the login nodes.\nAdd the command to your job script. Ensure to include module load singularity and call the container using the structure singularity  exec &lt;container&gt; &lt;command&gt; [args].\n\nBelow is an example Gadi job script running the foldseek container:\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -q normal\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l walltime=02:00:00\n#PBS -l storage=scratch/aa00+gdata/aa00\n#PBS -l wd\n\nmodule load singularity\n\ninput=/g/data/aa00/foldseek-inputs/input.fasta\ndatabase=/g/data/aa00/foldseek-inputs/database\noutput_dir=./foldseek-run/output\nresults=./foldseek-run/foldseek_result.tsv\nlog=./foldseek-run/foldseek.log\n\nsingularity exec \\\n    foldseek_10.941cd33--h5021889_1.sif \\\n    foldseek easy-search  ${input} \\\n    ${database} \\\n    ${output_dir} \\\n    --threads ${PBS_NCPUS} \\\n    ${results} &gt; ${log} 2&gt;&1"
  },
  {
    "objectID": "notebooks/12_walltime.html",
    "href": "notebooks/12_walltime.html",
    "title": "Working within walltime limits",
    "section": "",
    "text": "In this section, we will discuss ways of adapting your long walltime jobs from Artemis to NCI platforms.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/12_walltime.html#introduction",
    "href": "notebooks/12_walltime.html#introduction",
    "title": "Working within walltime limits",
    "section": "",
    "text": "In this section, we will discuss ways of adapting your long walltime jobs from Artemis to NCI platforms.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/12_walltime.html#gadi-walltime-limit",
    "href": "notebooks/12_walltime.html#gadi-walltime-limit",
    "title": "Working within walltime limits",
    "section": "Gadi walltime limit",
    "text": "Gadi walltime limit\nThe maximum walltime permitted on any of the Gadi HPC queues is 48 hours. In some cases, the walltime may be less (for example when requesting large numbers of nodes, or on copyq). See the Default walltime limit column of the queue limits tables to discover the maximum walltime that applies according to your resources requested.\nGiven that Artemis has much longer maximum walltimes, we understand this may generate some apprehension. Staff at both NCI and SIH can support you in adapting your workflows to NCI if you are still having difficulty after reviewing the suggestons below.\nIn short, there are 3 options to adapting a long-running Artemis workflow to NCI:\n\nSplit your single large job Artemis into a series of smaller jobs on Gadi\nUse NCI’s Nirin cloud instead of Gadi\nSpecial exception to the Gadi walltime limit granted on a case-by-case basis"
  },
  {
    "objectID": "notebooks/12_walltime.html#option-1.-split-or-checkpoint-your-job",
    "href": "notebooks/12_walltime.html#option-1.-split-or-checkpoint-your-job",
    "title": "Working within walltime limits",
    "section": "Option 1. Split or checkpoint your job",
    "text": "Option 1. Split or checkpoint your job\nThere are many advantages to splitting your job up into smaller discrete chunks.\n\nCheckpointing: if one of your jobs in a series fails, you only need to resubmit that discrete job script, rather than either the whole job or some finnicky “hashed out” version of your very long and complex workflow script. This simplifies debugging and rerunning, saves you hands-on time and walltime, minimises errors, and saves KSU\nEase of code maintenance: changing part of workflow, for example adjusting parameters, input files or software versions, is far simpler to implement for shorter chunks of code than it is for a long and complex code with many steps\nEase of benchmarking: Different stages of a complex workflow typically have different compute requirements, for example some long running single core tasks coupled with some GPU tasks, some high memory, some high CPU tasks etc. Benchmarking is more straightforward and informative when performed on discrete workflow chunks.\nGreater job efficiency: By benchmarking and optimising the resource configurations for each stage of the workflow, the series of jobs can be placed on an appropriate queue, and will not be reserving (and being charged for) unused resources. This will reduce KSU usage and resource wastage.\nShorter queue times: Requesting resources for a shorter walltime will result in a shorter queue time. The NCI scheduler is geared towards favouring ‘wider and shorter’ jobs, ie more CPUs/nodes for less time, over ‘taller and slimmer’ jobs (ie fewer CPUs/nodes for a longer time). For example a job may queue for less time if it requests 48 CPU for 1 hour, compared to 1 CPU for 48 hours. Of course the queue is highly dynamic and this cannot be predicted or calculated ahead of time, but in general, shorter walltimes will lead to shorter queue times.\n\nAt the end of this section we will demonstrate a handful of examples of real long-running Artemis workflows that have been adapted to fit within Gadi’s shorter maximum walltime."
  },
  {
    "objectID": "notebooks/12_walltime.html#option-2.-use-nirin",
    "href": "notebooks/12_walltime.html#option-2.-use-nirin",
    "title": "Working within walltime limits",
    "section": "Option 2. Use Nirin",
    "text": "Option 2. Use Nirin\nYour NCI KSUs can be used on Nirin as well as Gadi. Nirin has the advantage of theoretically infinite walltime, along with internet access which is another limitation of the Gadi compute queues.\nAs such, Nirin presents an easily accessible solution for users whose jobs are irreconcilably affected by the walltime and lack of internet access aspects of Gadi.\nThe Nirin quickstart guide walks you through the process of setting up your instance, including easy to follow screenshots for each step."
  },
  {
    "objectID": "notebooks/12_walltime.html#option-3.-gadi-special-walltime-request",
    "href": "notebooks/12_walltime.html#option-3.-gadi-special-walltime-request",
    "title": "Working within walltime limits",
    "section": "Option 3. Gadi special walltime request",
    "text": "Option 3. Gadi special walltime request\nIf your job cannot be split/checkpointed into a series of shorter jobs, and the Nirin flavours are not suited to your compute needs, you can make a request to NCI for an increase to the walltime. NCI will ask you to provide details of your job including the relevant code saved on Gadi, as well as a description of why you require a lift to the walltime for this particular job.\nFrom the Gadi queue limits page:\n“If a higher limit on core count (PBS_NCPUS) and walltime is needed, please launch a ticket on NCI help desk with a short description of the reasons why the exception is requested. For example, a current scalability study suggests linear speedup at the core count beyond the current PBS_NCPUS limit. We will endeavour to help on a case-by-case basis”"
  },
  {
    "objectID": "notebooks/12_walltime.html#examples-of-splitcheckpointed-jobs",
    "href": "notebooks/12_walltime.html#examples-of-splitcheckpointed-jobs",
    "title": "Working within walltime limits",
    "section": "Examples of split/checkpointed jobs",
    "text": "Examples of split/checkpointed jobs\n\nExample 1: A workflow with multiple discrete commands\nIn the field of genomics, the raw data is processed through a series of steps before the final output files are produced. Many groups perform all of these steps within a single, multi-command job, in order to only have to run one job to perform all the work.\nBy splitting apart each of these steps so that each is its own job, we can improve code manageability, reduce walltime, and increase overall processing efficiency.\nWhile this does require some extra effort in terms of submitting multiple processing jobs rather than just one, the benefits described above far outweigh this. The burden of multiple job submission can be ameliorated by parallel processing per sample, and even further by a workflow manager such as Nextflow. For an exammple Nextflow genomics processing workflow, view this repository, and for parallel jobs on Gadi, see this section.\nIn the below Artemis script example, a samplesheet is read in containing metadata from about 10 samples to be analysed. Each sample has one pair of raw ‘fastq’ input files that are processed through an analysis loop containing 4 steps:\n\n\n\n\n\n\n\n\n\n\nStep\nTask\nInput\nWalltime (hrs)\nCPUs used\n\n\n\n\n1\nQuality check\nRaw data\n2\n2\n\n\n2\nMap to reference\nRaw data\n7\n24\n\n\n3\nRecalibration metrics\nOutput of step 2\n5.5\n1\n\n\n4\nApply recalibration\nOutput of steps 2 + 3\n8.5\n1\n\n\n\nThe total walltime is 23 hours per sample, so the requested walltime in the below script is 240 hours (10 samples x 24 hours per sample)\nThere are multiple inefficiencies within this method, giving rise to an inflated walltime requirement of ~ 1 day per sample plus a very low overall CPU utilisation of the job.\n\nNow compare the above to a Gadi workflow, where each of these 4 steps are separated into their own job, with appropriate resource requests per job.\nJob 1: Quality check\nThe fastQC tool can only run one thread per file. If you provide multiple files through globbing, and provide multiple CPUs to the -t flag, it will process as many files at a time as the value you have provided to -t.\nSo for the current example with 10 samples each with a pair of files, we have 20 files and can run this section of the analysis workflow with 20 CPU. In this way, 100% of the 20 CPU requested are utilised, unlike the Artemis script above, where only one sample’s fastq files at a time could be analysed and thus used only 8.3% (2 CPU used of 24 requested).\n\nJob 2: Map to reference\nThe bwa tool can multi-thread, and tool benchmarking in peer-reviewed literature shows almost perfect scalability up to a thread count of 36. Gadi normal queue has 48 CPU per node, so you could run this job with 48 CPU, assigning 36 CPU to the mapping and 12 CPU to the piped sort command.\nThe key detail is to map each sample’s raw data as it’s own distinct job, instead of looping over each sample in series like the demo Artemis script. On Artemis, we can do this with job arrays but these are not available on Gadi. NCI and SIH recommend the use of nci-parallel (a custom wrapper utility for OpenMPI) for repeated runs of the same job script - see parallel jobs on Gadi for more details.\nTo avoid complicating this walltime section, we will provide an example of using a simple for loop for job submission. NOTE: loops should ONLY be used for a VERY SMALL NUMBER OF JOBS, and always include a sleep in the loop! NCI does monitor the login nodes and serial offending with long for loops will be targeted!\nNote that the directive for job name is provided on the command line as an argument to qsub, the sample metadata is provided with the qsub -v varname=\"varvalue\" syntax, and a 3-second sleep is used to avoid over-loading the job scheduler.\nScript:\n\nTo submit 10 samples as separate jobs:\nwhile read SAMPLE FC LN LIB PL CN\ndo\n  qsub -N map-${SAMPLE} -v SAMPLE=\"${SAMPLE}\",FC=\"${FC}\",LN=\"${LN}\",LIB=\"${LIB}\",PL=\"${PL}\",CN=\"${CN}\" step2_map.pbs \n  sleep 3\ndone &lt; my_samples_metedata.txt\nJob 3 and 4: Recalibration metrics and apply recalibration\nNote from the table above that these two steps do not multi-thread, and both have long walltimes. If you require a task like this in your workflow, it’s critical to interrogate the tool documentation for ways to increase throughput and efficiency.\nWithin this tool’s guide, we find there is a -L interval flag, which allows the tool to operate over discrete intervals of the reference file, rather than scanning the sample data over the whole reference file in one long running single-CPU task. The smaller the interval, the faster the run time, and the resultant output files are merged. This is an example of scatter-gather parallelism, where smaller sub tasks are scattered (distributed across the compute cluster) and then gathered (in this case, merged) to massively speed up a ‘tall and slim’ job (few resources consumed for a long time) into a ‘short and wide’ job (many resources consumed for a short time). Introducing parallelism into your job is crucial to get the most out of HPC.\nSince this section is not a specialised bioinformatics training, we will not go into details for this tool here, but instead provide the main overview of steps and how with a bit of extra work, massive walltime savings can be made.\nSteps 3 and 4 from the Artemis workflow are now executed as 5 jobs:\n\nSplit the reference file into intervals using the tools’s split intervals function\nRun step 3 over each interval for each sample as a separate job. For 32 intervals and 10 samples, that is 32 * 10 = 320 single-CPU jobs. To do this, we would use Open MPI via nci-parallel\nMerge the 32 outputs per sample into a single per-sample file with the tool’s merge function, using nci-parallel to launch the 10 sample * 1 CPU jobs\nRun step 4 over each interval for each sample as a separate job, using the merged output of step 3, another 32 * 10 = 320 single-CPU jobs launched in parallel by nci-parallel\nMerge the 32 outputs per sample into one final output file per sample with the tool’s merge function, using nci-parallel to launch the 10 sample * 1 CPU jobs\n\nAs you can see, our workflow which was one long-running single job with very poor overall CPU utilisation has now been split into 7 jobs. This may sound tedious, yet the massively improved walltime and CPU utilisation will pay off, and you will get to your results in a much faster turnaround time with fewer KSU expended. In this example, walltime of 240 hours has been reduced to 10 hours!\n\n\n\n\n\n\n\n\n\n\n\nStep\nTask\nInput\nWalltime (hrs)\nCPUs used per job\nCPUs total\n\n\n\n\n1\nQuality check\nRaw data\n2\n20\n20\n\n\n2\nMap to reference\nRaw data\n7\n48\n480\n\n\n3\nSplit intervals\nReference file\n&lt;1\n1\n1\n\n\n4\nRecalibration over intervals\nOutput of steps 2 + 3\n&lt;1\n1\n320\n\n\n5\nMerge recalibration tables\nOutput of step 4\n&lt;1\n1\n10\n\n\n6\nApply recalibration over intervals\nOutput of steps 2 + 5\n&lt;1\n1\n320\n\n\n7\nMerge recalibrated final output\nOutput of step 6\n&lt;1\n1\n10\n\n\n\n\n\nExample 2: A long running command with innate checkpointing\nMany commonly used tools have a built in option for saving the state of the process to a local file after a specified number of iterations in a repetitive task. This local file can be reloaded from disk and then the process can restart from where it previously left off. This is often called ‘checkpointing’. It’s a good idea to check the documentation of the tool you are using to see if it has some kind of checkpointing functionality built in since this greatly simplifies splitting up long running tasks into smaller chunks.\nIt is also recommended practice to use some kind of checkpointing because that way you can have a look at the output of jobs mid-way through to ensure that things are progressing as expected. Jobs can then be restarted with different parameters in case of unwanted results, thus saving resource allocations on your project.\nCheckpointing is often very simple to implement in iterative parameter minimisation tasks such as machine learning pipelines that train or fine-tune a model. As such most machine learning packages will have options for checkpointing built-in. If you are building your own machine learning pipeline, simple instructions can be found for how to implement checkpointing in either TensorFlow here or PyTorch here.\nIn this example we’ll demonstrate a script with checkpointing that comes from the SIH developed aigis package. aigis is a tool that fine-tunes a detectron2 image segmentation machine learning model to detect trees and buildings in aerial imagery datasets.\nAerial imagery datasets can be quite large and fine-tuning detectron2 models using them can take a long time. Because of this the aigis fine-tuning script has a flag that will only let it run for a given number of iterations and save the fine-tuned model to disk. The script can then be restarted with the previous output given as input.\nBelow shows an example that runs the aigis script called fine_tune_detectron2.py on artemis. The PBS script looks like:\naigis_script.pbs\n#! /bin/bash\n\n#PBS -P SIHNextgen\n#PBS -N fine_tune_example\n#PBS -l select=1:ncpus=1:mem=16gb:ngpus=1\n#PBS -l walltime=60:00:00\n#PBS -q defaultQ\n\n...   # Module loading and setup go here\n\n#Actually run the program\nfine_tune_detectron2.py --dataset-name MyDataSet --train-json MyDataSet_Train.json --test-json MyDataSet_Test.json --max-iter 20000 --output-dir /project/aigis/model_fine_tuning\nThe full run of this script with 20000 iterations (--max-iter 20000) takes 60hrs, which is longer than the maximum walltime allowed on gadi, however we can change the number of iterations to 10000 and run the script twice with checkpointing.\nTo do this we would convert the above PBS script above to run on gadi and set the walltime to 31 hours and max-iter to 10000:\niteration1.pbs\n#! /bin/bash\n\n#PBS -P qc03\n#PBS -N fine_tune_example_s1\n#PBS -l walltime=31:00:00\n#PBS -l ncpus=1\n#PBS -l ngpus=1\n#PBS -l mem=16GB\n#PBS -q gpuvolta\n#PBS -l wd\n#PBS -lstorage=scratch/qc03\n\n# NOTE: We use 31 hours waltime rather than 30\n# to allow for overheads in running the script\n\n...   # Module loading and setup go here\n\nfine_tune_detectron2.py --dataset-name MyDataSet --train-json MyDataSet_Train.json --test-json MyDataSet_Test.json --max-iter 10000 --output-dir /scratch/qc3/model_fine_tuning\nThen we can create a second pbs script that will pick up the output of the above and continue running for 10000 more iterations, again allowing half the walltime:\niteration2.pbs\n#! /bin/bash\n\n#PBS -P qc03\n#PBS -N fine_tune_example_s2\n#PBS -l walltime=31:00:00\n#PBS -l ncpus=1\n#PBS -l ngpus=1\n#PBS -l mem=16GB\n#PBS -q gpuvolta\n#PBS -l wd\n#PBS -lstorage=scratch/qc03\n\n...   # Module loading and setup go her - likely the same as above.\n\nfine_tune_detectron2.py --dataset-name MyDataSet --train-json MyDataSet_Train.json --test-json MyDataSet_Test.json --max-iter 10000 --output-dir /scratch/qc3/model_fine_tuning --model-file /scratch/qc3/model_fine_tuning/MODEL_9999.pth\nWe can split this into as many shorter chunks as we would like, each starting from the output of the previous iteration. These can then be run sequentially by adding the flag -W depend=afterok:&lt;jobid&gt; to qsub for each script after the first at the gadi command prompt like so:\n\nSubmit iteration1.pbs at the command line\n\n$ qsub iteration1.pbs\n136554542.gadi-pbs\n\ngadi will return the jobid of the first script: 136554542.gadi-pbs. You will need the jobid of the first script to pass to the second qsub command.\nNow we tell the scond script to only execute after the job in the first script has finished:\n\n$ qsub -W depend=afterok:136554542.gadi-pbs iteration2.pbs\nWhen you run qstat after this you will see the second script will have state H saying it has been held until iteration1.pbs has completed.\nIf you find the task of entering all these qsub commands tedious you can also make a bash script which you only need to run once to submit all the scripts:\nsubmit_jobs_checkpointed.py\n#!\\bin\\bash\n\n# Submit first PBS script and save the run id to JOB1.\nJOB1=$(qsub iteration1.pbs)\n\n# Submit second pbs script and tell it to run only after the first has COMPLETED.\n# Save the second run id to JOB2\nJOB2=$(qsub -W depend=afterok:$JOB1 iteration2.pbs)\nYou can of course chain as many iterations as necessary by continuing the above script.\nPlease be aware when you submit multiple long-running job scripts in this way that you should check the output from time to time to ensure everything is going smoothly, and kill the running script if it isn’t before fixing issues and restarting.\n\n\nExample 3: Add your own checkpointing to existing code\nSometimes, you might be running your own software which has no checkpointing available. In this case it still might be possible to add checkpointing to your code yourself with a minimum of fuss, particularly if the code you are running is a long sequential pipeline where each step depends on the result of the previous one.\nAn effective way to add checkpointing to existing code is by saving the current state of the variables in your code to disk and then reloading them before starting the next step. The process of doing this is called serialisation and many different popular python packages (e.g. Numpy, Pandas) are able to do it with a simple command. Even native python variables (lists dicts etc. can be saved to disk using the pickle module).\nAn example of how you might do this comes from an astronomy pipeline (VAST) which reads thousands of sky images and makes multiple measurements from then followed by associating the stars in them with one another and then generating statistics on each star.\nThis simple pipeline is easily split into multiple steps: 1. Read images 2. Measure stars in images 3. Associate stars with one another between images 4. Generate statistics about the stars.\nHere is a minimal example of the pipeline code between step 1 and 2. Step one is simple a function that returns a pandas DataFrame containing the image information and step 2 is another function that reads that dataframe before measureing stars in the image data from step 1.\npipeline.py\n# Imports and setup at top of pipeline script\n\n...\n\n# Pipeline Step 1: read images\n# Some call to a function that reads images into a pandas dataframe\nimages_df = get_parallel_assoc_image_df(\n                image_file_paths, ...\n            )\n\n...\n\n# Pipeline Step 2: make measurements\n# Call a function that measures stars from our image data in images_df\nsources_df = get_sources(\n                images_df, ...\n             )\n\n...\nThis can be split into python sripts step1 and step2 where the end of step1 involves writing the returned dataframe to disk and then the start of step2 involves reading the result from disk.\nstep1.py\n\n# Pipeline Step 1: read images\n# Some call to a function that reads images into a pandas dataframe\nimages_df = get_parallel_assoc_image_df(\n                image_file_paths, ...\n            )\n\n...\n\nimages_df.write_parquet('images_df.parquet')\nstep2.py\nimport pandas as pd\n\n# Possible config setup here.\n\nimages_df = pd.read_parquet('images_df.parquet')\n\n# Pipeline Step 2: make measurements\n# Call a function that measures stars from our image data in images_df\nsources_df = get_sources(\n                images_df, ...\n)\n\n...\n\npd.write_parquet(...)\nThese scripts can then be run sequentially using the method described in the previous example."
  },
  {
    "objectID": "notebooks/02_system_setup.html",
    "href": "notebooks/02_system_setup.html",
    "title": "Gadi overview",
    "section": "",
    "text": "This section will point you to the right sections of the NCI documentation and user guides to get you started on the Gadi HPC system."
  },
  {
    "objectID": "notebooks/02_system_setup.html#resources",
    "href": "notebooks/02_system_setup.html#resources",
    "title": "Gadi overview",
    "section": "Resources",
    "text": "Resources\n\nWhat is Gadi?\nGetting Started at NCI\nGadi User Guide\nGadi FAQs"
  },
  {
    "objectID": "notebooks/02_system_setup.html#gadi-technical-summary",
    "href": "notebooks/02_system_setup.html#gadi-technical-summary",
    "title": "Gadi overview",
    "section": "Gadi technical summary",
    "text": "Gadi technical summary\nNCI Gadi is one of Australia’s most powerful supercomputers, designed to support advanced computational research.\n\n\n\n\n\n\n\nComponent\nDetails\n\n\n\n\nCompute\n- Nodes: 4,962- Processors: Intel Sapphire Rapids, Cascade Lake, Skylake, and Broadwell CPUs- GPUs: NVIDIA V100 and DGX A100 GPUs- Performance: Over 10 petaflops of peak performance :contentReferenceoaicite:0\n\n\nStorage\n- Disk Drives: 7,200 4-Terabyte hard disks in 120 NetApp disk arrays- Capacity: 20 Petabytes total usable capacity- Performance: 980 Gigabytes per second maximum performance :contentReferenceoaicite:1\n\n\nFilesystems\n- Total Capacity: Approximately 90 Petabytes- Global Lustre Filesystems: Five, with an aggregate I/O performance of around 450 GB/second- IO Intensive Platform: Dedicated filesystem using 576 2-Terabyte NVMe drives, achieving around 960 Gigabytes per second cumulative performance :contentReferenceoaicite:2\n\n\nArchival Storage\n- Capacity: Over 70 Petabytes of archival project data stored in state-of-the-art magnetic tape libraries :contentReferenceoaicite:3\n\n\nNetworking\n- Interconnect: 100-gigabit network links connecting high-performance computing with high-performance data :contentReferenceoaicite:4\n\n\nCloud Systems\n- Nirin Cloud: High-availability and high-capacity zone integrated with Gadi and NCI’s multi-Petabyte national research data collections, comprising Intel Broadwell and Sandy Bridge processors and NVIDIA K80 GPUs :contentReferenceoaicite:5"
  },
  {
    "objectID": "notebooks/02_system_setup.html#conditions-of-use",
    "href": "notebooks/02_system_setup.html#conditions-of-use",
    "title": "Gadi overview",
    "section": "Conditions of use",
    "text": "Conditions of use\n\nConditions of use and policies\n\nAll users of NCI agree that they will keep themselves informed of, and comply with, all relevant legislation and The Australian National University policies and rules.\nAll users must acknowledge and understand that a breach of these will result in not only a loss of access to NCI resources but the user may be subject to Federal criminal prosecution resulting in fines and/or gaol legislated under the Acts listed on the conditions of use and policies page."
  },
  {
    "objectID": "notebooks/02_system_setup.html#login-nodes",
    "href": "notebooks/02_system_setup.html#login-nodes",
    "title": "Gadi overview",
    "section": "Login nodes",
    "text": "Login nodes\nThese nodes are the gateway for Gadi for users to access the resources of the HPC cluster. It is how you log in to Gadi, move around the filesystem, submit jobs to the scheduler, and do small tasks like view the contents of a file.\nWhen you first ssh into Gadi, you are working on a login node. These are distinct from the compute nodes and are not designed for running compute tasks or transferring data.\nWhile you can run simple commands on the login nodes on Gadi, commands are throttled. This means they execute more slowly than on the compute nodes and are automatically killed if they exceed CPU, memory and time restrictions that are in place. This is to ensure the login nodes are not over-loaded and remain responsive for all users. As such, all complex or resource-intensive tasks must be submitted to the job scheduler for execution on the cluster nodes."
  },
  {
    "objectID": "notebooks/02_system_setup.html#compute-nodes",
    "href": "notebooks/02_system_setup.html#compute-nodes",
    "title": "Gadi overview",
    "section": "Compute nodes",
    "text": "Compute nodes\nThese nodes are the workhorses of any HPC. They are dedicated for executing computational tasks as delegated by the job scheduler when you submit a job. There are various types of compute nodes with different hardware, built for different purposes on Gadi. Depending on the resource requirements of your job (e.g. high memory, GPUs) and the queue you specify, your job will be sent to a specific type of compute node.\nPlease see the queue structure on Gadi and the queue limits pages for detailed information on the compute node specifications.\nFor details of how to choose and request a specific queue for your job, please visit the section on PBS scripts and job submission."
  },
  {
    "objectID": "notebooks/02_system_setup.html#data-mover-nodes",
    "href": "notebooks/02_system_setup.html#data-mover-nodes",
    "title": "Gadi overview",
    "section": "Data mover nodes",
    "text": "Data mover nodes\nThese nodes are designed specifically for fast data movement. You can use these nodes to transfer files to and from Gadi at high-speed.\nPlease visit the Gadi file transfer page for more details.\nA comprehensive guide to data transfer between Gadi and University of Sydney RDS, including example commands, tempalte scripts and recommendations, can be found in the section on transferring data."
  },
  {
    "objectID": "notebooks/02_system_setup.html#filesystems",
    "href": "notebooks/02_system_setup.html#filesystems",
    "title": "Gadi overview",
    "section": "Filesystems",
    "text": "Filesystems\n\n$HOME\nWhen you first log in to Gadi, you’ll be placed in your personal $HOME directory (i.e. /home/555/aa1234). You are the only person who can access this directory. No work should be done in here due to the strict 10 GB storage limit, but you may wish to install things like custom R or Python libraries here. It is backed up.\nYou can navigate back here at any point if required by running:\ncd ~\n\n\n/scratch\nAll Gadi projects have a dedicated /scratch allocation that is only accessible to members of your project. This is only intended for active work and not for long-term storage. This is not backed up and any files not accessed for 100 days will be purged from the system, so be sure to back up your work to RDS following our data transfer guide.\nVisit the NCI scratch file management page for details on the purge policy and how to recover files from ‘quarantine’.\nScratch will be /scratch/&lt;nci-project-id&gt;. Each member of a project will have read/write permissions for this parent directory, as well as their /scratch/&lt;nci-project-id&gt;/&lt;nci-user-id&gt; directory.\nYou can navigate to your /scratch space by running:\ncd /scratch/&lt;project&gt;\n\n\n/g/data\nSome Gadi projects have a dedicated /g/data allocation that is only accessible to members of that group. This in intended for long-term large data storage, for example large reference files or databases that are regularly required for your compute jobs. Compute node jobs can read /g/data so this is an ideal place to store those files so that they are not subject to /scratch purge. Files that are on /g/data for this kind of use should also have a copy on RDS, since /g/data/ is not backed up.\nIf you are unsure if your project has a /g/data allocation, you can check by running:\ncd /g/data/&lt;project&gt;\nScratch is provided at no additional cost to users, however g/data has a cost per TB involved. This cost is covered by the NCI-Sydney Scheme for projects that have a justified need of it. To request /g/data storage space, please request this via an email to nci-sydney.scheme@sydney.edu.au with a brief justification.\n\n\n/apps\nThis directory is accessible to all Gadi users. It is a read-only system containing centrally installed software applications and their module files.\nPlease visit the software section for more information on global apps and alternatives."
  },
  {
    "objectID": "notebooks/00_gadi_access.html",
    "href": "notebooks/00_gadi_access.html",
    "title": "Accessing NCI",
    "section": "",
    "text": "To access any of NCI’s computing platforms including Gadi HPC, you first need to create an NCI account. You will also need to either create a new project or request to join an existing one. You can be a member of multiple projects.\nAll new users must create their account through the NCI online self service portal. To create your account you will need the following information:\nNote that resources at NCI are allocated to projects and not to individual users."
  },
  {
    "objectID": "notebooks/00_gadi_access.html#create-an-account-and-join-an-existing-project",
    "href": "notebooks/00_gadi_access.html#create-an-account-and-join-an-existing-project",
    "title": "Accessing NCI",
    "section": "Create an account and join an existing project",
    "text": "Create an account and join an existing project\n\nClick on ‘Sign up’ link on the NCI online self service portal: \nAccept the terms and conditions: \nProvide your personal details: \nProvide details on the project you’ll be working on. Select I need to join one or more existing projects. Enter the NCI project code (eg qc03 for the SIH training project) in the Projects menu: \nSelect University of Sydney as your institution: \n\nYour username will become active when a project Lead CI approves your request to join their project. You will receive a confirmation email from the Mancini system when your username is activated."
  },
  {
    "objectID": "notebooks/00_gadi_access.html#create-an-account-and-propose-a-new-project",
    "href": "notebooks/00_gadi_access.html#create-an-account-and-propose-a-new-project",
    "title": "Accessing NCI",
    "section": "Create an account and propose a new project",
    "text": "Create an account and propose a new project\n\nTo create your account, follow the first three steps from the above section\nOn the NCI page Step 3 of 5 - Project, select I intend to propose a new project\nComplete the account creation form and verify your account by following the account verification email\nOnce activated, you can login to MyNCI and propose a new project by selecting this from the menu bar on the left:\n\n\n\nFollow the prompts to fill the form for your proposed project\nAt Propose a project: step 5 of 10, select University of Sydney\nAt Propose a project: step 6 of 10, select Gadi (at NCI) under HPC Compute. Most projects will not require massdata. Some may require gdata, and this can be added later if necessary. All new projects are allocated some scratch space and this comes at no charge, however the other two file systems have an attached cost.\nAt Propose a project: step 7 of 10, you are asked to fill in your initial resource requirements in KSU. One KSU is 1,000 service units, which is roughly equivalent to 500 CPU hours depending on the type of resource used. We expect 1 KSU to be a sufficient starting amount for most projects coming from Artemis. If you have a good understanding that you will require more than this, please enter those values here. They can easily and quickly be increased at a later date if needed.\nComplete the rest of the form, ensuring that you carefully review the terms and conditions of use and declare the status of your project in relation to the Defence Trade Controls Act.\n\nYour new project will become active when a USyd Scheme Manager approves your new project proposal."
  },
  {
    "objectID": "notebooks/00_gadi_access.html#i-already-have-an-nci-account-how-can-i-join-an-existing-project",
    "href": "notebooks/00_gadi_access.html#i-already-have-an-nci-account-how-can-i-join-an-existing-project",
    "title": "Accessing NCI",
    "section": "I already have an NCI account, how can I join an existing project?",
    "text": "I already have an NCI account, how can I join an existing project?\n\nLogin to MyNCI and select Projects and groups from the menu bar on the left\nSwitch the tab to Find project or group and search for the project you wish to join\n\n\n\nSelect the project from the search results, confirm the project by checking the overview, then switch the tab to Join\nAfter agreeing to the terms and conditions, select Request membership. You membership needs to be approved by the Lead Chief Investigator"
  },
  {
    "objectID": "notebooks/08_job_script.html",
    "href": "notebooks/08_job_script.html",
    "title": "Running jobs on Gadi",
    "section": "",
    "text": "In this section, we will compare Artemis job scripts to Gadi job scripts, and provide some guidance on how to adapt your Artemis workflow to NCI Gadi HPC.\nWatch the pre-recorded session\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first two on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/08_job_script.html#introduction",
    "href": "notebooks/08_job_script.html#introduction",
    "title": "Running jobs on Gadi",
    "section": "",
    "text": "In this section, we will compare Artemis job scripts to Gadi job scripts, and provide some guidance on how to adapt your Artemis workflow to NCI Gadi HPC.\nWatch the pre-recorded session\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first two on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/08_job_script.html#job-scheduler",
    "href": "notebooks/08_job_script.html#job-scheduler",
    "title": "Running jobs on Gadi",
    "section": "Job scheduler",
    "text": "Job scheduler\nLike Artemis, NCI runs the Altair PBS professional workload manager.\nWhile you can run simple commands on the login nodes on Gadi, commands are throttled. This means they execute more slowly than on the compute nodes and are automatically killed if they exceed CPU, memory and time restrictions that are in place. This is to ensure the login nodes are not over-loaded and remain responsive for all users. As such, all complex or resource-intensive tasks must be submitted to the job scheduler for execution on the cluster nodes.\nSubmitting jobs on Gadi is very similar to submitting jobs on Artemis. You will submit a PBS (Portable Batch System) submission script that specifies your job’s compute requirements along with the commands to execute the tasks.\nPBS scripts are text files that contain directives and commands that specify the resources required for a job and the commands to run. Typically they are named &lt;script_name&gt;.pbs however the .pbs suffix is not required, merely helpful to discern the intention of the script.\nOnce submitted to the PBS job scheduler with the qsub command, the scheduler reads the compute requirements from the directives component of the script, and either runs your job right away (if the requested resources are immediately available) or queues the job to run later (if the requested resurces are not currently available)."
  },
  {
    "objectID": "notebooks/08_job_script.html#job-scheduling-priority",
    "href": "notebooks/08_job_script.html#job-scheduling-priority",
    "title": "Running jobs on Gadi",
    "section": "Job scheduling priority",
    "text": "Job scheduling priority\nOn Artemis, you will have some familiarity with the concept of fair share use, where compute jobs you run increase your project’s ‘fair share weight’ which temporarily decreases the priority of your jobs in the queue. This is not the case on Gadi, where all jobs have equal priority. The only factors that limit how quickly your jobs leave the queue and start running are the resources you request combined with current resource availability. In order to have your job be queued (and not ‘held’ after submission), you must have sufficient KSU in your project. This will be described under queue charge rates."
  },
  {
    "objectID": "notebooks/08_job_script.html#pbs-directives",
    "href": "notebooks/08_job_script.html#pbs-directives",
    "title": "Running jobs on Gadi",
    "section": "PBS directives",
    "text": "PBS directives\nPBS directives outline your job’s resource needs and execution details. Each directive starts with #PBS in order to directly communicate with the job scheduler and not be confused with other code or comments in your script. The directives section should sit at the top of your script, with no blank lines between them, and any commands required to perform your compute task follow below the last directive.\nBelow is a simple example of the PBS directives portion of a Gadi job script. For details on more options, please see the NCI Gadi PBS directives guide.\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -q normal\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l jobfs=200GB\n#PBS -l walltime=02:00:00\n#PBS -l storage=scratch/aa00+gdata/aa00\n#PBS -l wd\n\n-P: Project code for resource accounting. Must be a valid NCI project code of which you are a member\n-q: Queue selection (e.g., normal or hugemem). See Gadi’s queue structure and queue limits pages for more details\n-l ncpus: Number of requested CPUs\n-l mem: amount of requested memory\n-l jobfs: Local-to-the-node disk space on the compute node\n-l walltime: Requested job walltime. Your job will only be charged for the walltime it uses, not the maximum walltime requested\n-l storage: Filesystems your job will access. /scratch/&lt;project&gt; is accessible by default. To access any other scratch or gdata locations, list them here. Note to use no spaces or leading / characters\n-l wd: Set the working directory to the submission directory. This is equivalent to cd $PBS_O_WORKDIR"
  },
  {
    "objectID": "notebooks/08_job_script.html#differences-between-artemis-and-gadi-pbs-scripts",
    "href": "notebooks/08_job_script.html#differences-between-artemis-and-gadi-pbs-scripts",
    "title": "Running jobs on Gadi",
    "section": "Differences between Artemis and Gadi PBS scripts",
    "text": "Differences between Artemis and Gadi PBS scripts\n\nThe -l storage directive is required on Gadi but not Artemis. Failure to include required storage locations will kill the job, for example with No such file or directory errors\nOn Gadi, users must review their resource requirements against the queue structure and limits in order to request a specific queue. On Artemis, the scheduler managed this automatically according to requested resources and queue loads\nMaximum walltime for any queue is 48 hours. For large numbers of nodes requested in a single job, the maximum walltime reduces. This is described in the queue limits page. See Working within walltime limit for more details\nThe requested resources are checked against the quantity of remaining KSU in the project specified at -P. If there is insufficient KSU to run the job, the job will be held. This will show as H status when the job is queried with qstat. See queue charge rates for more details\nJob arrays (eg #PBS J 1-1000) are not permitted on Gadi. See Parallel jobs and nci-parallel for more details\nUnlike Artemis, Gadi compute nodes lack internet access. If you have a job script that relies on an external network call such as reading from a live database, you will need to adapt your method (for example pre-downloading the required information with copyq before running the compute job) or use an alternate platform such as Nirin\n\nBelow is an example Artemis job script:\n#!/bin/bash\n\n#PBS -P &lt;USyd project code&gt;\n#PBS -N myjobname\n#PBS -l walltime=02:00:00\n#PBS -l select=1:ncpus=4:mem=16gb\n#PBS -q defaultQ\n\nmodule load python/3.12.2\n\ncd $PBS_O_WORKDIR\n\npython3 ./myscript.py ./myinput\nThe same job script, adjusted for Gadi:\n#!/bin/bash\n\n#PBS -P &lt;NCI project code&gt;\n#PBS -N myjobname\n#PBS -l walltime=02:00:00\n#PBS -l ncpus=4\n#PBS -l mem=16GB\n#PBS -q normal\n#PBS -l storage=scratch/bb11+gdata/aa00+gdata/bb11\n#PBS -l wd\n\nmodule load python3/3.12.1\n\npython3 ./myscript.py ./myinput\nAs you can see, there is very little difference between these two scripts. They both request 4 CPUs, 16 GB RAM, and 2 hours walltime. They both change the working directory to the submission directory, they both load python (different versions as available on the system) and both run the same job command.\nThe command to submit this script is also the same on Artemis and Gadi:\nqsub run_my_python_script.pbs\nAdapting your existing Artemis job scripts to Gadi should be fairly simple for most users, beginning with adjusting the directives and establishing required software. See Software for more details on Gadi software availability."
  },
  {
    "objectID": "notebooks/08_job_script.html#selecting-the-right-queue",
    "href": "notebooks/08_job_script.html#selecting-the-right-queue",
    "title": "Running jobs on Gadi",
    "section": "Selecting the right queue",
    "text": "Selecting the right queue\nArtemis defaultQ routed jobs to the appropriate queue based on directives and resource avalability. Gadi requires users to directly specify the appropriate queue.\nTo select the queue, you match up the resources your job needs to the queue limits, also factoring in the charge rate.\nView the available queues on the Gadi queue structure page. Note that there are:\n\ngeneral purpose queues\nlarge memory queues\nexpress queues\nGPU queues\ndata transfer queue (copyq)\n‘Cascade Lake’ and ‘Broadwell (ex-Raijin)’ queues\n\nThe Cascade Lake nodes are newer hardware and thus faster than the Broadwell nodes. Raijin was the previous NCI HPC, decomissioned in 2019\nThey have a lower charge rate than the equivalent Cascade Lake queue, and this can be utilised to help minimise compute cost when the reduced processor speed is not overly detrimental to the job or your research timeline\nThey have different numbers of CPU (48 or 28) and different total memory per node\n\n\nEach queue has different hardware, limits, and charge rates. Before submitting any jobs on Gadi, it is important to review this page along with the queue limits page which describes each queue in more detail.\nYou will note that each queue also has a corresponding queue that ends in -exec. You cannot submit directly to the -exec (execution) queue. Your jobs will be placed on the execution queue via the ‘route queue’ that you submit to. For example, for a job you want to run on the Cascade Lake normal queue, you will include the directive #PBS -q normal (submit to route queue) and the job will run on normal-exec (execution queue).\n\nQueue charge rates\nBy now you should be familiar with the concept of an NCI service unit (SU, or sometimes KSU for 1,000 SU or MSU for 1 million SU).\nEach new NCI project under the Sydney Scheme is granted 1 KSU by default, and requests can be made for more as required.\nA service unit is based on a CPU hour, ie ‘one hour of walltime on one CPU’. Each queue has a different charge rate applied to the CPU hour, so that one CPU hour on a given resource may cost between 1.25 SU and 6 SU, depending on the charge rate for that queue. More specialised and scarce resources are charged at a higher rate to ensure that only users who genuinely need these use them.\nThe charge rates can be found in column 4 of the queue limits table.\nIt’s important to understand that requested memory also impacts the charge rate, not just the requested CPU, walltime and queue. In each queue, a CPU has an allocated amount of memory for accounting purposes. For example, in the Cascade Lake normal queue, there are 48 CPU and 192 GB total RAM. The amount of memory per CPU for accounting purposes is therefore 192 / 48 = 4 GB. If you request 1 CPU and 4 GB RAM, only the CPU affects the charge rate, as you are using only the memory allocation for one CPU. If however you request 1 CPU and 8 GB RAM, your charge rate will be based off 2 CPU of use, since you are using the memory allowance of 2 CPU. Note this is ‘for accounting purposes’ only, ie it is technically feasible for your job to run on 1 CPU and access 8 GB RAM (or more). This accounting is described on the NCI job costs page, and will also be sumamrised below.\nDon’t be alarmed by the charge rates: please submit your job to the most appropriate queue based on required resources. The accounting method combined with stricter walltimes, newer hardware and software, and more vast physcial resources compared to Artemis will likely see your compute jobs complete in a faster turnaround time compared to what you are used to.\nUnderstanding charge rates is important for two main reasons:\n\nJudicious use of resources. KSU is provided to you in-kind by The University of Sydney. It is your responsibility to ensure efficient use of these resources. Selecting the appropriate queue for your job avoids wastage and avoids unecessary impacts on other users of this national resource.\nEnsuring your job can run. Jobs can only leave the route queue and join the execution queue if sufficient SU are available to the project assuming the job runs for its full requested walltime.\n\nFor example, if your project has 1 KSU and you submit a job script with the following directives:\n#PBS -q hugemem\n#PBS -l ncpus=48\n#PBS -l mem=1470GB\n#PBS -l walltime=12:00:00\nYour job will not join the compute queue - it will be held, showing a status of H when qstat is run. The reason for hold status is that the requested job requires more service units than the project has available.\nYou can view your project budget with the following command:\nnci_account -P &lt;nci-project-code&gt;\nThis will show the total allocated for the current quarter, the amount used, the amount reserved (by running or queued jobs), and the amount available. Any new job you submit MUST request less than the amount available.\nThe required SU available to run the job can be calculated by the formula:\nwalltime-hours * MAX (CPU|MEM) * charge-rate\nwhere MAX is based on the greater value of CPUs or proportion of node memory requested.\nso for the above example:\n12 h * 48 CPU * 3 charge rate = 1728 SU\nIn this case, MAX is based on CPU, since the per-CPU memory request (1470 GB / 48 CPU = 30.625 GB) is less than or equal to the maximum proportion of memory per CPU on the hugemem queue.\nSince 1728 SU is more than the 1 KSU the project has available, the job cannot run. You will need to either:\n\nObtain more KSU\nReconfigure your job to fit under the 1 KSU you have available.\n\nYou might consider reducing walltime, CPUs, change the queue, etc, depending on your job and what you expect are its minimum viable resouce requests. You can do this by:\nKilling the job (qdel &lt;jobID&gt;), editing the directives and resubmitting, OR use the qalter PBS command to reduce the resource requests of the held job.\nFor the above example, let’s assume the requested walltime of 12 hours was an extremely conservative estimate and realistically you expect the job should complete in less than 2 hours. You could run this command:\nqalter -l walltime=02:00:00 &lt;jobid&gt; \nThis would reduce the SU for the submitted job to 288 SU and the job would then be picked up by the next scheduling cycle and enter the queue.\n\n\nQueue selection examples\n\nExample 1\nYou have a small job that only uses a single CPU and 2 GB RAM, but will run for a whole day. Which of the queues would be appropriate?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nnormal, normalbw, express, expressbw. While you could use the express queues, the charge rate is higher so the non-express normal queues would be more economical.\n\n\n\n\n\nExample 2\nYou have a job that requires 384 GB memory and 12 CPU. Which queue would you use?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nhugemem, with 12 CPU and 384 GB memory, or hugemembw, with 14 CPU as CPUs must be requested in multiples of 7 on this queue.\nWhich would be the better choice?\nIf the job ran for 2 hours, charge rate would be:\n\nhugemem: 12 CPU * 2 h * 3 charge rate = 72 SU\nhugemembw: 14 CPU * 2 h * 1.25 charge rate = 35 SU\n\nHugemem may execute faster with the newer hardware, yet hugemembw may consume less KSU. hugemembw also has more mem per CPU than hugemem (36 GB vs 32 GB). Benchmarking will demonstrate which of these configurations is more suited to your job.\nSee job efficiency for tips to determine the best compute resources for your job.\n\n\n\n\n\nExample 3\nYou have a job that requires 20,000 CPU. Fill in the below directives for this job, including the maximum permissable walltime:\n#PBS -l ncpus=&lt;value&gt;\n#PBS -l mem=&lt;value&gt;\n#PBS -l walltime=&lt;value&gt;\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n#PBS -l ncpus=20016\n#PBS -l mem=3803040GB\n#PBS -l walltime=05:00:00\nWhy 20,016? When requesting &gt;1 node on Gadi, only whole nodes can be requested. So to reach 20,000 CPU in a single job would require the use of the Cascade Lake normal queue, where the nodes have 48 CPU per node, and this would be 20,000 * 48 = 416.7 nodes, so we need to round up to 417 nodes, which is 417 * 48 = 20,016 CPU.\nWhy 5 hour walltime not 48 hours? As the quantity of CPU requested increases, maximum walltime goes down. This information can be found in the last column on the queue limts table. 5 hours is the maximum amount of walltime allowed for jobs requesting more than 3024 CPUs (63 nodes) in this queue. To request the maximum walltime of 48 hours on this queue, the job must request at most 672 cores (14 nodes).\nIf your job required exactly 20,000 CPU, you would simply provide this hard-coded value to the relevant command. The number of requested KSU to the job can be accessed from the environment variable $PBS_NCPUS.\n\n\n\n\n\nExample 4\nYour job requires GPUs. Which queues could you use?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ngpuvolta or dgxa100 queue"
  },
  {
    "objectID": "notebooks/08_job_script.html#lack-of-internet-access-on-compute-nodes",
    "href": "notebooks/08_job_script.html#lack-of-internet-access-on-compute-nodes",
    "title": "Running jobs on Gadi",
    "section": "Lack of internet access on compute nodes",
    "text": "Lack of internet access on compute nodes\nThe only Gadi queue with internet access is copyq. This queue is not suitable for running compute tasks. It allows only single-core jobs and has a maximum walltime of 10 hours. Jobs that require up-to-date information retrieval from external servers have a few options:\n\nSplit the job into two parts: a download or web query task submitted to copyq, ensuring that the retrieved data is saved to persistent disk (ie not the local-to-the-node SSD storage that is deleted upon job completion), followed by a a compute job submitted to one of the appropriate compute queues, reading in the requried inputs saved from job 1.\nRun the job via ARE, which provides a graphical user interface run on Gadi’s compute queues plus internet access capability.\nUse NCI’s Nirin cloud instead of Gadi."
  },
  {
    "objectID": "notebooks/08_job_script.html#submitting-a-pbs-script",
    "href": "notebooks/08_job_script.html#submitting-a-pbs-script",
    "title": "Running jobs on Gadi",
    "section": "Submitting a PBS script",
    "text": "Submitting a PBS script\nLike on Artemis, the qsub command is used to submit the job to the scheduler. Please visit the NCI Gadi job submisison page if you require more details on this.\nAfter your job is submitted, job monitoring and job logs are very similar to your experience on Artemis. Please see job monitoring for more details."
  },
  {
    "objectID": "notebooks/08_job_script.html#interactive-jobs",
    "href": "notebooks/08_job_script.html#interactive-jobs",
    "title": "Running jobs on Gadi",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for jobs that require user input feedback as an analysis progresses, or can be useful for testing commands/tools prior to submiting a full job via a PBS script.\nRunning an interactive job on Gadi is very similar to an Artemis interactive job: you provide the relevant directives on the command line rather than from within a script, and include -I instead of #PBS -q &lt;queue&gt;.\nFor example, to start an interactive job with 4 CPU for 1 hour, enter the following command on the Gadi login node:\nqsub -I -P &lt;nci-project-code&gt; -l walltime=00:01:00,ncpus=4,mem=16GB,storage=&lt;required-storage-paths&gt;,wd\nAfter you enter the command, you will receive a message\nqsub: waiting for job &lt;id&gt;.gadi-pbs to start\nOnce your interactive job has left the queue and started, you will receive a message\nqsub: job &lt;id&gt;.gadi-pbs ready\nNotice that your command prompt has changed, indicating the node ID you are on instead of the login node ID.\nYou can then interactively enter the commands required for your compute task. To terminate the interactive job, enter exit."
  },
  {
    "objectID": "notebooks/08_job_script.html#persistent-sessions",
    "href": "notebooks/08_job_script.html#persistent-sessions",
    "title": "Running jobs on Gadi",
    "section": "Persistent sessions",
    "text": "Persistent sessions\nTo support the use of long-running, low CPU and low memory demand processes, NCI provides a persistent sessions service on Gadi. This service is primarily designed for the use of workflow management tools (eg nextflow) that automatically submit and monitor PBS jobs to the Gadi compute queues.\nWorkflow management tools are a unique use case where the ‘head job’ requires internet access (provided through the persistent session) and access to the scheduler to submit a series of chained compute jobs to various queues depending on the unique workflow configuration.\nThis service is NOT designed for computational work, large downloads, or other intensive tasks. These jobs should be submitted to the appropriate PBS queues."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html",
    "href": "notebooks/09_job_monitoring.html",
    "title": "Monitoring your job",
    "section": "",
    "text": "You should monitor your jobs to keep track of their progress but refrain from checking your jobs too frequently. Repeated queries will be considered attacks, especially in quick succession and you may get a warning from NCI. NCI recommends querying your jobs’ status a maximum of once every 10 minutes.\nLike Artemis, you can use the qstat command to monitor jobs on Gadi. The NCI Gadi job monitoring page describes some commonly used flags to qstat."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-monitoring",
    "href": "notebooks/09_job_monitoring.html#job-monitoring",
    "title": "Monitoring your job",
    "section": "",
    "text": "You should monitor your jobs to keep track of their progress but refrain from checking your jobs too frequently. Repeated queries will be considered attacks, especially in quick succession and you may get a warning from NCI. NCI recommends querying your jobs’ status a maximum of once every 10 minutes.\nLike Artemis, you can use the qstat command to monitor jobs on Gadi. The NCI Gadi job monitoring page describes some commonly used flags to qstat."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-ids",
    "href": "notebooks/09_job_monitoring.html#job-ids",
    "title": "Monitoring your job",
    "section": "Job IDs",
    "text": "Job IDs\nLike Artemis jobs, jobs submitted to Gadi are given a jobID. This is shown to you as soon as it has been accepted, for example 135615373.gadi-pbs.\nWhen querying the job with qstat, you can use the full ID, or just the string of numbers (omit the .gadi.pbs).\nFor example, the below two commands are equivalent:\nqstat -xf 135615373.gadi-pbs\nqstat -xf 135615373\nIf you have multiple jobs running, you do not need to check them individually with the job ID. You can check the status of multiple jobs using your NCI user ID:\nqstat -u &lt;nci-user-id&gt;"
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#monitoring-resource-usage-in-real-time",
    "href": "notebooks/09_job_monitoring.html#monitoring-resource-usage-in-real-time",
    "title": "Monitoring your job",
    "section": "Monitoring resource usage in real time",
    "text": "Monitoring resource usage in real time\nTo view the CPU and memory utilisation of a running job, you can use the bespoke nqstat_anu utility (ANU = The Australian National University, where Gadi is housed).\nBy default, running the command nqstat_anu will report on all jobs for the current user under their default project, providing the following details:\n\n% CPU utilisation\nwalltime used\nwalltime requested\nRSS\nmaximum memory used by the job\nmemory requested\nCPUs requested\nwhether the job is queued or suspended\n\nRun nqstat_anu -h for more options.\n\n\n\n\n\n\nSwitching default project\n\n\n\nTo change your default project, run switchproj &lt;nci-project-id&gt;, providing the ID of the project you want to switch to. If you don’t know what your default project is, run the command nci_account with no arguments. Since -P &lt;nci-project-id&gt; is not supplied, it will report on your default project."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-logs",
    "href": "notebooks/09_job_monitoring.html#job-logs",
    "title": "Monitoring your job",
    "section": "Job logs",
    "text": "Job logs\nBy default, the PBS job logs will be created in the directory from which the qsub command was entered, and combine the job name and the job ID.\nFor example, a job with #PBS -N convert and job ID 133703660 will have standard output and resource usage written to convert.o133703660 and standard error written to convert.e133703660.\nThis differs slightly to Artemis, which has the same default filepath behaviour except the standard output is sent to the .o and the resource usage is sent to .o_usage.\nThe Gadi .o file has the resource usage at the end of the log making it easy to view a quick summary with the tail command:\n$ tail -n 11 convert.o133703660 \n                  Resource Usage on 2025-02-05 16:16:10:\n   Job Id:             133703660.gadi-pbs\n   Project:            aa00\n   Exit Status:        0\n   Service Units:      23.94\n   NCPUs Requested:    7                      NCPUs Used: 7               \n                                           CPU Time Used: 02:57:33        \n   Memory Requested:   63.0GB                Memory Used: 51.14GB         \n   Walltime requested: 08:00:00            Walltime Used: 02:44:09        \n   JobFS requested:    100.0MB                JobFS used: 0B              \n======================================================================================\nIf desired, you can change the default log filepaths with the -o and -e directives, for example:\n#PBS -o ./logs/convert-fast5.o\n#PBS -e ./logs/convert-fast5.e\nThis omits the job ID from being included in the log file name and sends the logs to a different directory."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#monitoring-job-disk-usage",
    "href": "notebooks/09_job_monitoring.html#monitoring-job-disk-usage",
    "title": "Monitoring your job",
    "section": "Monitoring job disk usage",
    "text": "Monitoring job disk usage\nEach project has an allocation of physical disk space and inodes on scratch, gdata and home. When running jobs on Gadi, it’s important to ensure you have sufficient allocation remaining to meet the needs of the job you are running, as exceeding the limit of these resources can cause a running job to fail. Please see the section on disk and inode usage for more details and how to query usage of these resources."
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html",
    "href": "notebooks/13_parallel_jobs.html",
    "title": "Running parallel jobs on Gadi",
    "section": "",
    "text": "TO BE COMPLETED"
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#introduction",
    "href": "notebooks/13_parallel_jobs.html#introduction",
    "title": "Running parallel jobs on Gadi",
    "section": "Introduction",
    "text": "Introduction\nIn this section, we will discuss how you can run parallel jobs on Gadi using the nci-parallel utility in place of the Artemis job array method.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nJob arrays not supported on Gadi\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/13_parallel_jobs.html#section",
    "href": "notebooks/13_parallel_jobs.html#section",
    "title": "Running parallel jobs on Gadi",
    "section": "",
    "text": "https://opus.nci.org.au/spaces/Help/pages/248840680/Nci-parallel…\nhttps://sydney-informatics-hub.github.io/training.gadi.intro/08-Example-parallel-job/index.html"
  },
  {
    "objectID": "notebooks/01_setup.html",
    "href": "notebooks/01_setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "Before accessing Gadi, you will need to have an NCI account. Ensure you have completed this step by following directions on the Gadi Access instructions on the previous page before proceeding.\nTo work on NCI Gadi, you will need to use a terminal application on your local computer or work on NCI’s Australian Research Environment (ARE) platform, which includes a web-based terminal interface to Gadi. If you already have a terminal application that you have used to access Artemis, for example putty or Mac term, you can continue to use that.\nBelow we will describe 3 options for accessing Gadi:"
  },
  {
    "objectID": "notebooks/01_setup.html#use-ncis-are-platform",
    "href": "notebooks/01_setup.html#use-ncis-are-platform",
    "title": "Set up your computer",
    "section": "Use NCI’s ARE platform",
    "text": "Use NCI’s ARE platform\n\n\n\n\n\n\nARE: fast access with limited customisation\n\n\n\nThis is a very lightweight solution for accessing Gadi, some interactive tools like Jupyter and RStudio. We recommended it for beginners who don’t want to customise their set up.\n\n\nNCI has created a web-based graphical interface for accessing their systems. It is very simple to use and recommended over access methods described below for beginners.\nSee NCI’s User Guide for instructions on how to access and use ARE."
  },
  {
    "objectID": "notebooks/01_setup.html#install-visual-studio-code",
    "href": "notebooks/01_setup.html#install-visual-studio-code",
    "title": "Set up your computer",
    "section": "Install Visual Studio Code",
    "text": "Install Visual Studio Code\n\n\n\n\n\n\nVScode: customised configuration with an integrated terminal\n\n\n\nThis is a more advanced solution for accessing Gadi, with more customisation options. We recommended it for users who are comfortable with terminal applications and want to customise their set up.\n\n\nVisual Studio Code (VS Code) is a lightweight and powerful source code editor available for Windows, macOS and Linux computers. As an alternative to a terminal application it offers additional functionality including file editing.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\nConnect to your instance with VS code by adding the host details to your .ssh config file:\nHost Gadi\n  HostName gadi.nci.org.au\n  User &lt;your-nci-username&gt;\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host and Gadi\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your NCI password and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you’re running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /scratch/iz89 to open your workspace. You can change this at any point by opening a new folder. Keep in mind you will be requested to provide your password each time.\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‘home’ then click Yes, I trust the authors\nTo open a terminal, type Ctrl+J if you’re on a Windows machine or Cmd+J on MacOS\n\nTips for using VS Code\n\nPeriodically delete your ~/.vscode-server/ directory on Gadi! VSCode will fill this directory with numerous files and cause your home quota to be exceeded\nVS code cheatsheet for Windows\nVS code cheatsheet for MacOS"
  },
  {
    "objectID": "notebooks/01_setup.html#use-macos-native-terminal",
    "href": "notebooks/01_setup.html#use-macos-native-terminal",
    "title": "Set up your computer",
    "section": "Use MacOS native terminal",
    "text": "Use MacOS native terminal\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‘terminal’. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2."
  },
  {
    "objectID": "notebooks/01_setup.html#install-a-terminal-client-on-your-windows-machine",
    "href": "notebooks/01_setup.html#install-a-terminal-client-on-your-windows-machine",
    "title": "Set up your computer",
    "section": "Install a terminal client on your windows machine",
    "text": "Install a terminal client on your windows machine\nWindows OS now comes with Windows Subsystem for Linux (WSL) so if you are familiar with using that, you can ssh to Gadi from a WSL terminal.\nIf not, you will need to install a terminal client. There are many options, including putty, Xwin-32 (which the University of Sydney has a license for), and MobaXterm.\nBelow we will describe the process to install the free version of MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‘Home Edition’ select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‘start local terminal’ (and install Cygwin if prompted)\n\n\nTo log in to Gadi, you will use a Secure Shell (SSH) connection. To connect, you need 3 things:\n\nThe address of your NCI Gadi, gadi.nci.org.au.\nYour Gadi username, e.g. ab1234.\nYour password.\n\nTo log in: type the following into your terminal, using your allocated instance’s IP address:\nssh &lt;username&gt;@gadi.nci.org.au\nThen provide your password when prompted.\n\n\n\n\n\n\n‼️ Pay Attention ‼️\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You’ll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nOnce you’ve logged in successfully, you should see a welcome screen like this:"
  },
  {
    "objectID": "notebooks/04_command_line.html",
    "href": "notebooks/04_command_line.html",
    "title": "Command line environment",
    "section": "",
    "text": "Working on Gadi will require you to have reasonable confidence on the Linux/Unix command line."
  },
  {
    "objectID": "notebooks/04_command_line.html#linux-command-line-training",
    "href": "notebooks/04_command_line.html#linux-command-line-training",
    "title": "Command line environment",
    "section": "Linux command line training",
    "text": "Linux command line training\nNCI host a number of training sessons, including introductory Linux. Please visit their training calendar.\nThere are numerous self-directed training tutorials online, such as this one from Sandbox.bio.\nA bash cheatsheet such as this one from NCI or this one can also be helpful in increasing your familiarity with the command line.\nDue to the plethora of high-quality introductory command line materials available online, we will not reproduce those here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCI for USyd researchers",
    "section": "",
    "text": "National Computational Infrastructure (NCI) is a services facility that provides high performance computing (HPC), cloud computing, and data services to Australian researchers.\nOur insitutional HPC ‘Artemis’ will be de-comissioned in August 2025. NCI has been chosen as the replacement computing platform. All staff and students with a valid University of Sydney unikey can access NCI computing time under the NCI-Sydney Scheme.\nIf HPC is not suitable for your workload, please consider NCI cloud or virtual research desktops. We are confident that your research computing will be well-supported by these platforms.\nThis site is focused on the use of NCI’s Gadi HPC, in the context of University of Sydney researchers.\nPlease be mindful this is not an exhaustive resource for using Gadi. It is only intended to orient you to the system and navigate the comprehensive NCI user documentation."
  },
  {
    "objectID": "index.html#what-is-high-performance-computing",
    "href": "index.html#what-is-high-performance-computing",
    "title": "NCI for USyd researchers",
    "section": "What is high performance computing?",
    "text": "What is high performance computing?\nHigh performance computing refers to the use of parallel processing techniques to solve complex computation problems efficiently. HPC systems, like Gadi, consist of clusters of interconnected computers, each equipped with multiple processors and large amounts of memory. These systems are capable of handling massive datasets and perform computations at speeds far beyond those achievable by your personal computer."
  },
  {
    "objectID": "index.html#why-do-we-need-hpc-for-research-computing",
    "href": "index.html#why-do-we-need-hpc-for-research-computing",
    "title": "NCI for USyd researchers",
    "section": "Why do we need HPC for research computing?",
    "text": "Why do we need HPC for research computing?\nResearch computing comes in all shapes and sizes. In some cases, your compute needs are well-met by your personal computer or a web-based platform. In other cases, these platforms are not sufficient and this is where HPC is critical to ensuring a timely analysis of your data.\nTo use HPC, it is not a requirement that your workflow makes use of the multi-node architecture. There are many reasons why HPC would be justified:\n\nLarge input data requiring vast physical storage for inputs and outputs\nHigh CPU or node requirement\nGPU requirement\nHigh memory requirement\nLong walltime requirement\nFaster I/O operations than your local computer can handle\nFreeing up your local computer resources for other tasks, or simply to shut down for the day without affecting the compute analysis you are running\n\nHPC provides a reliable and efficient means of analysing data of all shapes and sizes from all research domains."
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "NCI for USyd researchers",
    "section": "Support",
    "text": "Support\nSIH is limited in the support it can provide for NCI Gadi users. If you are new to HPC and Gadi, we expect you will attend NCI’s Intro to Gadi courses. Additionally, familiarise yourself with Gadi using the NCI Gadi user guide.\nLive and self-directed training is also offered by SIH and NCI on HPC and data analysis topics:\n\nSIH training calendar\nNCI training calendar\n\nFor additional support, please contact the following people depending on your needs:\n\n\n\nType of issue\nWho\nHow\nDetails\n\n\n\n\nService unit allocation for running jobs. Request for g/data storage.\nSIH\nMake a request\nSydney Documentation\n\n\nA techincal issue with NCI. An error in your running job. New software installation.\nNCI Helpdesk\nLog an NCI ticket\nProvide your error, log file, jobid. Versions or links for installs.\n\n\nA bug in an SIH pipeline\nSIH\nSubmit an issue on Github\nProvide details e.g. this issue\n\n\nGeneral research computing or bioinformatics advice\nSIH\nLog an SIH ticket\nProvide relevant context, errors, tool names, scripts"
  }
]