[
  {
    "objectID": "tips_tricks.html",
    "href": "tips_tricks.html",
    "title": "Tips and tricks",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/05_data_transfer.html",
    "href": "notebooks/05_data_transfer.html",
    "title": "Transferring data to and from Gadi",
    "section": "",
    "text": "In this section, we will look at transferring data between the Research Data Store and Gadi.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at Data Transfer. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decommission date.\n\n\n\n\n\nMethod\nSuitable data size/type\nPros/Cons\n\n\n\n\nGUI based data transfer client e.g. filezilla or cyberduck\nOnly small files, config scripts etc.\n\nEasy to use\nHost computer can limit transfer speed\n\n\n\nsftp transfer from Gadi terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nUses login node for computation\nCannot restart after interruption\n\n\n\nrsync transfer from Artemis terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nCan restart after interruption\nUses login node for computation\nArtemis only around until August\n\n\n\nsftp copy using copyq\nLarge files and datasets to copy all in one go.\n\nRobust, runs in background\nFaster speed than lftp\nAlways copies all files\n\n\n\nlftp sync using copyq\nLarge datasets with many files and only a few updated with each transfer.\n\nRobust, runs in background\nOnly copies whats changed\nSlower speed than sftp\n\n\n\nArtemis rsync using dtq\nAny large dataset.\n\nRobust, runs in background\nOnly copies whats changed\nArtemis only around until August\n\n\n\nGlobus\nAny file/dataset size.\n\nShould do everything\n\nGUI\nTerminal copy and sync\ncopyq copy and sync\n\nUnknown time-frame for availability\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/05_data_transfer.html#introduction",
    "href": "notebooks/05_data_transfer.html#introduction",
    "title": "Transferring data to and from Gadi",
    "section": "",
    "text": "In this section, we will look at transferring data between the Research Data Store and Gadi.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at Data Transfer. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decommission date.\n\n\n\n\n\nMethod\nSuitable data size/type\nPros/Cons\n\n\n\n\nGUI based data transfer client e.g. filezilla or cyberduck\nOnly small files, config scripts etc.\n\nEasy to use\nHost computer can limit transfer speed\n\n\n\nsftp transfer from Gadi terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nUses login node for computation\nCannot restart after interruption\n\n\n\nrsync transfer from Artemis terminal\nFiles up to a few GB.\n\nConvenient for quickly copying data\nCan restart after interruption\nUses login node for computation\nArtemis only around until August\n\n\n\nsftp copy using copyq\nLarge files and datasets to copy all in one go.\n\nRobust, runs in background\nFaster speed than lftp\nAlways copies all files\n\n\n\nlftp sync using copyq\nLarge datasets with many files and only a few updated with each transfer.\n\nRobust, runs in background\nOnly copies whats changed\nSlower speed than sftp\n\n\n\nArtemis rsync using dtq\nAny large dataset.\n\nRobust, runs in background\nOnly copies whats changed\nArtemis only around until August\n\n\n\nGlobus\nAny file/dataset size.\n\nShould do everything\n\nGUI\nTerminal copy and sync\ncopyq copy and sync\n\nUnknown time-frame for availability"
  },
  {
    "objectID": "notebooks/05_data_transfer.html#where-to-put-project-files-on-gadi",
    "href": "notebooks/05_data_transfer.html#where-to-put-project-files-on-gadi",
    "title": "Transferring data to and from Gadi",
    "section": "Where to put project files on Gadi",
    "text": "Where to put project files on Gadi\nOn Gadi (like on Artemis) you have access to a number of different storage areas for project files and data. Check the NCI User Guides for more detailed information. Here we provide a brief introduction for those familiar with Artemis.\nEach space is intended for use in a specific way:\n\n/home\nYour home space (/home/&lt;nci_user_id&gt;) is owned by you and has 10 GiB of available space. This cannot be increased.\nIt works similarly to your /home space on Artemis and should only be used to store things like program code, batch scripts or software configuration information. Note that as space is extremely limited here it is unadvisable to use this space for storing project data.\n\n\n/scratch\nYour scratch space (/scratch/&lt;project&gt;) is owned by your project and has 1 TiB of available space which can be increased upon request to NCI. It is roughly equivalent to /scratch on Artemis.\nData are not backed up and files not accessed for 100 days will be quarantined for 14 days and then removed (See here for instructions for removing files from quarantine).\nScratch should be used for temporary files associated with a job that has large data IO and not for longer term storage. Always ensure any data you need from a job that is left in /scratch is promptly backed up to the Research Data Store.\n\n\n/g/data\nYour /g/data space (/g/data/&lt;project&gt;) is owned by your project and has the available space allocated by the Sydney Scheme Manager.\nThe files on /g/data are not backed up but they will persist there for the lifetime of your project.\n/g/data is intended to be used to store longer term files that are regularly used by your project during its lifetime. Always ensure your data is regularly backed up from here to the Research Data Store.\n/g/data may be accessed directly from PBS job scripts by using the -lstorage PBS directive.\nTo check the amount of disk space you have available in the data areas listed above you can type the command lquota at the Gadi prompt."
  },
  {
    "objectID": "notebooks/05_data_transfer.html#research-data-store-rds",
    "href": "notebooks/05_data_transfer.html#research-data-store-rds",
    "title": "Transferring data to and from Gadi",
    "section": "Research data store (RDS)",
    "text": "Research data store (RDS)\nThe RDS is NOT being decommissioned along with Artemis HPC. Any RDS projects you currently have will persist on RDS. It is your responsibility to backup any data on Artemis filesystems (/home, /scratch, /project) that you wish to keep prior to the decommission date of August 29 2025. For information on how to go about this see the SIH Artemis Training Series.\nIn this section, we will mainly focus on how to transfer data between Gadi HPC and RDS. You should be able follow similar methods for copying data between your own laptop/server and Gadi."
  },
  {
    "objectID": "notebooks/05_data_transfer.html#data-transfer-options",
    "href": "notebooks/05_data_transfer.html#data-transfer-options",
    "title": "Transferring data to and from Gadi",
    "section": "Data transfer options",
    "text": "Data transfer options\nDepending on the size and complexity of the data you are transferring you have multiple options available:\n\nFor small transfers (&lt;1GB) you can use a GUI based data transfer client such as filezilla or cyberduck.\nFor mid sized transfers up to tens of GB you can use terminal based transfer.\nFor large transfers you should use the data transfer queue options on either Gadi (copyq) or Artemis (dtq).\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data transfer to and from Gadi should be made using the “Data Mover Node” at gadi-dm.nci.org.au where possible rather than the login nodes. This ensures that data transfer will not consume otherwise limited resources on the login nodes.\n\n\n\nGlobus - COMING SOON\nIn the coming months, Globus will be available for simplified and efficient data transfer. We will provide training and materials on this once available.\nIn the meantime, the below options are available, and detailed examples for each method are provided in the subsequent sections.\n\n\nTransfer using RDS mapped network drive and data transfer client\nFor smaller files or datasets, for example a set of scripts that you are going to run, you can map your RDS project as a network drive and transfer the data to Gadi via an intermediate data transfer client GUI such as filezilla or cyberduck.\nWhile simple to use, these are not recommended for large data transfers, as the local computer becomes a bottleneck and they are generally not resumable after interruption. Faster speeds will be obtained if you are on campus, but still this method may be prohibitively slow for larger datasets.\n\n\n\n\n\n\nExample using cyberduck\n\n\n\nThe following are instructions using the cyberduck data transfer client. The process using filezilla is similar with the same username and server IP address as shown in this example.\nTo mount your RDS drive in either Windows or MacOS, please follow the instructions described here. You should have a File Explorer (Windows) or Finder (MacOS) window open and displaying the files and folders in your RDS project directory.\nNext download cyberduck from https://cyberduck.io and open it and connect to Gadi:\n\nClick on the Open Connection icon at the top of the window.\nSelect SFTP (SSH File Transfer Protocol) from the drop-down menu at the top of box.\nIn the Server field, enter gadi-dm.nci.org.au.\nIn the Username field, enter your NCI Username.\nIn the Password field, enter your NCI password.\nClick Connect.\nIf an Unknown fingerprint box appears, click the Always check box in the lower-left hand corner, then click Allow.\n\nIf you have successfully logged in, you will see a directory listing of /home/&lt;user_login&gt;. You can browse to your project folder either in /scratch or /g/data by pressing Ctrl + g and then typing /scratch/&lt;project_id&gt; or /g/data/&lt;project_id&gt;.\nYou can then transfer data to and from RDS and NCI by dragging and dropping files between your computer’s file explorer and the Cyberduck window.\n\n\n\n\nTransfer from Gadi/Artemis terminal to/from RDS\nYou can use commands in your terminal application (Mac and linux: Terminal, Windows: Windows Terminal or Powershell equivalent) as an alternative to graphical applications.\nSince the connection will be terminated if your computer sleeps, terminal crashes, network drops out etc, this method is not particularly robust for large transfers. For these rather use the queue based methods (either copyq on Gadi or dtq on Artemis) described below.\n\nTransfers from a terminal on Gadi\nDue to stringent security settings around Artemis and RDS, familiar commands like rsync or scp cannot be initiated from NCI Gadi login nodes. Instead you have to use commands like sftp to copy the data.\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that running these commands on the login nodes is not the recommended way to transfer research data to Gadi! For smaller downloads, this is OK, but for normal purposes the use of Gadi’s copyq and data mover nodes are the appropriate tools.\n\n\n\n\n\n\n\n\nHow to transfer data from RDS to Gadi (and vice-versa) from a Gadi login.\n\n\n\nTo transfer data between RDS and Gadi from the Gadi login shell:\n\nOpen a terminal (using the ‘Terminal’ app on MacOS or the ‘Command Prompt’ app on Windows and then log into Gadi using ssh:\n\nssh &lt;nci_user_id&gt;@gadi.nci.org.au\nYou may be prompted to enter your NCI password at this point.\n\nGet the data from RDS, to a specific location on Gadi, e.g:\n\nsftp -r &lt;your_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder&gt; &lt;location on Gadi&gt;\nFor example if I wanted to copy data from my Training project on RDS in the folder MyData to Gadi in the scratch space for my NCI project named qc03:\nsftp -r &lt;my_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-Training/MyData /scratch/qc03/MyData\nreplace /rds in the above with /project or /scratch for your preferred source folder or file.\nIf you want to copy the other way around (ie. from Gadi /scratch to RDS) use\nsftp &lt;your_unikey&gt;@research-data-ext.sydney.edu.au:/rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder&gt; &lt;&lt;&lt; $\"put -r &lt;location on Gadi&gt;\"\n\n\n\n\nTransfers from a terminal on Artemis using rsync\n\n\n\n\n\n\nWarning\n\n\n\nThis option is only available prior to the decommission of Artemis on 29 August 2025 - after that date you will have to use either GLOBUS (preferred), or copy data when logged into the Gadi terminal (either at the login shell or using copyq scripts).\n\n\nWhen logged into Artemis you can use the rsync or scp command to copy data directly to/from Gadi, since Gadi allows the kind of secure connection that these commands require.\nUsing rsync will allow you to sync data between RDS and Gadi, this means that only files that have been updated since the last transfer will be copied. This will allow small changes to large datasets to be transferred quickly.\n\n\n\n\n\n\nTransfer from direct connection to RDS\n\n\n\nYou can also initiate the transfer in a terminal from a connection via ssh to research-data-int.sydney.edu.au (on campus or USyd VPN) and using the rsync method described here - just replace hpc.sydney.edu.au with research-data-int.sydney.edu.au in step 1.\n\n\n\n\n\n\n\n\nHow to transfer from RDS to Gadi (and vice-versa) at an Artemis terminal using rsync\n\n\n\nTo transfer data between RDS and Gadi from the Artemis login shell:\n\nOpen a terminal (using the ‘Terminal’ app on MacOS or the ‘Command Prompt’ app on Windows and then log into Artemis using ssh:\n\nssh &lt;your_unikey&gt;@hpc.sydney.edu.au\nYou may be prompted to enter your password at this stage.\n\nCopy the data from rds, to a specific location on Gadi, e.g:\n\nrsync -rtlPvz /rds/PRJ-&lt;Project Short ID&gt;/&lt;Path to File or Folder in RDS&gt; &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;Destination on Gadi&gt;\nFor example if I wanted to sync data from my Training project on RDS in the folder MyData to Gadi in the scratch space for my NCI project named qc03:\nrsync -rtlPvz /rds/PRJ-Training/MyData &lt;nci_user_id&gt;@gadi-dm.nci.org.au:/scratch/qc03/MyData\n\nYou will be prompted for the password associated with your username on Gadi and the transfer will commence.\n\nIf you want to copy the other way around (ie. from Gadi /scratch to RDS) then simply reverse the order of the above command in step 2, e.g:\nrsync -rtlPvz &lt;nci_user_id&gt;@gadi-dm.nci.org.au:&lt;Source on Gadi&gt; /rds/PRJ-&lt;Project Short ID&gt;/&lt;Destination on RDS&gt;\n\n\n\n\nUsing tmux to run your job in the background\n\n\n\n\n\n\nWarning\n\n\n\nWhile this method can help you run your copy job in the background over a long period of time, it is recommended to rather use the copyq transfer method described below for large file transfers, as running jobs on the login node can overwhelm their scant resources.\n\n\nYou can run all of the above copy commands in a tmux session which can be detached to the background allowing you to log out of Gadi and switch off your computer while the copy still runs in the background.\nTo do this, after logging in (step 1. above) enter the command tmux at the prompt. This will send you into a tmux terminal session, then inside the tmux session enter the copy command (step 2. above) and the copy will start. Then while the copy command is running, enter &lt;ctrl&gt;-b then d to detach the tmux session and return to the login prompt. You can now log out of Gadi while your copy job is running in the background on Gadi. To check its status simply log back into Gadi in a terminal (step 1. above) and enter the command tmux attach. This will re-attach your running tmux session and you can investigate its output to check if its done. When things are finished you can exit the running tmux session by typing exit inside it.\nFor more info about tmux check here.\n\n\n\nTransfer using sftp or lftp from Gadi copyq\nThe data transfer queue on Gadi is called copyq. This is comparable to the data transfer queue on Artemis dtq. Data transfer methods/scripts that you used to put data onto Artemis for example from the web via wget or from another server should be easily portable to use on Gadi’s copyq.\nPlease note that the compute nodes on Gadi do not have internet access like the Artemis compute nodes do, so all required data must first be downloaded before submitting a compute job that requires the data.\nDue to stringent security settings around Artemis and RDS, commands like rsync or scp cannot be initiated from NCI Gadi login nodes or copyq. To initiate the transfer from Gadi, sftp or lftp must be used. In the not too distant future Globus will become available for data transfer and then that will be the preferred method for transferring data to and from Gadi.\n\nHow to set up SSH keys for passwordless data transfer\nIf you are transferring data directly for example scp on the command line or via a transfer client on your local computer, entering a password to initiate the transfer is straightforward. If however you want to transfer via a job submitted to either copyq or dtq, you will need to set up SSH keys first, or else your script will halt while it waits fro a password to be entered.\nYou only need to set this up once.\nSSH key pairs are used for secure communication between two systems. The pair consists of a private key and a public key. The private key should remain private and only be known by the user. It is stored securely on the user’s computer. The public key can be shared with any system the user wants to connect to. It is added to the remote system’s authorized keys. When a connection is attempted, the remote system uses the public key to create a message for the user’s system.\nThere are many general guides for this online, for example this one. For step-by-step instructions on how to set up keys between Gadi and RDS, expand the drop down below.\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\nFollow the below steps carefully to set up SSH keys between RDS and Gadi. Note, you only need to do this once.\n\nLog into Gadi with your chosen method, e.g:\n\nssh ab1234@gadi.nci.org.au\n\nMove to your home directory:\n\ncd ~\n\nMake a .ssh directory, if you don’t already have one:\n\nmkdir -p .ssh \n\nSet suitable permissions for the .ssh directory and move into it:\n\nchmod 700 .ssh\ncd .ssh\n\nGenerate SSH key pair:\n\nssh-keygen\nHit enter when prompted, saving the key in ~/.ssh/id_rsa and enter for NO passphrase. A public key will be located in ~/.ssh/id_rsa.pub and a private key in ~/.ssh/id_rsa.\n\nSet suitable permissions for the keys:\n\nchmod 600 id_rsa\nchmod 644 id_rsa.pub\n\nMake an authorized_keys file if you don’t already have one that can be transferred to USyd’s Artemis/RDS system:\n\ntouch ~/.ssh/authorized_keys\n\nCopy the contents of the public key file (~/.ssh/id_rsa.pub) to the authorized_keys file to be transferred to USyd’s Artemis/RDS system:\n\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n\nSet permissions for the authorized_keys file to be transferred to USyd’s Artemis/RDS system:\n\nchmod 600 ~/.ssh/authorized_keys\n\nConnect to USyd’s Artemis/RDS system using lftp and your unikey:\n\nlftp sftp://&lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nProvide your password when prompted. Then make and move into a .ssh directory if you don’t already have one:\nmkdir -p ~/.ssh\ncd ~/.ssh\n\nTransfer the authorized_keys file from Gadi to USyd’s Artemis/RDS system:\n\nput authorized_keys\nDoing this will transfer authorized_keys on Gadi to your current directory. With lftp, it will look for the file relative to where you launched lftp. You can check where you are on Gadi using:\nlocal pwd\n\nExit your lftp connection to USyd’s Artemis/RDS system ctrl + d and test the passwordless connection:\n\nsftp &lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nThis time, you shouldn’t be prompted for a password. You can proceed to transfer data between Gadi and USyd’s Artemis/RDS system now on the copyq.\nIf you get the error “Fatal error: Host key verification failed” you may have to get an “ssh fingerprint” first. Do this by sending an ssh request to the RDS with:\nssh &lt;your-unikey&gt;@research-data-ext.sydney.edu.au\nAccept that you trust the connection and enter your passowrd. The connection will then close with the following message:\nThis service allows sftp connections only.\nConnection to research-data-ext.sydney.edu.au closed.\nBut now try lftp connection again!\n\n\n\n\n\nTemplate copyq scripts for transferring data with sftp\nThe scripts below use sftp to transfer data between RDS and Gadi on the Gadi copyq. sftp can transfer whole files and directories but must copy all of your data every time, it cannot only copy modified files like rsync can. This makes it considerably slower for copying large datasets where only minor changes have been made during a run. An alternative command lftp can behave like rsync but is slower to transfer than sftp. We also provide a template lftp script below.\nCopies of these scripts have been placed in /scratch/qc03/data-transfer-scripts/gadi-scripts. You can make a copy of these scripts to your /scratch/&lt;nci-project-code&gt; or /home/&lt;nci-user-id&gt; workspace on Gadi and edit (for example using nano &lt;script&gt;), by replacing the names described in the header to suit your needs.\nThere are two scripts:\n\nfrom_gadi_to_rds.pbs is used to transfer a file or folder from Gadi to RDS\nfrom_rds_to_gadi.pbs is used to transfer a file or folder from RDS to Gadi\n\n\n\n\n\n\n\nTransfer from Gadi to RDS\n\n\n\n\n\nfrom_gadi_to_rds.pbs script, replace variables in &lt;brackets&gt; as described:\n#!/bin/bash\n\n# Transfer from Gadi to RDS\n#\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;      : Your USyd unikey\n# &lt;rds_project&gt; : Your RDS project name\n# &lt;local_path&gt;  : The local file or folder you want to copy\n# &lt;remote_path&gt; : The location on RDS to put your folder\n# &lt;nci_project&gt; : Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\nremote_path=/rds/PRJ-&lt;rds_project&gt;/&lt;remote_path&gt;\n\n# NOTE: Add a trailing slash (/) to local_path if you don't want to create the\n# parent directory at the destination.\nlocal_path=&lt;local_path&gt;\n\nsftp ${remote_user}@${remote_host}:${remote_path} &lt;&lt;&lt; $\"put -r ${local_path}\"\n\n\n\n\n\n\n\n\n\nTransfer from RDS to Gadi\n\n\n\n\n\nfrom_rds_to_gadi.pbs script, replace variables in &lt;brackets&gt; as described:\n#!/bin/bash\n\n# Transfer a folder from RDS to Gadi\n# This will recreate your RDS path (/rds/PRJ-&lt;rds_project&gt;)\n# on Gadi in /scratch/&lt;nci_project&gt;\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;      : Your USyd unikey\n# &lt;rds_project&gt; : Your RDS project name\n# &lt;remote_path&gt; : The location on RDS of your file ot directory to copy\n# &lt;local_path&gt;  : The name of the folder to copy to\n# &lt;nci_project&gt; : Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\n# RDS:\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\n\n# NOTE: Add a trailing slash (/) to remote_path if you don't want to create the\n# parent directory at the destination. \nremote_path=/rds/PRJ-&lt;rds_project&gt;/&lt;remote_path&gt;\n\n# Gadi:\ndest_path=/scratch/&lt;nci_project&gt;/&lt;local_path&gt;\n\n# Copy folder with sftp\nsftp -r ${remote_user}@${remote_host}:${remote_path} ${dest_path}\n\n\n\nHere is an example showing you how to transfer a folder called MyData in the RDS project Training to some scratch space in Gadi owned by project aa00.\n\n\n\n\n\n\nExample copyq transfer from RDS to Gadi\n\n\n\nLog into Gadi and change directory to your project space and make a folder for your workspace:\n# Using an example username tm0000\nssh tm0000@gadi.nci.org.au\n\ncd /scratch/aa00\n\n# Make a folder called workspace in /scratch/aa00/tm0000\nmkdir -p /scratch/aa00/tm0000/workspace\nCopy the required data transfer script template from /scratch/qc03 to your newly made workspace. In this case we are copying from RDS to Gadi so we use the from_rds_to_gadi.pbs script. You can also cut and paste the template script into your editor from above and save the edited script to your workspace.\ncp /scratch/qc03/data-transfer-scripts/gadi-scripts/from_rds_to_gadi.pbs /scratch/aa00/tm0000/workspace\nThen follow the script and move to that workspace and open the script in an editor (in this example we’ll use the nano editor):\ncd /scratch/aa00/tm0000/workspace\n\nnano from_rds_to_gadi.pbs\nYou need to edit the script by replacing all the variables marked with &lt;&gt; described in the script header and fill in the following details before using it:\nIn the # PBS variables part of the script:\n\nProvide the -P variable by replacing &lt;nci_project&gt; with your NCI project code. In this example aa00.\nIncrease the walltime if you are transferring large files, the limit on this queue is 10 hours.\nAlter -lstorage=scratch/&lt;project&gt; as required. If you also need to access g/data, you can change this to scratch/&lt;project&gt;+g/data/&lt;project&gt;. In this example we’ll just use scratch/aa00\n\nIn the body of the script:\n\nProvide the remote_user variable by replacing &lt;unikey&gt; with your USyd unikey.\nProvide the remote_path variable by replacing &lt;rds_project&gt; and &lt;local_path&gt; with your RDS project name and path to the file or directory you want to transfer. In this example we use remote_path=/rds/PRJ-Training/MyData\n\nHere is what the script will look like when correctly edited for this example:\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/aa00\n\n# RDS:\nremote_user=tm0000     #Example unikey tm0000\nremote_host=research-data-ext.sydney.edu.au\n\n# NOTE: Add a trailing slash (/) to remote_path if you don't want to create the\n# parent directory at the destination. \nremote_path=/rds/PRJ-Training/MyData\n\n# Gadi:\n# This will create /scratch/aa00/MyData if transferring a folder and it doesn't already exist.\ndest_path=/scratch/aa00/MyData\n\n# Copy with sftp\nsftp -r ${remote_user}@${remote_host}:${remote_path} ${dest_path}\nWhen you have finished editing the script save it (using &lt;ctrl&gt;-x and answering y at the prompt if using nano as your editor)\nRun the transfer script\nOnce you have customised the script, you can submit it to the copyq on Gadi. Run the script from the directory where you saved it:\nqsub from_rds_to_gadi.pbs\nThis can be a nerve-wracking process, especially if you are transferring large files. You can check the status of your job on Gadi using:\nqstat -Esw\nOnce it says R (running), you can confirm it is going to where you want on RDS/Artemis or Gadi by logging into the system and checking for the presence of the file/directory in its expected location using:\nls MyData\n\n\n\n\nConfirm transfers after competion\nTo confirm the transfer was successful, you’ll need to check your job logs. These are located in the same directory as your script and are named transfer.o&lt;jobid&gt;.\nHowever, this doesn’t guarantee the integrity of the files. You should check the files themselves to ensure they are intact. You can do this using md5checksums. See this SIH tidbits blogpost about how to use these. You’ll need to create md5checksums for the original files if they don’t already exist and compare them after transfer.\n\n\nTemplate copyq scripts for syncing data with lftp\nIf you have a large project with many files and only a few of them are modified at any time you can use the mirror command in lftp to only sync the modified files when you are backing up data to RDS. The lftp - mirror command transfers data at a slower speed than sftp but will significantly speed up the backup of data since it generally only needs to copy a small subset of all of the data on Gadi if only a few files have changed since the last sync.\nBelow is a template .pbs script that can be used to sync files between Gadi and RDS, you can copy it to your workspace and modify it as needed similarly to the example above. You can also find this script on Gadi in /scratch/qc03/data-transfer-scripts/gadi-scripts/sync_gadi_to_rds.pbs.\n\n\n\n\n\n\nWarning\n\n\n\nNote that when using this script to copy a folder you should ensure the target directory exists on RDS first, otherwise the parent directory will not be synced with the data.\n\n\n\n\n\n\n\n\nSync between Gadi and RDS\n\n\n\n\n\n#!/bin/bash\n\n# Mirror directory from Gadi to RDS\n#\n\n# Make the following replacements in this file:\n# &lt;unikey&gt;:       Your USyd unikey\n# &lt;rds_project&gt;:  Your RDS project name\n# &lt;path-on-rds&gt;:  The location on RDS to put your directory\n# &lt;path-on-gadi&gt;: The directory on Gadi to mirror to rds\n# &lt;nci_project&gt;:  Your NCI project name\n\n#PBS -P &lt;nci_project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q copyq\n#PBS -l wd\n#PBS -lstorage=scratch/&lt;nci_project&gt;\n\nremote_user=&lt;unikey&gt;\nremote_host=research-data-ext.sydney.edu.au\nremote_path=/rds/PRJ-&lt;rds-project&gt;/&lt;path-on-rds&gt;\n\nsource_path=&lt;path-on-gadi&gt;\n\noutfile=\"${PBS_O_WORKDIR}/${PBS_JOBNAME}_${PBS_JOBID}.log\"\necho \"Writing log to: ${outfile}\"\ntouch $outfile\n\nlftp -u ${remote_user}, sftp://${remote_host} &lt;&lt;EOF\nset xfer:log true\nset xfer:log-file \"${outfile}\"\nmirror -p --verbose -R ${source_path} ${remote_path}\nexit\nEOF\n\n\n\n\n\nSuggested workflow for copying data between RDS and Gadi and keeping it up-to-date\nThe sftp copy method and lftp sync methods described above both have their pros and cons:\n\nsftp has a faster transfer speed but can only copy all your files in bulk when it is run.\nlftp has a slower transfer speed but it can sync only the subset of files that have changed.\n\nBecause of this we suggest users be selective about which method they use based on their needs.\nFor example a simple workflow for large projects that have a lot of data and many files in a folder, only a few of which are changed between backups to RDS would be:\n\nInitially use the sftp based from_rds_to_gadi.pbs script to bulk copy your data from RDS to your project space on Gadi, since this is faster for bulk transfers.\nSubsequently use the lftp based sync_gadi_to_rds.pbs to sync smaller files (e.g. output logs and data) back to RDS, without having to re-copy the bulk of the data back to RDS.\n\n\n\n\nTransfer using rsync from Artemis dtq\n\n\n\n\n\n\nWarning\n\n\n\nThis option is only available prior to the decommission of Artemis on 29 August 2025 - after that date you will have to use either GLOBUS (preferred), or copy data when logged into the Gadi terminal (either at the login shell or using copyq scripts).\n\n\nFor transfer of large files directly from Artemis to Gadi, the use of resumable rsync is recommended (see script below). The transfer can be initiated using Artemis dtq and using Gadi’s data mover node: gadi-dm.nci.org.au. The below template script can be used with Artemis’ dtq using rsync.\nFor further info about copying data from Artemis dtq can be found in the SIH Artemis Training Series.\n\n\n\n\n\n\nTemplate Artemis data transfer script using rsync\n\n\n\n\n\n#!/bin/bash\n\n# This is an Artemis data transfer script\n\n#PBS -P &lt;project&gt;\n#PBS -N transfer\n#PBS -l walltime=04:00:00\n#PBS -l ncpus=1\n#PBS -l mem=8GB\n#PBS -W umask=022\n#PBS -q dtq\n\n# NOTE: Add a trailing slash (/) to source_path if you don't want to create the\n# parent directory at the destination. \nsource_path=/rds/PRJ-&lt;project&gt;/&lt;path&gt;/&lt;file&gt;\n\ndestination=&lt;user&gt;@gadi-dm.nci.org.au\ndestination_path=&lt;path-on-gadi&gt;\n\nwhile [ 1 ]\ndo\n        echo Transferring ${source_path} to ${destination}:${destination_path}\n        rsync -rtlPvz --append-verify ${source_path} ${destination}:${destination_path}\n\n        if [ \"$?\" = \"0\" ]\n        then\n                echo \"Rsync completed normally\"\n                dt=$(date)\n                echo Finished transferring at $dt\n        exit\n        else\n                echo \"Rsync failure. Backing off and retrying in 3 minutes\"\n                sleep 180\n        fi\ndone"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html",
    "href": "notebooks/10_job_efficiency.html",
    "title": "Optimising your job",
    "section": "",
    "text": "COMING SOON - NOT COMPLETE!\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#simple-example-1",
    "href": "notebooks/10_job_efficiency.html#simple-example-1",
    "title": "Optimising your job",
    "section": "Simple example 1",
    "text": "Simple example 1\nThe Broadwell nodes have different memory per CPU ratios than the Cascade Lake nodes. This can be particularly helpful in reducing KSU usage for ‘normal’ compute jobs that require more than 4 GB RAM per CPU but less than what justifies a high memory queue. On the Broadwell ‘normalbw’ queue, there are nodes with 9 GB RAM per CPU. With a charge rate of just 1.25 SU per CPU hour (compared to 2 SU per CPU hour on the equivalent Cascade Lake queue), a job requiring 64 GB RAM for 1 hour could use 7 CPUs on the Broadwell nodes for a charge of 7 cpu * 1 h * 1.25 charge rate = 8.75 SU. The same job on the Cascade Lake ‘normal’ queue would require 16 CPU, totalling 16 cpu * 1 h * 2 charge rate = 32 SU. The reduced processor speed on Broadwell vs Cascade Lake may cause the Broadwell job to require slightly more walltime, however this increase in walltime will typically still cost less overall. We will discuss this in more detail in the section on job efficiency and optimisation where we will review tool benchmarking and job efficiency strategies."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#simple-example-2",
    "href": "notebooks/10_job_efficiency.html#simple-example-2",
    "title": "Optimising your job",
    "section": "Simple example 2",
    "text": "Simple example 2"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#resource-benchmarking",
    "href": "notebooks/10_job_efficiency.html#resource-benchmarking",
    "title": "Optimising your job",
    "section": "Resource benchmarking",
    "text": "Resource benchmarking\n\nwhat is benchmarking\n\nwe are talking about benchmarking compute resources, not technical benchmarking\n\nwhy benchmark\nhow to benchmark"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#benchmarking-template-scripts",
    "href": "notebooks/10_job_efficiency.html#benchmarking-template-scripts",
    "title": "Optimising your job",
    "section": "Benchmarking template scripts",
    "text": "Benchmarking template scripts\nGadi tempalte benchmarking scripts - this repository contains a pair of scripts designed to test single runs of a command/tool at various CPU and memory settings on differnet queues. It does requrie some modification (and carefully use and follow the guide!) to set it up, but once you know how to use this tempalte, it can expedite testing chunks of your workflow to obtain the most efficient (ie optimsied) queue and resource requets for the task. Running the gadi_usage_report.pl script from this repository will summarise the resources used by the benchmark jobs into a table that can be viewed or plotted to determine best resoruces.\nIt is not critical to use this template, but it can be a helpful tool if you have not benchmarked before, or if you benchmark multiple tools/code chunks regularly and want a simple and replicable method."
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#tips-for-benchmarking",
    "href": "notebooks/10_job_efficiency.html#tips-for-benchmarking",
    "title": "Optimising your job",
    "section": "Tips for benchmarking",
    "text": "Tips for benchmarking\n\nTest individual parts of your code where possible - ie one command, one tool, one chunk of code\n\nthis enables you to determine which parts of your workflow have differeing cmpute requirements\nparts with differing compute requirements can be allocated to different queues and resources, saving you KSU\n\nDo initial benchmarking on a small subset of your data - ie subsample, reduce sample numbers, reduce permutations, etc\nFollow up with scalability testing: Once you have refined the candidate best resources, re-run the benchmark on a representative subset (ie whole sample, more iterations) and compare the CPU efficiency\n\nIs it as good as the initial test benchmark in terms of CPU and memory efficiency?\nIf so, then go ahead and apply this setting to your full run\nIf not, re-run full benchmarks with the larger test dataset, or dig deeper into what is causing the loss of effiency at scale\n\nEmbrace the labour of benchmarking!\n\nWhile it may seem like a time-consuming impediment to getting on with analysing your data, it can save you a lot of time and KSU down the track.\nBenchmarking will make your analysis faster and use less USyd-funded resources and energy resources\nbenchmarking can prevent avoidable job failures such as a job runing out of walltime or memory, which will cost more time and resources to resubmit"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#demo-benchmarking-activity",
    "href": "notebooks/10_job_efficiency.html#demo-benchmarking-activity",
    "title": "Optimising your job",
    "section": "Demo benchmarking activity",
    "text": "Demo benchmarking activity\n\nuse the template, demo how to edit\nrun on a couple of queues\nrun the usage script\nview the table and identify optimal resources"
  },
  {
    "objectID": "notebooks/10_job_efficiency.html#example-of-a-complete-benchmark-study-including-scalability-testing-plots",
    "href": "notebooks/10_job_efficiency.html#example-of-a-complete-benchmark-study-including-scalability-testing-plots",
    "title": "Optimising your job",
    "section": "Example of a complete benchmark study including scalability testing plots",
    "text": "Example of a complete benchmark study including scalability testing plots"
  },
  {
    "objectID": "notebooks/06_accounting.html",
    "href": "notebooks/06_accounting.html",
    "title": "Accounting",
    "section": "",
    "text": "Coming soon! For now, see: https://opus.nci.org.au/spaces/Help/pages/236881132/Allocations…\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03_expectations.html",
    "href": "notebooks/03_expectations.html",
    "title": "What does the system expect of its users?",
    "section": "",
    "text": "The do’s and don’ts of using Gadi\nSIH Into to Gadi HPC tutorial\nPro tips for bioinformatics on HPC webinar\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/03_expectations.html#what-is-high-performance-computing",
    "href": "notebooks/03_expectations.html#what-is-high-performance-computing",
    "title": "What does the system expect of its users?",
    "section": "What is high performance computing?",
    "text": "What is high performance computing?\nHigh performance computing refers to the use of parallel processing techniques to solve complex computation problems efficiently. HPC systems, like Gadi, consist of clusters of interconnected computers, each equipped with multiple processors and large amounts of memory. These systems are designed to handle massive datasets and perform computations at speeds far beyond those achievable by your personal computer."
  },
  {
    "objectID": "notebooks/03_expectations.html#why-do-we-need-hpc-for-bioinformatics",
    "href": "notebooks/03_expectations.html#why-do-we-need-hpc-for-bioinformatics",
    "title": "What does the system expect of its users?",
    "section": "Why do we need HPC for bioinformatics?",
    "text": "Why do we need HPC for bioinformatics?\nIn bioinformatics, researchers deal with massive datasets generated by technologies such as next-generation sequencing (genomics, transcriptomics) and mass spectrometry (proteomics). Analysing these datasets requires computationally intensive tasks such as sequence alignment, genome assembly, and statistical analysis. HPC systems provide the computational power and memory resources necessary to process these datasets efficiently."
  },
  {
    "objectID": "notebooks/03_expectations.html#expectations",
    "href": "notebooks/03_expectations.html#expectations",
    "title": "What does the system expect of its users?",
    "section": "Expectations",
    "text": "Expectations\nGadi is a shared resource and its efficient use not only ensures fair access for all users but also helps minimise the environmental impact of high-performance computing, as systems like Gadi consume significant energy resources. When you are using a system like Gadi, there are potentially hundreds of other users accessing the system at the same time as you. For Gadi to remain efficient and usable, everyone needs to be courteous and use the system with consideration for others.\nHere are some tips to help you be a good citizen of the HPC community:\n\n1. Use job queues appropriately\n\nGadi job queues\nGadi queue limits\n\nGadi runs a PBSpro job scheduler that manages the allocation of resources to users. When you submit a job, it is placed in a queue and will run when the requested resources become available. Unlike on Artemis where your job is allocated to a suitable queue based on your resource request, Gadi users need to explicitly request their job is sent to a specific queue. It is important for you to pick a job queue that is appropriate for your job.\n\n\n2. Responsibly manage your data\n\nNCI file management policy\nTransferring data between RDS and Gadi\n\n/scratch is not a safe space for long term data storage. If it has not been accessed in 100 days, it will be subjected to NCI’s clean up policy. If you have a /g/data allocation, this is a better place to store your data whilst working on Gadi. Once you have finished your analysis, it is best practice to move your data to a more permanent storage solution, like RDS.\n\n\n3. Don’t request more resources than you need\n\nGadi benchmarking tool\n\nDon’t request resources that you won’t need, it will only result in your job and other users jobs being held up, and you wasting your service unit allocation. The PBS scheduler will find time for 2 cpus faster than 4 cpus, so in the interest of speed, be efficient. Given the bursty nature of some jobs, it can be hard to know what resources a tool needs. We suggest the following:\n\nStep 1: Consult the software documentation\n\nOften, developers will outline the minimum amount of RAM (memory) and whether a tool is multi-threaded (e.g. use &gt;1 CPU or GPU)\n\nStep 2: Run a test job using our Gadi benchmarking tool\n\nThis will give you a good idea of how much resources you need to request for your main job.\n\nStep 3: Ask for help\n\n\n\n4. Keep track of your resource usage\n\nMonitor your jobs\nMonitor your project allocation\nWhat does a job cost?\nWhy are my jobs not running?\n\nRunning jobs on gadi requires users to have sufficient compute hours available. These compute hours are granted to projects rather than directly to the user. It is important to communicate with your project team to ensure you are not using more than your fair share of resources. You can monitor your project’s usage by running:\nnci_project -P &lt;project&gt; -v\nIf you are consistently overusing resources, you may need to look into optimising your workloads and/or requesting more resources from NCI. Get in touch with SIH to discuss your options.\nAt completion, your project is only charged the SU actually consumed by the job (ie based on walltime used, not walltime requested). Like Artemis, Gadi produces PBS logs. The “.o” job log will report the compute used (similar to the Artemis “.o” and “usage” logs combined).\nLike KSU, each project is assigned a finite amount of disk space and iNode (index node - can be likened to the total number of files and folders). You MUST monitor your disk and iNode usage, and this can be done with the command:\nlquota\nwhich shows disk resource availability for every project you are a member of.\nIt is important to have an understanding of how much output your job will create, and ensure that you can remain within quotas/limits. Jobs can fail with “disk quota exceeded” messages."
  },
  {
    "objectID": "notebooks/11_software.html",
    "href": "notebooks/11_software.html",
    "title": "Software options on Gadi",
    "section": "",
    "text": "global apps\nspecialised environments\nself-installed tools\nsingularity containers\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/11_software.html#global-apps",
    "href": "notebooks/11_software.html#global-apps",
    "title": "Software options on Gadi",
    "section": "Global apps",
    "text": "Global apps\nOn Gadi, there are shared (global) apps that are installed and managed by the system administrators. On Artemis, these were in /usr/local/ directory, on Gadi they are in /apps/ directory.\nYou can use the same module commands that you are familiar with on Artemis to query and load apps on Gadi.\n# List all global apps starting with 'p':\nls  /apps/p*\n# List all modules for python3:\nmodule avail python3\n# Load a specific version of python3:\nmodule load python3/3.12.1\nEach global app has a default version, so if you run without specifying a version, the default version will be loaded. While this is OK in some circumstances, it is typically recommended to specify the version you know works for your code. Default versons of global apps will change over time without warning, so reproducibility and functionality is best maintained by explicitly stating the version when you load a module within your script."
  },
  {
    "objectID": "notebooks/11_software.html#software-groups-and-specialised-environments",
    "href": "notebooks/11_software.html#software-groups-and-specialised-environments",
    "title": "Software options on Gadi",
    "section": "Software groups and specialised environments",
    "text": "Software groups and specialised environments\nNCI provides a range of software groups and specialised environments for different resarch fields. Before attempting to self-install a tool, you should check if the tool you need is avalable through global apps or one of these environments.\nPlease visit the NCI pages for more details:\n\nAI/machine learning\ndata analysis\nbioinformatics and genomics\nclimate and weather\nearth observation and environment\ngeophysics\nquantum computing\nvisualisation"
  },
  {
    "objectID": "notebooks/11_software.html#self-installed-tools",
    "href": "notebooks/11_software.html#self-installed-tools",
    "title": "Software options on Gadi",
    "section": "Self-installed tools",
    "text": "Self-installed tools\nUnlike Artemis, request of new apps to be installed are not always agreed to. NCI limits global apps to those with a high user base. This is to ensure good maintenance and curation of global apps.\nUsers are encouraged to either self-install apps from source into their /home or /g/data locations, or (recommended) use singularity containers. Installing into /scratch is not recommended due to the 100-day file purge policy. Install into /g/data is ideal when other members of your project need to use the same tool.\nNCI may provide support for users through the self-install process; to request assistance, please email your detailed request including what software tool and version you are attempting to install and describe the issues you are having with the installation.\nOnce a tool has been installed, you can make that tool available to the module commands, by following the steps described here. This is not essential, but can be helpful when managing tools that multiple group members will use.\nTo avoid the burden of installing software that is not available through global apps or specialised environments, the use of singularity containers is recommended."
  },
  {
    "objectID": "notebooks/11_software.html#running-singularity-containers-on-gadi",
    "href": "notebooks/11_software.html#running-singularity-containers-on-gadi",
    "title": "Software options on Gadi",
    "section": "Running singularity containers on Gadi",
    "text": "Running singularity containers on Gadi\nSingularity can be used to execute containerised applications on Gadi. It is installed as a global app:\nmodule load singularity\nsingularity version\n# 3.11.3\nNote that the singularity project was recently migrated to apptainer. Please continue to run singularity commands on Gadi for now.\nSingularity (Apptainer) is a “container platform. It allows you to create and run containers that package up pieces of software in a way that is portable and reproducible. You can build a container using Apptainer on your laptop, and then run it on many of the largest HPC clusters in the world, local university or company clusters, a single server, in the cloud, or on a workstation down the hall. Your container is a single file, and you don’t have to worry about how to install all the software you need on each different operating system.” (from apptainer.org)\nThere are numerous container repositories, for example Docker Hub or quay.io. You can search these repositories, or build your own container if the tool or tool version you need is not yet available.\nSeqera have greatly simplifed the process of building custom containers with their build-your-own container tool! Simply search for the tool(s) you want in your container, and click Get Container. This tool will manage the build for you, and host the created container file.\n\nExample\nLet’s assume you want to use the tool FoldSeek. Below are the steps to search for, obtain, test, and use this container in a PBS job script.\n\nRun module load singularity in your Gadi terminal (or copyq container download script)\nVisit quay.io and search for this tool by typing the tool name into the search bar at the top right of the page. There may be multiple containers available. These may reflect different contributors, different tool versions, etc. Look for containers with recent updates and a high star rating where possible.\nSelect the biocontainers container, and then select the Tags page from the icons on the left (options are Information, Tags, Tag history).\nOn the far right of the most recent tag, select the Fetch tag icon and then change image format to Docker Pull (by tag).\nCopy the command. We need to made some changes to the command before we execute it.\nPaste the command into your terminal (or copyq container download script) and change docker to singularity and add prefix docker:// to the contaner path, as shown below:\n\n# Default command:\ndocker pull quay.io/biocontainers/foldseek:10.941cd33--h5021889_1\n# Change to:\nsingularity pull docker://quay.io/biocontainers/foldseek:10.941cd33--h5021889_1\n\nRun the pull command (note: you need to have the singularity module loaded in your terminal or download script). This will download the docker container to a Singularity Image File (.sif). Most containers are lightweight enough to pull directly on the login node. If a container is bulky and slow to download (or you need many containers), you may need to submit the download as a copyq job.\nTest the container with a basic help or version command. When running a container, typical usage involves the following command structure:\n\nsingularity  exec &lt;container&gt; &lt;command&gt; [args]\nSo everything after the container name is the same as you would run when using a locally installed version of the tool. For foldseek, we would use foldseek version to view the tool verison or foldseek help to view the help menu. To run these commands via the container:\nsingularity exec foldseek_10.941cd33--h5021889_1.sif foldseek version\nsingularity exec foldseek_10.941cd33--h5021889_1.sif foldseek help\nNote that the version of foldseek corresponds to the tag name: in this case, v. 10.941cd33.\n\nTest run the full tool command that you need to use in your analysis. Where possible, use a small subset of your data to test the command (as you would routinely do for new software). You may need to use the interactive job queue or a compute job script for testing if the test command exceeds resource limits on the login nodes.\nAdd the command to your job script. Ensure to include module load singularity and call the container using the structure singularity  exec &lt;container&gt; &lt;command&gt; [args].\n\nBelow is an example Gadi job script running the foldseek container:\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -q normal\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l walltime=02:00:00\n#PBS -l storage=scratch/aa00+gdata/aa00\n#PBS -l wd\n\nmodule load singularity\n\ninput=/g/data/aa00/foldseek-inputs/input.fasta\ndatabase=/g/data/aa00/foldseek-inputs/database\noutput_dir=./foldseek-run/output\nresults=./foldseek-run/foldseek_result.tsv\nlog=./foldseek-run/foldseek.log\n\nsingularity exec \\\n    foldseek_10.941cd33--h5021889_1.sif \\\n    foldseek easy-search  ${input} \\\n    ${database} \\\n    ${output_dir} \\\n    --threads ${PBS_NCPUS} \\\n    ${results} &gt; ${log} 2&gt;&1"
  },
  {
    "objectID": "notebooks/12_walltime.html",
    "href": "notebooks/12_walltime.html",
    "title": "Working within walltime limits",
    "section": "",
    "text": "TO BE COMPLETED\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/12_walltime.html#introduction",
    "href": "notebooks/12_walltime.html#introduction",
    "title": "Working within walltime limits",
    "section": "Introduction",
    "text": "Introduction\nIn this section, we will discuss ways of adapting your long walltime jobs from Artemis to NCI platforms.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/12_walltime.html#gadi-walltime-limit",
    "href": "notebooks/12_walltime.html#gadi-walltime-limit",
    "title": "Working within walltime limits",
    "section": "Gadi walltime limit",
    "text": "Gadi walltime limit\nThe maximum walltime permitted on any of the Gadi HPC queues is 48 hours. In some cases, the walltime may be less (for example when requesting large numbers of nodes, or on copyq).\nGiven that Artemis has much longer maximum walltimes, we understand this may generate some apprehension. Staff at both NCI and SIH can support you in adapting your workflows to NCI if you are still having difficulty after reviewing the suggestons below.\nIn short, there are 3 options to adapting a long-running Artemis workflow to NCI:\n\nSplit your single large job Artemis into a series of smaller jobs on Gadi\nUse NCI’s Nirin cloud instead of Gadi\nSpecial exception to the Gadi walltime limit granted on a case-by-case basis"
  },
  {
    "objectID": "notebooks/12_walltime.html#split-your-job",
    "href": "notebooks/12_walltime.html#split-your-job",
    "title": "Working within walltime limits",
    "section": "1. Split your job",
    "text": "1. Split your job\nThere are many advantages to splitting your job up into smaller discrete chunks.\n\nCheckpointing: if one of your jobs in a series of fails, you only need to resubmit that discrete job script, rather than either the whole job or some finnicky “hashed out” version of your very long and comlex workflow script. This simplifies debugging and rerunning, saves you hands-on time and walltime, minimises errors, and saves KSU\nEase of code maintenance: changing part of workflow, for example adjusting parameters, input files or software versions, is far simpler to implement for shorter chunks of code than it is for a long and complex code with many steps\nEase of benchmarking: Different stages of a complex workflow typically have different compute requirements, for example some long running single core tasks coupled with some GPU tasks, some high memory, some high CPU tasks etc. Benchamrking is more straightforward and informative when performed on discrete workflow chunks.\nGreater job efficiency: By benchmarking and optimising the resource configurations for each stage of the workflow, the series of jobs can be placed on an appropriate queue, and will not be reserving (and being charged for) unused resources. This will reduce KSU usage and resource wastage.\nShorter queue times: Requesting resources for a shorter walltime will result in a shorter queue time. The NCI scheduler is geared towards favouring ‘wider and shorter’ jobs, ie more CPUs/nodes for less time, over ‘taller and slimmer’ jobs (ie fewer CPUs/nodes for a longer time). For example a job may queue for less time if it requests 48 CPU for 1 hour, compred to 1 CPU for 48 hours. Of course the queue is highly dynamic and this cannotbe predicted or calculated ahead of time, but in general, shorter walltimes will lead to shorter queue times."
  },
  {
    "objectID": "notebooks/12_walltime.html#use-nirin",
    "href": "notebooks/12_walltime.html#use-nirin",
    "title": "Working within walltime limits",
    "section": "2. Use Nirin",
    "text": "2. Use Nirin\nYour NCI KSUs can be used on Nirin as well as Gadi. Nirin has the advantage of theoretically infinite walltime, along with internet access which is another limitation of the Gadi compute queues.\nAs such, Nirin presents an easily accessible solution for users whose jobs are irreconcilably affected by the walltime and lack of internet access aspects of Gadi.\nThe Nirin quickstart guide walks you through the process of setting up your instance, including easy to follow screenshots for each step."
  },
  {
    "objectID": "notebooks/12_walltime.html#gadi-special-walltime-request",
    "href": "notebooks/12_walltime.html#gadi-special-walltime-request",
    "title": "Working within walltime limits",
    "section": "3. Gadi special walltime request",
    "text": "3. Gadi special walltime request\nIf your job cannot be split/checkpointed into a series of shorter jobs, and the Nirin flavours are not suited to your compute needs, you can make a request to NCI for an increase to the walltime. NCI will ask you to provide details of your job including the relevant code saved on Gadi, as well as a description of why you require a lift to the walltime for this particular job.\nFrom the Gadi queue limits page:\n“If a higher limit on core count (PBS_NCPUS) and walltime is needed, please launch a ticket on NCI help desk with a short description of the reasons why the exception is requested. For example, a current scalability study suggests linear speedup at the core count beyond the current PBS_NCPUS limit. We will endeavour to help on a case-by-case basis”"
  },
  {
    "objectID": "notebooks/02_system_setup.html",
    "href": "notebooks/02_system_setup.html",
    "title": "How is the system set up?",
    "section": "",
    "text": "This section will point you to the right sections of the NCI documentation and user guides to get you started on the Gadi HPC system.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/02_system_setup.html#resources",
    "href": "notebooks/02_system_setup.html#resources",
    "title": "How is the system set up?",
    "section": "Resources",
    "text": "Resources\n\nWhat is Gadi?\nGetting Started at NCI\nGadi User Guide\nGadi FAQs\n\nAnd for a Linux/Unix refresher:\n\nSandbox.bio terminal basics tutorial\nBash cheatsheet"
  },
  {
    "objectID": "notebooks/02_system_setup.html#gadi-technical-summary",
    "href": "notebooks/02_system_setup.html#gadi-technical-summary",
    "title": "How is the system set up?",
    "section": "Gadi technical summary",
    "text": "Gadi technical summary\nNCI Gadi is one of Australia’s most powerful supercomputers, designed to support advanced computational research.\n\n\n\n\n\n\n\nComponent\nDetails\n\n\n\n\nCompute\n- Nodes: 4,962- Processors: Intel Sapphire Rapids, Cascade Lake, Skylake, and Broadwell CPUs- GPUs: NVIDIA V100 and DGX A100 GPUs- Performance: Over 10 petaflops of peak performance :contentReferenceoaicite:0\n\n\nStorage\n- Disk Drives: 7,200 4-Terabyte hard disks in 120 NetApp disk arrays- Capacity: 20 Petabytes total usable capacity- Performance: 980 Gigabytes per second maximum performance :contentReferenceoaicite:1\n\n\nFilesystems\n- Total Capacity: Approximately 90 Petabytes- Global Lustre Filesystems: Five, with an aggregate I/O performance of around 450 GB/second- IO Intensive Platform: Dedicated filesystem using 576 2-Terabyte NVMe drives, achieving around 960 Gigabytes per second cumulative performance :contentReferenceoaicite:2\n\n\nArchival Storage\n- Capacity: Over 70 Petabytes of archival project data stored in state-of-the-art magnetic tape libraries :contentReferenceoaicite:3\n\n\nNetworking\n- Interconnect: 100-gigabit network links connecting high-performance computing with high-performance data :contentReferenceoaicite:4\n\n\nCloud Systems\n- Nirin Cloud: High-availability and high-capacity zone integrated with Gadi and NCI’s multi-Petabyte national research data collections, comprising Intel Broadwell and Sandy Bridge processors and NVIDIA K80 GPUs :contentReferenceoaicite:5"
  },
  {
    "objectID": "notebooks/02_system_setup.html#conditions-of-use",
    "href": "notebooks/02_system_setup.html#conditions-of-use",
    "title": "How is the system set up?",
    "section": "Conditions of use",
    "text": "Conditions of use\n\nConditions of use and policies\n\nAll users of NCI agree that they will keep themselves informed of, and comply with, all relevant legislation and The Australian National University policies and rules.\nAll users must acknowledge and understand that a breach of these will result in not only a loss of access to NCI resources but the user may be subject to Federal criminal prosecution resulting in fines and/or gaol legislated under the Acts listed on the conditions of use and policies page."
  },
  {
    "objectID": "notebooks/02_system_setup.html#login-nodes",
    "href": "notebooks/02_system_setup.html#login-nodes",
    "title": "How is the system set up?",
    "section": "Login nodes",
    "text": "Login nodes\nThese nodes are the gateway for Gadi for users to access the resources of the HPC cluster. It is how you log in to Gadi, move around the filesystem, submit jobs to the scheduler, and do small tasks like view the contents of a file.\nWhen you first ssh into Gadi, you are working on a login node. These are distinct from the compute nodes and are not designed for running compute tasks or transferring data.\nWhile you can run simple commands on the login nodes on Gadi, commands are throttled. This means they execute more slowly than on the compute nodes and are automatically killed if they exceed CPU, memory and time restrictions that are in place. This is to ensure the login nodes are not over-loaded and remain responsive for all users. As such, all complex or resource-intensive tasks must be submitted to the job scheduler for execution on the cluster nodes."
  },
  {
    "objectID": "notebooks/02_system_setup.html#compute-nodes",
    "href": "notebooks/02_system_setup.html#compute-nodes",
    "title": "How is the system set up?",
    "section": "Compute nodes",
    "text": "Compute nodes\nThese nodes are the workhorses of any HPC. They are dedicated for executing computational tasks as delegated by the job scheduler when you submit a job. There are various types of compute nodes with different hardware, built for different purposes on Gadi. Depending on the resource requirements of your job (e.g. high memory, GPUs) and the queue you specify, your job will be sent to a specific type of compute node.\nPlease see the queue structure on Gadi and the queue limits pages for detailed information on the compute node specifications.\nFor details of how to choose and request a specific queue for your job, please visit the section on PBS scripts and job submission."
  },
  {
    "objectID": "notebooks/02_system_setup.html#data-mover-nodes",
    "href": "notebooks/02_system_setup.html#data-mover-nodes",
    "title": "How is the system set up?",
    "section": "Data mover nodes",
    "text": "Data mover nodes\nThese nodes are designed specifically for fast data movement. You can use these nodes to transfer files to and from Gadi at high-speed.\nPlease visit the Gadi file transfer page for more details.\nA comprehensive guide to data transfer between Gadi and University of Sydney RDS, including example commands, tempalte scripts and recommendations, can be found in the section on transferring data."
  },
  {
    "objectID": "notebooks/02_system_setup.html#filesystems",
    "href": "notebooks/02_system_setup.html#filesystems",
    "title": "How is the system set up?",
    "section": "Filesystems",
    "text": "Filesystems\n\n$HOME\nWhen you first log in to Gadi, you’ll be placed in your personal $HOME directory (i.e. /home/555/aa1234). You are the only person who can access this directory. No work should be done in here due to the strict 10 GB storage limit, but you may wish to install things like custom R or Python libraries here. It is backed up.\nYou can navigate back here at any point if required by running:\ncd ~\n\n\n/scratch\nAll Gadi projects have a dedicated /scratch allocation that is only accessible to members of your project. This is only intended for active work and not for long-term storage. This is not backed up and any files not accessed for 100 days will be purged from the system, so be sure to back up your work to RDS following our data transfer guide.\nVisit the NCI scratch file management page for details on the purge policy and how to recover files from ‘quarantine’.\nScratch will be /scratch/&lt;nci-project-id&gt;. Each member of a project will have read/write permissions for this parent directory, as well as their /scratch/&lt;nci-project-id&gt;/&lt;nci-user-id&gt; directory.\nYou can navigate to your /scratch space by running:\ncd /scratch/&lt;project&gt;\n\n\n/g/data\nSome Gadi projects have a dedicated /g/data allocation that is only accessible to members of that group. This in intended for long-term large data storage, for example large reference files or databases that are regularly required for your compute jobs. Compute node jobs can read /g/data so this is an ideal place to store those files so that they are not subject to /scratch purge. Files that are on /g/data for this kind of use should also have a copy on RDS, since /g/data/ is not backed up.\nIf you are unsure if your project has a /g/data allocation, you can check by running:\ncd /g/data/&lt;project&gt;\nScratch is provided at no additional cost to users, however g/data has a cost per TB involved. This cost is covered by the NCI-Sydney Scheme for projects that have a justified need of it. To request /g/data storage space, please request this via an email to nci-sydney.scheme@sydney.edu.au with a brief justification.\n\n\n/apps\nThis directory is accessible to all Gadi users. It is a read-only system containing centrally installed software applications and their module files.\nPlease visit the software section for more information on global apps and alternatives."
  },
  {
    "objectID": "notebooks/00_gadi_access.html",
    "href": "notebooks/00_gadi_access.html",
    "title": "Accessing NCI",
    "section": "",
    "text": "To access any of NCI’s computing platforms including Gadi HPC, you first need to create an NCI account. You will also need to either create a new project or request to join an existing one. You can be a member of multiple projects.\nAll new users must create their account through the NCI online self service portal. To create your account you will need the following information:\nNote that resources at NCI are allocated to projects and not to individual users.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/00_gadi_access.html#create-an-account-and-join-an-existing-project",
    "href": "notebooks/00_gadi_access.html#create-an-account-and-join-an-existing-project",
    "title": "Accessing NCI",
    "section": "Create an account and join an existing project",
    "text": "Create an account and join an existing project\n\nClick on ‘Sign up’ link on the NCI online self service portal: \nAccept the terms and conditions: \nProvide your personal details: \nProvide details on the project you’ll be working on. Select I need to join one or more existing projects. Enter the NCI project code (eg qc03 for the SIH training project) in the Projects menu: \nSelect University of Sydney as your institution: \n\nYour username will become active when a project Lead CI approves your request to join their project. You will receive a confirmation email from the Mancini system when your username is activated."
  },
  {
    "objectID": "notebooks/00_gadi_access.html#create-an-account-and-propose-a-new-project",
    "href": "notebooks/00_gadi_access.html#create-an-account-and-propose-a-new-project",
    "title": "Accessing NCI",
    "section": "Create an account and propose a new project",
    "text": "Create an account and propose a new project\n\nTo create your account, follow the first three steps from the above section\nOn the NCI page Step 3 of 5 - Project, select I intend to propose a new project\nComplete the account creation form and verify your account by following the account verification email\nOnce activated, you can login to MyNCI and propose a new project by selecting this from the menu bar on the left:\n\n\n\nFollow the prompts to fill the form for your proposed project\nAt Propose a project: step 5 of 10, select University of Sydney\nAt Propose a project: step 6 of 10, select Gadt (at NCI) under HPC Compute. Most projects will not require massdata. Some may require gdata, and this can be added later if necessary. All new projects are allocated some scratch space and this comes at no charge, however the other two file systems have an attached cost.\nAt Propose a project: step 7 of 10, you are asked to fill in your initial resource requirements in KSU. One KSU is 1,000 service units, which is roughly equivalent to 500 CPU hours depending on the type of resource used. We expect 1 KSU to be a sufficient starting amount for most projects coming from Artemis. If you have a good understanding that you will require more than this, please enter those values here. They can easily and quickly be increased at a later date if needed.\nComplete the rest of the form, ensuring that you carefully review the terms and conditions of use and declare the status of your project in relation to the Defence Trade Controls Act.\n\nYour new project will become active when a USyd Scheme Manager approves your new project proposal."
  },
  {
    "objectID": "notebooks/00_gadi_access.html#i-already-have-an-nci-account-how-can-i-join-an-existing-project",
    "href": "notebooks/00_gadi_access.html#i-already-have-an-nci-account-how-can-i-join-an-existing-project",
    "title": "Accessing NCI",
    "section": "I already have an NCI account, how can I join an existing project?",
    "text": "I already have an NCI account, how can I join an existing project?\n\nLogin to MyNCI and select Projects and groups from the menu bar on the left\nSwitch the tab to Find project or group and search for the project you wish to join\n\n\n\nSelect the project from the search results, confirm the project by checking the overview, then switch the tab to Join\nAfter agreeing to the terms and conditions, select Request membership. You membership needs to be approved by the Lead Chief Investigator"
  },
  {
    "objectID": "notebooks/08_job_script.html",
    "href": "notebooks/08_job_script.html",
    "title": "Running jobs on Gadi",
    "section": "",
    "text": "In this section, we will compare Artemis job scripts to Gadi job scripts, and provide some guidance on how to adapt your Artemis workflow to NCI Gadi HPC.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first two on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/08_job_script.html#introduction",
    "href": "notebooks/08_job_script.html#introduction",
    "title": "Running jobs on Gadi",
    "section": "",
    "text": "In this section, we will compare Artemis job scripts to Gadi job scripts, and provide some guidance on how to adapt your Artemis workflow to NCI Gadi HPC.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nGadi walltime limit of 48 hours\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\nJob arrays not supported on Gadi\n\nIn this section, we will look at the first two on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/08_job_script.html#job-scheduler",
    "href": "notebooks/08_job_script.html#job-scheduler",
    "title": "Running jobs on Gadi",
    "section": "Job scheduler",
    "text": "Job scheduler\nLike Artemis, NCI runs the Altair PBS professional workload manager.\nWhile you can run simple commands on the login nodes on Gadi, commands are throttled. This means they execute more slowly than on the compute nodes and are automatically killed if they exceed CPU, memory and time restrictions that are in place. This is to ensure the login nodes are not over-loaded and remain responsive for all users. As such, all complex or resource-intensive tasks must be submitted to the job scheduler for execution on the cluster nodes.\nSubmitting jobs on Gadi is very similar to submitting jobs on Artemis. You will submit a PBS (Portable Batch System) submission script that specifies your job’s compute requirements along with the commands to execute the tasks.\nPBS scripts are text files that contain directives and commands that specify the resources required for a job and the commands to run. Typically they are named &lt;script_name&gt;.pbs however the .pbs suffix is not required, merely helpful to discern the intention of the script.\nOnce submitted to the PBS job scheduler with the qsub command, the scheduler reads the compute requirements from the directives component of the script, and either runs your job right away (if the requested resources are immediately available) or queues the job to run later (if the requested resurces are not currently available)."
  },
  {
    "objectID": "notebooks/08_job_script.html#job-scheduling-priority",
    "href": "notebooks/08_job_script.html#job-scheduling-priority",
    "title": "Running jobs on Gadi",
    "section": "Job scheduling priority",
    "text": "Job scheduling priority\nOn Artemis, you will have some familiarity with the concept of fair share use, where compute jobs you run increase your project’s ‘fair share weight’ which temporarily decreases the priority of your jobs in the queue. This is not the case on Gadi, where all jobs have equal priority. The only factors that limit how quickly your jobs leave the queue and start running are the resources you request combined with current resource availability. In order to have your job be queued (and not ‘held’ after submission), you must have sufficient KSU in your project. This will be described under queue charge rates."
  },
  {
    "objectID": "notebooks/08_job_script.html#pbs-directives",
    "href": "notebooks/08_job_script.html#pbs-directives",
    "title": "Running jobs on Gadi",
    "section": "PBS directives",
    "text": "PBS directives\nPBS directives outline your job’s resource needs and execution details. Each directive starts with #PBS in order to directly communicate with the job scheduler and not be confused with other code or comments in your script. The directives section should sit at the top of your script, with no blank lines between them, and any commands required to perform your compute task follow below the last directive.\nBelow is a simple example of the PBS directives portion of a Gadi job script. For details on more options, please see the NCI Gadi PBS directives guide.\n#!/bin/bash\n\n#PBS -P aa00\n#PBS -q normal\n#PBS -l ncpus=48\n#PBS -l mem=190GB\n#PBS -l jobfs=200GB\n#PBS -l walltime=02:00:00\n#PBS -l storage=scratch/aa00+gdata/aa00\n#PBS -l wd\n\n-P: Project code for resource accounting. Must be a valid NCI project code of which you are a member\n-q: Queue selection (e.g., normal or hugemem). See Gadi’s queue structure and queue limits pages for more details\n-l ncpus: Number of requested CPUs\n-l mem: amount of requested memory\n-l jobfs: Local-to-the-node disk space on the compute node\n-l walltime: Requested job walltime. Your job will only be charged for the walltime it uses, not the maximum walltime requested\n-l storage: Filesystems your job will access. /scratch/&lt;project&gt; is accessible by default. To access any other scratch or gdata locations, list them here. Note to use no spaces or leading / characters\n-l wd: Set the working directory to the submission directory. This is equivalent to cd $PBS_O_WORKDIR"
  },
  {
    "objectID": "notebooks/08_job_script.html#differences-between-artemis-and-gadi-pbs-scripts",
    "href": "notebooks/08_job_script.html#differences-between-artemis-and-gadi-pbs-scripts",
    "title": "Running jobs on Gadi",
    "section": "Differences between Artemis and Gadi PBS scripts",
    "text": "Differences between Artemis and Gadi PBS scripts\n\nThe -l storage directive is required on Gadi but not Artemis. Failure to include required storage locations will kill the job, for example with No such file or directory errors\nOn Gadi, users must review their resource requirements against the queue structure and limits in order to request a specific queue. On Artemis, the scheduler managed this automatically according to requested resources and queue loads\nMaximum walltime for any queue is 48 hours. For large numbers of nodes requested in a single job, the maximum walltime reduces. This is described in the queue limits page. See Working within walltime limit for more details\nThe requested resources are checked against the quantity of remaining KSU in the project specified at -P. If there is insufficient KSU to run the job, the job will be held. This will show as H status when the job is queried with qstat. See queue charge rates for more details\nJob arrays (eg #PBS J 1-1000) are not permitted on Gadi. See Parallel jobs and nci-parallel for more details\nUnlike Artemis, Gadi compute nodes lack internet access. If you have a job script that relies on an external network call such as reading from a live database, you will need to adapt your method (for example pre-downloading the required information with copyq before running the compute job) or use an alternate platform such as Nirin\n\nBelow is an example Artemis job script:\n#!/bin/bash\n\n#PBS -P &lt;USyd project code&gt;\n#PBS -N myjobname\n#PBS -l walltime=02:00:00\n#PBS -l select=1:ncpus=4:mem=16gb\n#PBS -q defaultQ\n\nmodule load python/3.12.2\n\ncd $PBS_O_WORKDIR\n\npython3 ./myscript.py ./myinput\nThe same job script, adjusted for Gadi:\n#!/bin/bash\n\n#PBS -P &lt;NCI project code&gt;\n#PBS -N myjobname\n#PBS -l walltime=02:00:00\n#PBS -l ncpus=4\n#PBS -l mem=16GB\n#PBS -q normal\n#PBS -l storage=scratch/bb11+gdata/aa00+gdata/bb11\n#PBS -l wd\n\nmodule load python3/3.12.1\n\npython3 ./myscript.py ./myinput\nAs you can see, there is very little difference between these two scripts. They both request 4 CPUs, 16 GB RAM, and 2 hours walltime. They both change the working directory to the submission directory, they both load python (different versions as available on the system) and both run the same job command.\nThe command to submit this script is also the same on Artemis and Gadi:\nqsub run_my_python_script.pbs\nAdapting your existing Artemis job scripts to Gadi should be fairly simple for most users, beginning with adjusting the directives and establishing required software. See Software for more details on Gadi software availability."
  },
  {
    "objectID": "notebooks/08_job_script.html#selecting-the-right-queue",
    "href": "notebooks/08_job_script.html#selecting-the-right-queue",
    "title": "Running jobs on Gadi",
    "section": "Selecting the right queue",
    "text": "Selecting the right queue\nArtemis defaultQ routed jobs to the appropriate queue based on directives and resource avalability. Gadi requires users to directly specify the appropriate queue.\nTo select the queue, you match up the resources your job needs to the queue limits, also factoring in the charge rate.\nView the available queues on the Gadi queue structure page. Note that there are:\n\ngeneral purpose queues\nlarge memory queues\nexpress queues\nGPU queues\ndata transfer queue (copyq)\n‘Cascade Lake’ and ‘Broadwell (ex-Raijin)’ queues\n\nThe Cascade Lake nodes are newer hardware and thus faster than the Broadwell nodes. Raijin was the previous NCI HPC, decomissioned in 2019\nThey have a lower charge rate than the equivalent Cascade Lake queue, and this can be utilised to help minimise compute cost when the reduced processor speed is not overly detrimental to the job or your research timeline\nThey have different numbers of CPU (48 or 28) and different total memory per node\n\n\nEach queue has different hardware, limits, and charge rates. Before submitting any jobs on Gadi, it is important to review this page along with the queue limits page which describes each queue in more detail.\nYou will note that each queue also has a corresponding queue that ends in -exec. You cannot submit directly to the -exec (execution) queue. Your jobs will be placed on the execution queue via the ‘route queue’ that you submit to. For example, for a job you want to run on the Cascade Lake normal queue, you will include the directive #PBS -q normal (submit to route queue) and the job will run on normal-exec (execution queue).\n\nQueue charge rates\nBy now you should be familiar with the concept of an NCI service unit (SU, or sometimes KSU for 1,000 SU or MSU for 1 million SU).\nEach new NCI project under the Sydney Scheme is granted 1 KSU by default, and requests can be made for more as required.\nA service unit is based on a CPU hour, ie ‘one hour of walltime on one CPU’. Each queue has a different charge rate applied to the CPU hour, so that one CPU hour on a given resource may cost between 1.25 SU and 6 SU, depending on the charge rate for that queue. More specialised and scarce resources are charged at a higher rate to ensure that only users who genuinely need these use them.\nThe charge rates can be found in column 4 of the queue limits table.\nIt’s important to understand that requested memory also impacts the charge rate, not just the requested CPU, walltime and queue. In each queue, a CPU has an allocated amount of memory for accounting purposes. For example, in the Cascade Lake normal queue, there are 48 CPU and 192 GB total RAM. The amount of memory per CPU for accounting purposes is therefore 192 / 48 = 4 GB. If you request 1 CPU and 4 GB RAM, only the CPU affects the charge rate, as you are using only the memory allocation for one CPU. If however you request 1 CPU and 8 GB RAM, your charge rate will be based off 2 CPU of use, since you are using the memory allowance of 2 CPU. Note this is ‘for accounting purposes’ only, ie it is technically feasible for your job to run on 1 CPU and access 8 GB RAM (or more). This accounting is described on the NCI job costs page, and will also be sumamrised below.\nDon’t be alarmed by the charge rates: please submit your job to the most appropriate queue based on required resources. The accounting method combined with stricter walltimes, newer hardware and software, and more vast physcial resources compared to Artemis will likely see your compute jobs complete in a faster turnaround time compared to what you are used to.\nUnderstanding charge rates is important for two main reasons:\n\nJudicious use of resources. KSU is provided to you in-kind by The University of Sydney. It is your responsibility to ensure efficient use of these resources. Selecting the appropriate queue for your job avoids wastage and avoids unecessary impacts on other users of this national resource.\nEnsuring your job can run. Jobs can only leave the route queue and join the execution queue if sufficient SU are available to the project assuming the job runs for its full requested walltime.\n\nFor example, if your project has 1 KSU and you submit a job script with the following directives:\n#PBS -q hugemem\n#PBS -l ncpus=48\n#PBS -l mem=1470GB\n#PBS -l walltime=12:00:00\nYour job will not join the compute queue - it will be held, showing a status of H when qstat is run. The reason for hold status is that the requested job requires more service units than the project has available.\nYou can view your project budget with the following command:\nnci_account -P &lt;nci-project-code&gt;\nThis will show the total allocated for the current quarter, the amount used, the amount reserved (by running or queued jobs), and the amount available. Any new job you submit MUST request less than the amount available.\nThe required SU available to run the job can be calculated by the formula:\nwalltime-hours * MAX (CPU|MEM) * charge-rate\nwhere MAX is based on the greater value of CPUs or proportion of node memory requested.\nso for the above example:\n12 h * 48 CPU * 3 charge rate = 1728 SU\nIn this case, MAX is based on CPU, since the per-CPU memory request (1470 GB / 48 CPU = 30.625 GB) is less than or equal to the maximum proportion of memory per CPU on the hugemem queue.\nSince 1728 SU is more than the 1 KSU the project has available, the job cannot run. You will need to either:\n\nObtain more KSU\nReconfigure your job to fit under the 1 KSU you have available.\n\nYou might consider reducing walltime, CPUs, change the queue, etc, depending on your job and what you expect are its minimum viable resouce requests. You can do this by:\nKilling the job (qdel &lt;jobID&gt;), editing the directives and resubmitting, OR use the qalter PBS command to reduce the resource requests of the held job.\nFor the above example, let’s assume the requested walltime of 12 hours was an extremely conservative estimate and realistically you expect the job should complete in less than 2 hours. You could run this command:\nqalter -l walltime=02:00:00 &lt;jobid&gt; \nThis would reduce the SU for the submitted job to 288 SU and the job would then be picked up by the next scheduling cycle and enter the queue.\n\n\nQueue selection examples\n\nExample 1\nYou have a small job that only uses a single CPU and 2 GB RAM, but will run for a whole day. Which of the queues would be appropriate?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nnormal, normalbw, express, expressbw. While you could use the express queues, the charge rate is higher so the non-express normal queues would be more economical.\n\n\n\n\n\nExample 2\nYou have a job that requires 384 GB memory and 12 CPU. Which queue would you use?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nhugemem, with 12 CPU and 384 GB memory, or hugemembw, with 14 CPU as CPUs must be requested in multiples of 7 on this queue.\nWhich would be the better choice?\nIf the job ran for 2 hours, charge rate would be:\n\nhugemem: 12 CPU * 2 h * 3 charge rate = 72 SU\nhugemembw: 14 CPU * 2 h * 1.25 charge rate = 35 SU\n\nHugemem may execute faster with the newer hardware, yet hugemembw may consume less KSU. hugemembw also has more mem per CPU than hugemem (36 GB vs 32 GB). Benchmarking will demonstrate which of these configurations is more suited to your job.\nSee job efficiency for tips to determine the best compute resources for your job.\n\n\n\n\n\nExample 3\nYou have a job that requires 20,000 CPU. Fill in the below directives for this job, including the maximum permissable walltime:\n#PBS -l ncpus=&lt;value&gt;\n#PBS -l mem=&lt;value&gt;\n#PBS -l walltime=&lt;value&gt;\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n#PBS -l ncpus=20016\n#PBS -l mem=3803040GB\n#PBS -l walltime=05:00:00\nWhy 20,016? When requesting &gt;1 node on Gadi, only whole nodes can be requested. So to reach 20,000 CPU in a single job would require the use of the Cascade Lake normal queue, where the nodes have 48 CPU per node, and this would be 20,000 * 48 = 416.7 nodes, so we need to round up to 417 nodes, which is 417 * 48 = 20,016 CPU.\nWhy 5 hour walltime not 48 hours? As the quantity of CPU requested increases, maximum walltime goes down. This information can be found in the last column on the queue limts table. 5 hours is the maximum amount of walltime allowed for jobs requesting more than 3024 CPUs (63 nodes) in this queue. To request the maximum walltime of 48 hours on this queue, the job must request at most 672 cores (14 nodes).\nIf your job required exactly 20,000 CPU, you would simply provide this hard-coded value to the relevant command. The number of requested KSU to the job can be accessed from the environment variable $PBS_NCPUS.\n\n\n\n\n\nExample 4\nYour job requires GPUs. Which queues could you use?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ngpuvolta or dgxa100 queue"
  },
  {
    "objectID": "notebooks/08_job_script.html#lack-of-internet-access-on-compute-nodes",
    "href": "notebooks/08_job_script.html#lack-of-internet-access-on-compute-nodes",
    "title": "Running jobs on Gadi",
    "section": "Lack of internet access on compute nodes",
    "text": "Lack of internet access on compute nodes\nThe only Gadi queue with internet access is copyq. This queue is not suitable for running compute tasks. It allows only single-core jobs and has a maximum walltime of 10 hours. Jobs that require up-to-date information retrieval from external servers have a few options:\n\nSplit the job into two parts: a download or web query task submitted to copyq, ensuring that the retrieved data is saved to persistent disk (ie not the local-to-the-node SSD storage that is deleted upon job completion), followed by a a compute job submitted to one of the appropriate compute queues, reading in the requried inputs saved from job 1.\nRun the job via ARE, which provides a graphical user interface run on Gadi’s compute queues plus internet access capability.\nUse NCI’s Nirin cloud instead of Gadi."
  },
  {
    "objectID": "notebooks/08_job_script.html#submitting-a-pbs-script",
    "href": "notebooks/08_job_script.html#submitting-a-pbs-script",
    "title": "Running jobs on Gadi",
    "section": "Submitting a PBS script",
    "text": "Submitting a PBS script\nLike on Artemis, the qsub command is used to submit the job to the scheduler. Please visit the NCI Gadi job submisison page if you require more details on this.\nAfter your job is submitted, job monitoring and job logs are very similar to your experience on Artemis. Please see job monitoring for more details."
  },
  {
    "objectID": "notebooks/08_job_script.html#interactive-jobs",
    "href": "notebooks/08_job_script.html#interactive-jobs",
    "title": "Running jobs on Gadi",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for jobs that require user input feedback as an analysis progresses, or can be useful for testing commands/tools prior to submiting a full job via a PBS script.\nRunning an interactive job on Gadi is very similar to an Artemis interactive job: you provide the relevant directives on the command line rather than from within a script, and include -I instead of #PBS -q &lt;queue&gt;.\nFor example, to start an interactive job with 4 CPU for 1 hour, enter the following command on the Gadi login node:\nqsub -I -P &lt;nci-project-code&gt; -l walltime=00:01:00,ncpus=4,mem=16GB,storage=&lt;required-storage-paths&gt;,wd\nAfter you enter the command, you will receive a message\nqsub: waiting for job &lt;id&gt;.gadi-pbs to start\nOnce your interactive job has left the queue and started, you will receive a message\nqsub: job &lt;id&gt;.gadi-pbs ready\nNotice that your command prompt has changed, indicating the node ID you are on instead of the login node ID.\nYou can then interactively enter the commands required for your compute task. To terminate the interactive job, enter exit."
  },
  {
    "objectID": "notebooks/08_job_script.html#persistent-sessions",
    "href": "notebooks/08_job_script.html#persistent-sessions",
    "title": "Running jobs on Gadi",
    "section": "Persistent sessions",
    "text": "Persistent sessions\nTo support the use of long-running, low CPU and low memory demand processes, NCI provides a persistent sessions service on Gadi. This service is primarily designed for the use of workflow management tools (eg nextflow) that automatically submit and monitor PBS jobs to the Gadi compute queues.\nWorkflow management tools are a unique use case where the ‘head job’ requires internet access (provided through the persistent session) and access to the scheduler to submit a series of chained compute jobs to various queues depending on the unique workflow configuration.\nThis service is NOT designed for computational work, large downloads, or other intensive tasks. These jobs should be submitted to the appropriate PBS queues."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html",
    "href": "notebooks/09_job_monitoring.html",
    "title": "Monitoring your job",
    "section": "",
    "text": "You should monitor your jobs to keep track of their progress but refrain from checking your jobs too frequently. Repeated queries will be considered attacks, especially in quick succession and you may get a warning from NCI. NCI recommends querying your jobs’ status a maximum of once every 10 minutes.\nLike Artemis, you can use the qstat command to monitor jobs on Gadi. The NCI Gadi job monitoring page describes some commonly used flags to qstat.\nYou can also use the bespoke nqstat_anu utility (ANU = The Australian National University, where Gadi is housed) which provides a way of observing how much CPU and memory your job is currently using.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-monitoring",
    "href": "notebooks/09_job_monitoring.html#job-monitoring",
    "title": "Monitoring your job",
    "section": "",
    "text": "You should monitor your jobs to keep track of their progress but refrain from checking your jobs too frequently. Repeated queries will be considered attacks, especially in quick succession and you may get a warning from NCI. NCI recommends querying your jobs’ status a maximum of once every 10 minutes.\nLike Artemis, you can use the qstat command to monitor jobs on Gadi. The NCI Gadi job monitoring page describes some commonly used flags to qstat.\nYou can also use the bespoke nqstat_anu utility (ANU = The Australian National University, where Gadi is housed) which provides a way of observing how much CPU and memory your job is currently using."
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-ids",
    "href": "notebooks/09_job_monitoring.html#job-ids",
    "title": "Monitoring your job",
    "section": "Job IDs",
    "text": "Job IDs\nLike Artemis jobs, jobs submitted to Gadi are given a jobID. This is shown to you as soon as it has been accepted, for example 135615373.gadi-pbs.\nWhen querying the job with qstat, you can use the full ID, or just the string of numbers (omit the .gadi.pbs).\nFor example, the below two commands are equivalent:\nqstat -xf 135615373.gadi-pbs\nqstat -xf 135615373\nIf you have multiple jobs running, you do not need to check them individually with the job ID. You can check the status of multiple jobs using your NCI user ID:\nqstat -u &lt;nci-user-id&gt;"
  },
  {
    "objectID": "notebooks/09_job_monitoring.html#job-logs",
    "href": "notebooks/09_job_monitoring.html#job-logs",
    "title": "Monitoring your job",
    "section": "Job logs",
    "text": "Job logs\nBy default, the PBS job logs will be created in the directory from which the qsub command was entered, and combine the job name and the job ID.\nFor example, a job with #PBS -N convert and job ID 133703660 will have standard output and resource usage written to convert.o133703660 and standard error written to convert.e133703660.\nThis differs slightly to Artemis, which has the same default filepath behaviour except the standard output is sent to the .o and the resource usage is sent to .o_usage.\nThe Gadi .o file has the resource usage at the end of the log making it easy to view a quick summary with the tail command:\n$ tail -n 11 convert.o133703660 \n                  Resource Usage on 2025-02-05 16:16:10:\n   Job Id:             133703660.gadi-pbs\n   Project:            aa00\n   Exit Status:        0\n   Service Units:      23.94\n   NCPUs Requested:    7                      NCPUs Used: 7               \n                                           CPU Time Used: 02:57:33        \n   Memory Requested:   63.0GB                Memory Used: 51.14GB         \n   Walltime requested: 08:00:00            Walltime Used: 02:44:09        \n   JobFS requested:    100.0MB                JobFS used: 0B              \n======================================================================================\nIf desired, you can change the default log filepaths with the -o and -e directives, for example:\n#PBS -o ./logs/convert-fast5.o\n#PBS -e ./logs/convert-fast5.e\nThis omits the job ID from being included in the log file name and sends the logs to a different directory."
  },
  {
    "objectID": "notebooks/13_parralel_jobs.html",
    "href": "notebooks/13_parralel_jobs.html",
    "title": "Running parallel jobs on Gadi",
    "section": "",
    "text": "TO BE COMPLETED\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/13_parralel_jobs.html#introduction",
    "href": "notebooks/13_parralel_jobs.html#introduction",
    "title": "Running parallel jobs on Gadi",
    "section": "Introduction",
    "text": "Introduction\nIn this section, we will discuss how you can run parallel jobs on Gadi using the nci-parralel utility in place of the Artemis job array method.\nThe main challenges users may face adapting Artemis workflows to Gadi are:\n\nJob arrays not supported on Gadi\nGadi walltime limit of 48 hours\nAdjusting PBS directives to suit Gadi requirements and queue structure\nLack of internet access for Gadi compute nodes\nData transfer\nUnderstanding NCI accounting of KSU, disk and iNode limits\nAutomatic 100-day Gadi /scratch purge policy\nSoftware installation and version upgrades on Gadi\n\nIn this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date."
  },
  {
    "objectID": "notebooks/13_parralel_jobs.html#section",
    "href": "notebooks/13_parralel_jobs.html#section",
    "title": "Running parallel jobs on Gadi",
    "section": "",
    "text": "https://opus.nci.org.au/spaces/Help/pages/248840680/Nci-parallel…\nhttps://sydney-informatics-hub.github.io/training.gadi.intro/08-Example-parallel-job/index.html"
  },
  {
    "objectID": "notebooks/01_setup.html",
    "href": "notebooks/01_setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "Before accessing Gadi, you will need to have an NCI account. Ensure you have completed this step by following directions on the Gadi Access instructions on the previous page before proceeding.\nTo work on NCI Gadi, you will need to use a terminal application on your local computer or work on NCI’s Australian Research Environment (ARE) platform, which includes a web-based terminal interface to Gadi. If you already have a terminal application that you have used to access Artemis, for example putty or Mac term, you can continue to use that.\nBeliow we will describe 3 options for accessing Gadi:\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01_setup.html#use-ncis-are-platform",
    "href": "notebooks/01_setup.html#use-ncis-are-platform",
    "title": "Set up your computer",
    "section": "Use NCI’s ARE platform",
    "text": "Use NCI’s ARE platform\n\n\n\n\n\n\nARE: fast access with limited customisation\n\n\n\nThis is a very lightweight solution for accessing Gadi, some interactive tools like Jupyter and RStudio. We recommended it for beginners who don’t want to customise their set up.\n\n\nNCI has created a web-based graphical interface for accessing their systems. It is very simple to use and recommended over access methods described below for beginners.\nSee NCI’s User Guide for instructions on how to access and use ARE."
  },
  {
    "objectID": "notebooks/01_setup.html#install-visual-studio-code",
    "href": "notebooks/01_setup.html#install-visual-studio-code",
    "title": "Set up your computer",
    "section": "Install Visual Studio Code",
    "text": "Install Visual Studio Code\n\n\n\n\n\n\nVScode: customised configuration with an integrated terminal\n\n\n\nThis is a more advanced solution for accessing Gadi, with more customisation options. We recommended it for users who are comfortable with terminal applications and want to customise their set up.\n\n\nVisual Studio Code (VS Code) is a lightweight and powerful source code editor available for Windows, macOS and Linux computers. As an alternative to a terminal application it offers additional functionality including file editing.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\nConnect to your instance with VS code by adding the host details to your .ssh config file:\nHost Gadi\n  HostName gadi.nci.org.au\n  User &lt;your-nci-username&gt;\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host and Gadi\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your NCI password and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you’re running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /scratch/iz89 to open your workspace. You can change this at any point by opening a new folder. Keep in mind you will be requested to provide your password each time.\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‘home’ then click Yes, I trust the authors\nTo open a terminal, type Ctrl+J if you’re on a Windows machine or Cmd+J on MacOS\n\nTips for using VS Code\n\nPeriodically delete your ~/.vscode-server/ directory on Gadi! VSCode will fill this directory with numerous files and cause your home quota to be exceeded\nVS code cheatsheet for Windows\nVS code cheatsheet for MacOS"
  },
  {
    "objectID": "notebooks/01_setup.html#use-macos-native-terminal",
    "href": "notebooks/01_setup.html#use-macos-native-terminal",
    "title": "Set up your computer",
    "section": "Use MacOS native terminal",
    "text": "Use MacOS native terminal\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‘terminal’. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2."
  },
  {
    "objectID": "notebooks/01_setup.html#install-a-terminal-client-on-your-windows-machine",
    "href": "notebooks/01_setup.html#install-a-terminal-client-on-your-windows-machine",
    "title": "Set up your computer",
    "section": "Install a terminal client on your windows machine",
    "text": "Install a terminal client on your windows machine\nWindows OS now comes with Windows Subsystem for Linux (WSL) so if you are familiar with using that, you can ssh to Gadi from a WSL terminal.\nIf not, you will need to install a terminal client. There are many options, including putty, Xwin-32 (which the University of Sydney has a license for), and MobaXterm.\nBelow we will describe the process to install the free version of MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‘Home Edition’ select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‘start local terminal’ (and install Cygwin if prompted)\n\n\nTo log in to Gadi, you will use a Secure Shell (SSH) connection. To connect, you need 3 things:\n\nThe address of your NCI Gadi, gadi.nci.org.au.\nYour Gadi username, e.g. ab1234.\nYour password.\n\nTo log in: type the following into your terminal, using your allocated instance’s IP address:\nssh &lt;username&gt;@gadi.nci.org.au\nThen provide your password when prompted.\n\n\n\n\n\n\n‼️ Pay Attention ‼️\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You’ll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nOnce you’ve logged in successfully, you should see a welcome screen like this:"
  },
  {
    "objectID": "notebooks/04_command_line.html",
    "href": "notebooks/04_command_line.html",
    "title": "Command line environment",
    "section": "",
    "text": "Working on Gadi will require you to have reasonable confidence on the Linux/Unix command line.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/04_command_line.html#linux-command-line-training",
    "href": "notebooks/04_command_line.html#linux-command-line-training",
    "title": "Command line environment",
    "section": "Linux command line training",
    "text": "Linux command line training\nNCI host a number of training sessons, including introductory Linux. Please visit their training calendar.\nThere are numerous self-directed training tutorials online, such as this one from Sandbox.bio.\nA bash cheatsheet such as this one from NCI or this one can also be helpful in increasing your familiarity with the command line.\nDue to the plethora of high-quality introductory command line materials available online, we will not reproduce those here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NCI for USyd researchers",
    "section": "",
    "text": "National Computational Infrastructure (NCI) is a services facility that provides high performance computing (HPC), cloud computing, and data services to Australian researchers.\nOur insitutional HPC ‘Artemis’ will be de-comissioned in August 2025. NCI has been chosen as the replacement computing platform. All staff and students with a valid University of Sydney unikey can access NCI computing time under the NCI-Sydney Scheme.\nIf HPC is not suitable for your workload, please consider NCI cloud or virtual research desktops. We are confident that your research computing will be well-supported by these platforms.\nThis site is focused on the use of NCI’s Gadi HPC, in the context of University of Sydney researchers.\nPlease be mindful this is not an exhaustive resource for using Gadi. It is only intended to orient you to the system and navigate the comprehensive NCI user documentation.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "NCI for USyd researchers",
    "section": "Support",
    "text": "Support\nSIH is limited in the support it can provide for NCI Gadi users. If you are new to HPC and Gadi, we expect you will attend NCI’s Intro to Gadi courses. Additionally, familiarise yourself with Gadi using the NCI Gadi user guide.\nLive and self-directed training is also offered by SIH and NCI on HPC and data analysis topics:\n\nSIH training calendar\nNCI training calendar\n\nFor additional support, please contact the following people depending on your needs:\n\n\n\nType of issue\nWho\nHow\nDetails\n\n\n\n\nService unit allocation for running jobs. Request for g/data storage.\nSIH\nMake a request\nSydney Documentation\n\n\nA techincal issue with NCI. An error in your running job. New software installation.\nNCI Helpdesk\nLog an NCI ticket\nProvide your error, log file, jobid. Versions or links for installs.\n\n\nA bug in an SIH pipeline\nSIH\nSubmit an issue on Github\nProvide details e.g. this issue\n\n\nGeneral research computing or bioinformatics advice\nSIH\nLog an SIH ticket\nProvide relevant context, errors, tool names, scripts"
  }
]