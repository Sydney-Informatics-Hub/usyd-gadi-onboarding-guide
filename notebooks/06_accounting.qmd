---
title: "**Accounting**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

Coming soon! For now, see: https://opus.nci.org.au/spaces/Help/pages/236881132/Allocations...


**TO BE COMPLETED** 


## Introduction

The main challenges users may face adapting Artemis workflows to Gadi are:

- Understanding NCI accounting of KSU, disk and iNode limits
- [Gadi walltime limit of 48 hours](./12_walltime.qmd)
- [Adjusting PBS directives to suit Gadi requirements and queue structure](./08_job_script.qmd)
- [Lack of internet access for Gadi compute nodes](./08_job_script.qmd#lack-of-internet-access-on-compute-nodes)
- [Data transfer](./05_data_transfer.qmd)
- [Automatic 100-day Gadi /scratch purge policy](https://opus.nci.org.au/spaces/Help/pages/241926268/Recover+Files...#RecoverFiles...-RecoverQuarantinedFilesonscratch)
- [Software installation and version upgrades on Gadi](./11_software.qmd) 
- [Job arrays not supported on Gadi](./13_parralel_jobs.qmd) 

In this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date.

## What are the important accounting metrics? 

1. Service units. The charge rate per resource hour. 
2. Disk (scratch, home, gdata). This refers to the amount of physical disk space used.   
3. iNodes (scratch, home, gdata). Data structures used to store metadata about files and directories; NCI imposes an iNode limit. 

## Service units 

For a detailed description of the NCI SU and why 1 SU does not equal 1 CPU hour, see [here](https://opus.nci.org.au/spaces/Help/pages/119964320/Definition+Service+Unit+SU)

For every resource hour you consume on an NCI compute platform, you are charged at a specific rate. As a simple example the `normal` queue on Gadi has a charge rate of 2, so a job using 1 CPU for 1 hour will be charged 2 SU. 

We often speak in KSU or 1,000 SU for simplicity. Under the Sydney Scheme, you can easily request more KSU as you need it from the [management portal](http://nci.sydney.edu.au). 

Before running a job on Gadi, it is important to:

1. Calculate the amount of KSU the job will require
2. Checking your available KSU

### Calculate the amount of KSU the job will require 

In the simple example above, we decided that our 1 CPU 1 hour `normal` queue job would cost 2 SU. There is actually another factor to consider, and that is memory. Each queue has nodes of a specific CPU:memory ratio, and if your job consumes more memory than this ratio allows, you will be charged based on the memory used. From the [NCI job costs page](https://opus.nci.org.au/spaces/Help/pages/236880942/Job+Costs...):

*"However, some jobs will request less CPUs and more memory. When this happens, you are taking memory away from the other CPUs in the node and will be charged accordingly, as other users can’t access those CPUs while your job is using that memory allocation"*

So let's assume our 1 CPU job requests 12 GB memory. Checking the [Gadi queue structure](https://opus.nci.org.au/spaces/Help/pages/236880996/Queue+Structure+on+Gadi...) page, under the `Intel Xeon Cascade Lake` dropdown, the `normal` nodes have the following hardware:

- 2 x 24-coreIntel Xeon Platinum 8274 (Cascade Lake) 3.2 GHz CPUs per node
- 192 GiB RAM per node

From here we can determine that the CPU:mem ratio is 48:192 = 1:4 (1 CPU per 4 GB mem). By requesting 12 GB mem for our 1 CPU job, we are using 3 times as much mem per CPU as the ratio governs for this queue. So, we are likewise charged 3 times as much, or in other words, we are charged based on the MEMORY rather than the CPU. 

The equation for every job run on Gadi is charged using the formula:

```default
SU = Queue Charge Rate  ✕  Max (NCPUs, Memory Proportion)  ✕  Walltime Used (Hours)
```

For our example, this expands to:

```default
SU = 2 charge rate X 3 memory proportion X 1 hour
SU = 6
```

#### Challenge 1: calculate this job's SU cost

Use the [queue limits](https://opus.nci.org.au/spaces/Help/pages/236881198/Queue+Limits...) and [queue structure](https://opus.nci.org.au/spaces/Help/pages/236880996/Queue+Structure+on+Gadi...) pages to help find the answer. 

```bash
#PBS -P MYPROJECT
#PBS -l walltime=02:00:00
#PBS -l ncpus=4
#PBS -l mem=48GB
#PBS -q normal
#PBS -W umask=022
#PBS -l wd
#PBS -l storage=gdata/MYPROJECT
```

::: {.callout-note collapse="true"}
## Answer

First, we need to calculate the CPU:mem ratio of our job, which in this case is 4:48 = 1:12, ie 12 GB mem per CPU. 

Next, we need to check that against the queue hardware: The `normal` queue has a charge rate of 2 SU per resource hour. These nodes have 48 CPU and 192 GB RAM. So the CPU:mem rato for this queue is 48:192 = 1:4 ie 4 GB RAM per CPU. By requesting 12 GB mem per CPU, we have requested *3 times as much mem per CPU* than the queue hardware provides for, so we will be charged based off the mem not the CPU. 

We will be charged based on 48 GB requested / 4 node ratio  = 12 memory proportion.  

```default
SU = 2 charge rate X 12 memory proportion X 2 hour
SU = 48
```
:::


#### Challenge 2: calculate this job's SU cost

Use the [queue limits](https://opus.nci.org.au/spaces/Help/pages/236881198/Queue+Limits...) and [queue structure](https://opus.nci.org.au/spaces/Help/pages/236880996/Queue+Structure+on+Gadi...) pages to help find the answer. 

```bash
#PBS -P MYPROJECT
#PBS -l walltime=04:00:00
#PBS -l ncpus=280
#PBS -l mem=2520GB
#PBS -q normalbw
#PBS -W umask=022
#PBS -l wd
#PBS -l storage=gdata/MYPROJECT
```

::: {.callout-note collapse="true"}
## Answer

First, we need to calculate the CPU:mem ratio of our job, which in this case is 280:2520 = 1:9, ie 9 GB mem per CPU. 

Next, we need to chech that against the queue hardware: The `broadwell` queues (except express) have a charge rate of 1.25 SU per resource hour. The directives have requested the `normalbw` or "normal broadwell" queue, which has some nodes with 128 GB RAM and some nodes with 256 GB RAM. There is no difference in the charge rate for these nodes, and jobs are placed on either queue depending on both memory per CPU requested and resource availability. These nodes have 28 CPU per node. So the CPU:mem rato for this queue is *either* 28:128 *or* 28:256. Since the 256 GB nodes are charged the same as the 128 GB nodes, we go with the higher ratio, 28:256 = 1:9.14 ie 9.14 GB mem per CPU. 

Now we know that our job has requested *less mem per CPU than the queue allows per CPU* so our charge rate will be based on CPU not mem. 

```default
SU = 1.25 charge rate X 280 CPU X 4 hour
SU = 1400
KSU = 1.4
```
:::

### Check your available KSU

Once you have established your job script/s and calculated your KSU cost, you should check that you have sufficient KSU budget to rub the job.

Use the below command, replacing the `<nci-project-id>` with your NCI project code:

```bash
nci_account -P <nci-project-id>
```

In the below screenshot, you can see that project `qc03` has used 3.24 SU of its 1 KSU allocation (Grant). There are no SU reserved (ie assigned to jobs that are currently running on in queue) and there are 996.76 SU still available for the rest of the quarter. 
 
![](../fig/nci_account-P.png)

In order to run a job under this project code, it would need to have a computed job cost of less than or equal to 996.76 SU. 

If a job is submitted under a project that does not have sufficient SU to cover the expected cost, the job will be held. You can reveal the reason for a held job (status = 'H') from the "Comment" included in the `qstat -f` output:

![](../fig/gadi-job-insufficient-su.png)

This job requires 19.2 KSU yet we know only 966 SU is available to the project. To run this job, you would need to [obtain more KSU](https://sydneyuni.atlassian.net/wiki/spaces/RC/pages/3448733944/NCI-Sydney+Scheme#Additional-Service-Unit-Requests). 

If the amount by which your job exceeds your current SU budget is small, you may consider reducing the requested resources to fit under the current budget. You can do this by either killing the job (`qdel <jobID>`), adjusting the resource requests then resubmitting, *or* use the `qalter` PBS command to reduce the resource requests of the held job via the command line. For example, to reduce CPU, mem and walltime:

```bash
qalter -l walltime=02:00:00 -l ncpus=48 -l mem=190GB <jobid> 
```

## Disk

## iNodes

An inode (index node) is a data structure used by Unix-like file systems to store metadata about a file or directory. In simple terms, each file and directory contributes an inode count of 1, ie the inode of a filesystem is the count of all files and directories stored on that filesystem. Like physical disk space, a filesystem also has an inode limit, and if they get used up (even if there's still free disk space), you won’t be able to create new files.

Each filesystem on Gadi has an attached quota for physical disk space ***AND*** inode limit. You can see the inode limits and usage for your project's scratch and gdata with the `nci_account` command:

![](../fig/nci_account-P-inode.png)

This project has been allocated (iAllocation) 4.5 million inodes in scratch, and has used (iUsed) 1.59 million inodes. With ~ 3 million inodes spare, there is plenty to continue work. 

Another way to check the inode limit (as well as disk space) is with the `lquota` command: 




