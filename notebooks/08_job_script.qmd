---
title: "**Gadi job script**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

- [PBS directives](https://opus.nci.org.au/spaces/Help/pages/236881349/PBS+Directives...)
- [Job submission tutorial](https://opus.nci.org.au/spaces/Help/pages/241927319/Job+Submission+Tutorial...)

## Introduction to PBS scripts

Like Artemis, NCI runs the [Altair PBS professional workload manager](https://altair.com/pbs-professional). 

While you can run simple commands on the login nodes on Gadi, there are limits to the resources you can access here. Commands that exceed CPU, memory and walltime restrictions are killed. 

As such, you must run complex or resource-intensive tasks on Gadi's job queues. To do this, you need to create a PBS (Portable Batch System) submission script that specifies your job's compute requirements along with the commands to execute the tasks. 

PBS scripts are text files that contain directives and commands that specify the resources required for a job and the commands to run. Typically they are named `<script_name>.pbs` however the `.pbs` suffix is not required, merely helpful to discern the intention of the script. 

Once submitted to the PBS job scheduler with the `qsub` command, the scheduler reads the compute requirements from the `directives` component of the script, and either runs your job right away (if the requested resources are available) or queues the job to run later (if the requested resurces are not currently available). 

## PBS directives

PBS directives outline your job's resource needs and execution details. Each directive starts with `#PBS` in order to directly communicate with the job scheduler and not be confused with other code or comments in your script. Th directives section should sit at the top of your script, and any commands required to perform your compute task follow below.  

For example:

```default
#!/bin/bash

#PBS -P aa00
#PBS -q normal
#PBS -l ncpus=48
#PBS -l mem=190GB
#PBS -l jobfs=200GB
#PBS -l walltime=02:00:00
#PBS -l storage=scratch/aa00+gdata/aa00
#PBS -l wd
```

- `-P`: Project code for resource accounting
    - Must be a valid NCI project code of which you are a member
- `-q`: Queue selection (e.g., normal or hugemem)
    - See Gadi's [queue structure](https://opus.nci.org.au/spaces/Help/pages/236880996/Queue+Structure+on+Gadi... and [queue limits](https://opus.nci.org.au/spaces/Help/pages/236881198/Queue+Limits...) pages for more details
- `-l ncpus`: Number of CPUs
- `-l mem`: Memory allocation
- `-l jobfs`: Local-to-the-node disk space on the compute node
- `-l walltime`: Maximum job duration
- `-l storage`: Filesystems your job will access
    - `/scratch/<project>` is accessible by default. To access any other gdata or scratch locations, list them here. Note to use no spaces or leading `/` characters
- `-l wd`: Set the working directory to the submission directory
    - This is equivalent to `cd $PBS_O_WORKDIR`


## Differences between Artemis and Gadi PBS scripts

- The `-l storage` directive is required on Gadi but not Artemis. Failure to include required storage locations will kill the job, for example with `no such file or directory` errors
- On Gadi, user must review their resoure requirements against the queue structure and limits in order to request a specific queue. On Artemis, the scheduler managed this automatically according to requested resources and queue loads
- Maximum walltime for any queue is 48 hours. For large numbers of nodes requested in a single job, the maximum walltime reduces. This is described in the [queue limits](https://opus.nci.org.au/spaces/Help/pages/236881198/Queue+Limits...) page. 
    - See [Working within walltime limit](./NN_walltime_limit.qmd) for more details 
- The requested resources are checked against the quantity of remaining KSU in the project specified at `-P`. If there is insufficient KSU to run the job, the job will be held. This will show as `H` status when the job is queried with `qstat`. 
    - See [Accounting](./06_accounting.qmd) for more details 
- Job arrays (eg `$#PBS J 1-1000`) are not permitted on Gadi 
    - See [Parallel jobs](./NN_parallel_jobs.qmd) and [nci-parallel](https://opus.nci.org.au/spaces/Help/pages/248840680/Nci-parallel...) for more details
- Unlike Artemis, Gadi compute nodes lack internet access. If you have a job script that relies on an external network call such as reading from a live database, you will need to adapt your method (for example pre-downloading the required information with `copyq` before running the compute job) or use an alternate platform such as [Nirin](https://opus.nci.org.au/spaces/Help/pages/152207472/Nirin+Cloud+User+Guide)


Below is an example Artemis job script: 

```bash
#!/bin/bash

#PBS -P aa00
#PBS -N myjob
#PBS -l walltime=02:00:00
#PBS -l select=1:ncpus=4:mem=16gb
#PBS -q defaultQ

module load python/3.12.2

cd $PBS_O_WORKDIR

python3 ./myscript.py ./myinput
```

The same job script, adjusted for Gadi:

```bash
#!/bin/bash

#PBS -P aa00
#PBS -N myjob
#PBS -l walltime=02:00:00
#PBS -l ncpus=4
#PBS -l mem=16GB
#PBS -q normal
#PBS -l storage=scratch/bb11+gdata/aa00+gdata/bb11
#PBS -l wd

module load python3/3.12.1

python3 ./myscript.py ./myinput
```

As you can see, there is very little difference between these two scripts. 

Adapting your existing Artemis job scripts to Gadi should be fairly simple for most users, beginning with adjusting the directives and establishing required software. See [Software](./11_software.qmd). for more details on this. 



## __2. Load Necessary Modules__

Gadi uses the module system to manage software applications that are installed in `/apps`. If you're using one or more tools installed in `/apps` you must load the appropriate modules for your workflow before running your script. Modules ensure you’re using the correct software versions and their dependencies. This should go directly under the PBS directives and before the commands to run your application.

For example:

```bash
module load python3/3.7.4
module load gcc/9.2.0
```

To find available modules:
- Use `module avail` to list all available software.
- Use `module spider <software>` to search for a specific software package and its versions.

## __3. Execute your chosen application__

Specify the commands to run your application or script within the job script. Ensure all file paths and environment variables are correctly set. Redirect output and error messages to log files for easier debugging. 

Gadi supports running both native software and containerized applications using **Singularity**. Containers are a convenient way to use pre-built environments. Here are examples for different scenarios:

#### **Running a custom script**

If you’re running a Python script:

```bash
python3 your_script.py > output.log 2>&1
```
- `> output.log`: Redirects standard output to a log file.
- `2>&1`: Redirects standard error to the same log file.

#### **Running a compiled program**

If you’re running a pre-compiled executable:

```bash
my_program input_data.txt > results.log 2>&1
```

#### **Running a Singularity Container**

Singularity is used to execute containerised applications on Gadi. Here's an example using a **FoldSeek** container from [quay.io](https://quay.io/repository/biocontainers/foldseek):

First, you'll need to get a copy of the container you want to run as Gadi job queues do not have external network access and cannot download anything from the internet. 

Run the following commands to download the container:

```bash
module load singularity
singularity pull docker://quay.io/biocontainers/foldseek:0.6.2--h779adbc_0
```

This will create a `.sif` (Singularity Image File) in the current directory, **`docker://`**: Pulls the container from Quay.io or DockerHub directly:

```bash
foldseek_0.6.2--h779adbc_0.sif
```

Add the Singularity commands to your job script. For example:

#### **Redirecting Output in Singularity**
To capture output:
```bash
singularity exec \
    foldseek_0.6.2--h779adbc_0.sif \
    foldseek easy-search input.fasta \
    database \
    output \
    foldseek_result.tsv > output.log 2>&1
```

## __4. Submit your job__

Save your job script with a `.sh` or `.pbs` extension and submit it using the qsub command:

```bash
qsub job_script.sh
```

After submission, PBS assigns a unique job ID, which you can use to monitor your job.

## Full script example 

Here is a very simple example of a full script that runs a Foldseek container on Gadi's normal CPU queue: 

```bash
#!/bin/bash

#PBS -P aa00
#PBS -q normal
#PBS -l ncpus=4
#PBS -l mem=10GB
#PBS -l jobfs=200GB
#PBS -l walltime=02:00:00
#PBS -l storage=scratch/aa00+gdata/aa00
#PBS -l wd

# Load modules
module load singularity

# Run FoldSeek
singularity exec \
    /scratch/aa00/foldseek_0.6.2--h779adbc_0.sif \
    foldseek easy-search /scratch/aa00/input.fasta \
    /scratch/aa00/database \
    /scratch/aa00/output \
    /scratch/aa00/foldseek_result.tsv > /scratch/aa00/output.log 2>&1
````