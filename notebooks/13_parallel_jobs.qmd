---
title: "**Running parallel jobs on Gadi**"
output:
  html_document:
    toc: false
    toc_float: false
from: markdown+emoji
---

**TO BE COMPLETED** 


## Introduction

In this section, we will discuss how you can run parallel jobs on Gadi using the `nci-parallel` utility in place of the Artemis job array method. 

The main challenges users may face adapting Artemis workflows to Gadi are:

- Job arrays not supported on Gadi
- [Gadi walltime limit of 48 hours](./12_walltime.qmd)
- [Adjusting PBS directives to suit Gadi requirements and queue structure](./08_job_script.qmd)
- [Lack of internet access for Gadi compute nodes](./08_job_script.qmd#lack-of-internet-access-on-compute-nodes)
- [Data transfer](./05_data_transfer.qmd)
- [Understanding NCI accounting of KSU, disk and iNode limits](./06_accounting.qmd)
- [Automatic 100-day Gadi /scratch purge policy](https://opus.nci.org.au/spaces/Help/pages/241926268/Recover+Files...#RecoverFiles...-RecoverQuarantinedFilesonscratch)
- [Software installation and version upgrades on Gadi](./11_software.qmd) 


In this section, we will look at the first challenge on this list. For the remaining challenges, please visit the specific linked content. We will run training sessions on some of these during the lead up to the Artemis decomission date.

## Parallel jobs

A parallel job is a job that executes across multiple processors, nodes, or cores simultaneously to speed up computations. This may be operating the same analysis over different input files, over different parameter values, or by dividing a large computational task into smaller subtasks that run concurrently, either through shared memory or distributed computing.

On Artemis, we could use the `#PBS -J <range>` directive to submit arrays of parallel jobs. On Gadi, **job arrays are not supported**. In order to simplify parallel jobs without the use of arrays, NCI have created a helper utility [`nci-parallel`](https://opus.nci.org.au/spaces/Help/pages/248840680/Nci-parallel...) which uses [OpenMPI](https://www.open-mpi.org/) for distributed computing of parallel tasks on Gadi.   

## Using nci-parallel

Use of this utility will require 3 job files:

1. PBS script that launches the parallel job with the requested overall size (how many nodes/CPUs) plus the details of how many CPUs to assign to each task, and how many tasks to run concurrently
2. The input arguments file or command file. This is a text file that provides a new set of arguments per line. Each line of the input arguments file is used by a separate execution of the task script
3. The task script. This is the script file that contains the commands to run the job and uses the arguments provided by the input arguments file.
 

NCI provides an [example](https://opus.nci.org.au/spaces/Help/pages/248840680/Nci-parallel...) where the input to the task script (in this case, named `test.sh`) is a single numeric value. The command `test.sh <value>` is included in the input argument file, as many times as needed for the job (in this case, 1000). Another way to do this (demonstrated in the [example below](TBA)) is having an input argument list that *does not include* the name of the script file. The name of the script file is instead included within the PBS script. Both methods achieve the same result, the user can choose whichever they prefer.

**Option 1: task script call with arguments provided in a command file**


**Option 2: task script call combined with arguments file within the PBS script**


Care must be taken to ensure the task script reads in the input arguments correctly. **Always run a single task small test before submitting the full parallel job!** 

It is also important to check that the `binding` is done correctly. **TBA...**





## Parallel job efficiency 

[Benchmarking](./10_job_efficiency.qmd) should initially be done on the single task, in order to optimise resource requests for the full job. In some cases, efficiency can decline for parallel jobs, and this results in an increased walltime per task and overall SU cost for the job. Where possible, benchmarking tests should also be performed at scale, to assess any decline in performance when the workload is scaled up. This can enable you to further optimise, or allow for the decline in efficiency when requesting walltime, to prevent avoidable job failures due to running out of walltime. 

For example, you have determined the optimum resources for your tasks to be 12 CPU and 48 GB mem, for justunder 20 minutes. You have 1,000 tasks to run. This equates to 250 `normal` nodes. Prior to submitting a 20 minute 250-node job, you should:

1. Test 4 tasks in a parallel job - that's one full node. Does the CPU efficiency remain the same as benchmarked for the single task?
2. Test a handful of nodes, say 10 - does the CPU efficiency remain the same as bencharmked, now 40 tasks are runnning concurrently?
3. If so, go ahead and submit the remaining tasks (remember to remove the tasks already run through this benchmarking, if you have done this on full size inputs). If not, what happened to the walltime? If the efficiency loss was drastic and walltime much worse, consider reoptimising. If the decline was minimal, extrapolate from this to estimate the walltime required for the full job, potentially running a larger task lot first (for example the next 100 tasks in the input argument list) for more accurate extrapolation.  
4. Keep records of the benchmark runs and full run resources for future reference. We recommend the [Gadi usage script](https://github.com/Sydney-Informatics-Hub/HPC_usage_reports/blob/master/Scripts/gadi_usage_report.pl) which will summarise compute resources into a tab delimited table for ease of review and record keeping. If you prefer to rely on your Gadi job logs alone for long term records of your jobs usage, please ensure to back these up to RDS as they will be purged from scratch in time.  

This may sound like a lot of extra work, but it could save a large amount of KSU as well as walltime and your own frustration in the long run. Understanding how well your workload scales to a parallel job is important prior to submitting a large multi-node job like this.  

## One disadvantage of nci-parallel

In job arrays, each subjob of the array is a discrete task consuming a chunk of resources that may or may not be on the same node as other subjobs of the array. When that subjob completes, those resources are returned to the cluster and become available for other jobs. As such, they are no longer consuming CPU hours and contributing to the users account. In contrast, `nci-parallel` reserves the entire collection of resources *for all parallel tasks in the job* as a single large chunk. This means that tasks that have completed while other tasks are still running are still consuming those resources and thus still accruing service units spent. Because of this, parallel jobs where there is a large distrubution of walltime across the tasks are likely to result in poor efficiency and a lot of wasted resources. 

There are a number of strategies you can employ to mitigate this:

1. timeout - see https://opus.nci.org.au/spaces/Help/pages/248840680/Nci-parallel...
2. for small numbers of parallel tasks, use a loop, with varying walltime based on expecttations for the task, adding a sleep within the submission loop 
3. sort tasks by epected order of walltime, longest to shortest, and request resources such that less than the total number of tasks are running in parallel
4. group tasks, for example instead of one large job, run a "fast", "medium" and "slow" job with the tasks grouped by expected walltime 


## Example parallel jobs

### Example 1: Genomics

Parallel processing in genomics can faciliate rapid processing of terabytes of raw DNA sequencing data. These workloads are highly amenable to parallel processing for 2 reasons:

1. The same processing steps typically need to be run on many different samples
2. Most of these processing steps can be further broken apart into smaller subtasks

By parallelising at the level of sample, we may have say 10 or 100 tasks in a parallel job, depending on the number of samples in the analysis. Yet if we break the processing steps each into smaller subtasks, we can then parallelise to a much higher level of throughput, `number of samples * number of subtasks`. 

One excellent example for how HPC and distributed computing can massively speed up processing is the case of mapping the DNA sequnces to the reference genome. In sequencing projects, each sample may have a few hundred million DNA sequences. Users typically map all of these in one job, that may take several hours and can multi-thread up to all of the cores on a node natively. However, since each DNA sequence is mapped *independently* to the reference, we can physically split the input into numerous smaller inputs and perform the mapping in parallel. 

Say we have 100 samples, each with approximately 400 million sequences each. To parallelise by sample, we could run 100 tasks concurrently. If we split the input into chunks of 10 million sequences, each sample would have 40 inputs, so we could run a parallel job with `100 * 40 =  4,000` tasks. If each task required 5 minutes on 6 CPU, that's `6 * 4000 = 24,000 CPU = 500 nodes` for 5 minutes to run all tasks concurrently. This exceeds the maximum nodes per job of 432 for the `normal` queue. We could run 2,000 tasks concurrently, and this would require double the walltime for half the nodes, so 10 minutes on 250 nodes. 250 nodes is a large chunk of resources, so possibly 1,000 tasks concurrently for 20 minutes on 125 nodes would be easier to schedule and result in less queue time. This is a massive speedup compared to the ~ 6-8 hours required to process all 400 million sequences in one task. 

Now how would we set this up? First up would be splitting the inputs, then running the mapping job, then performing a merge to per-sample final output. We won't cover the details of splitting or merging here as that is domain-specific, but we will look at the trio of scripts to demonstrate how the parallelisation is created. 

Given the large number of tasks and complexity of the metadata, we find an additional script is required: we call this the `make_inputs` script, which reads your metadata and creates the input arguments file. We then have a set of 4 scripts that are inextricably linked for the job. A script to make the input arguments list if not required for all jobs, only when the arguments are complex. Often, a simple loop is sufficient, for examnple to iterate over a range of numbers or files. 

In the example scripts below, the metadata has been simplified for clarity. 

For this workflow, only the make inputs file would be edited each time the analysis was run on a new set of input samples, and the PBS script directives would be adjusted based on the number of tasks to be run. The input arguments file is created by the make inputs script, and the task script would not need to be edited at all, as long as no changes to the tool commands were required for example updated tool arguments, parameters etc. 


#### Example scripts

TBA demo 4 scripts 

#### Testing it before you run the full job 

eg just run it on 4 tasks and print out the command (wrap in printf for example )

#### Checking the parallel job

- check outputs - right numberm, right name, right format, right size
- check PBS .o for exit 0 and expected mem, time, cpu usage
- check .e log file and how to check it re "exited with status 0" 


### Example 2: Calculation of pi

![](../fig/calc_pi.png)

The script whatispi.py runs a simulation on estimating the number pi given a set of numbers - The higher the simulation number the closer the result should be to the true value of pi.

```python
import numpy.random as rng
import time
import sys
from multiprocessing import Pool
from numpy import power

def pi_run(num_trials):
    """Calculate pi based on points with a box and circle of radius 1
    """
    r = 1.0
    r2 = r * r
    Ncirc = 0
    for _ in range(num_trials):
        x = rng.random()
        y = rng.random()
        if ( power(x,2) + power(y,2) < r2 ):
            Ncirc +=1
    pi = 4.0 * ( Ncirc / num_trials )
    return pi

def cycle_pi(trials, noprocs=1):
    """calculate pi based on a set of trials.
    -- trials: a large number representing points
    -- noprocs: number of python processes used in simulation
    """
    print("number of processes used: ", noprocs)
    with Pool(noprocs) as pool:
        pi_estimates = pool.map(pi_run, trials)

    return pi_estimates

if __name__ == "__main__":

    trials = list(map(int, sys.argv[2:]))
    print("estimating pi based on", trials)
    print(cycle_pi(trials, int(sys.argv[1])))
```

Within the `whatispi.py` file above, some basic concepts that use the Python multiprocessing library are:

the `Pool(noprocs)` object creates a pool of processes. processes is the number of worker processes to use (i.e Python interpreters). If processes is None then the number returned by `os.cpu_count()` is used. This is the first argument to our script.

The `pool.map(pi_run, trials)` attribute of this object uses the pool to map a defined function (`pi_run`) to a list/iterator object (`trials`).

To run our estimation script with a pool of 2 cpus with 3 trials, we would use:

```bash
python whatispi.py 2 1000 2000 3000
```

this would produce the output:

```default
estimating pi based on [1000, 2000, 3000]
number of processes used:  2
[3.124, 3.134, 3.154]
```

The three trials above can be converted into a single estimate by taking the average (weighted by number of trials) of each run.


TBA: How to run this on Artemis using job arrays

```
2 1000 2000
2 2000 4000
2 10000 20000
2 50000 100000
```

Here is a simple Artemis PBS script that uses the parameters above:

```
#!/bin/bash
#PBS -P SIHnextgen
#PBS -l select=1:ncpus=2:mem=6GB
#PBS -l walltime=00:10:00
#PBS -J 1-4
#PBS -o piDemo^array_index^.o
#PBS -e piDemo^array_index^.e

module load python/3.8.2

cd $PBS_O_WORKDIR
params=`sed "${PBS_ARRAY_INDEX}q;d" job_params`
python3 whatispi.py ${params}
```

Running this script on Artemis will produce 4 .o and .e files number by their job array number.

NOTE: Each job used separate resources on the queue so the shorter running jobs were not waiting for the longer running to finish.

TBA: How to convert Artemis script to Gadi script 

TBA: how to optimize Gadi script - eg. when some runs have different average walltime.